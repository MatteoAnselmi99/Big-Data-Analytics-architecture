2025-07-20 19:13:29,179 INFO: Closing down clientserver connection
2025-07-20 23:44:28,755 INFO: Closing down clientserver connection
2025-07-21 23:20:20,891 ERROR: Traceback (most recent call last):
2025-07-21 23:20:20,891 ERROR: File "/home/matteo/PycharmProjects/Tesi/BDA_Scripts/retention_area/8_dw_feeder.py", line 184, in <module>
2025-07-21 23:20:20,892 ERROR: ).getOrCreate()
2025-07-21 23:20:20,892 ERROR: ^
2025-07-21 23:20:20,892 ERROR: ^
2025-07-21 23:20:20,892 ERROR: ^
2025-07-21 23:20:20,892 ERROR: ^
2025-07-21 23:20:20,892 ERROR: ^
2025-07-21 23:20:20,892 ERROR: ^
2025-07-21 23:20:20,892 ERROR: ^
2025-07-21 23:20:20,892 ERROR: ^
2025-07-21 23:20:20,892 ERROR: ^
2025-07-21 23:20:20,892 ERROR: ^
2025-07-21 23:20:20,892 ERROR: ^
2025-07-21 23:20:20,892 ERROR: ^
2025-07-21 23:20:20,892 ERROR: ^
2025-07-21 23:20:20,892 ERROR: File "/home/matteo/PycharmProjects/Tesi/.venv1/lib/python3.12/site-packages/pyspark/sql/session.py", line 556, in getOrCreate
2025-07-21 23:20:20,892 ERROR: sc = SparkContext.getOrCreate(sparkConf)
2025-07-21 23:20:20,892 ERROR: ^
2025-07-21 23:20:20,893 ERROR: ^
2025-07-21 23:20:20,893 ERROR: ^
2025-07-21 23:20:20,893 ERROR: ^
2025-07-21 23:20:20,893 ERROR: ^
2025-07-21 23:20:20,893 ERROR: ^
2025-07-21 23:20:20,893 ERROR: ^
2025-07-21 23:20:20,893 ERROR: ^
2025-07-21 23:20:20,893 ERROR: ^
2025-07-21 23:20:20,893 ERROR: ^
2025-07-21 23:20:20,893 ERROR: ^
2025-07-21 23:20:20,893 ERROR: ^
2025-07-21 23:20:20,893 ERROR: ^
2025-07-21 23:20:20,893 ERROR: ^
2025-07-21 23:20:20,893 ERROR: ^
2025-07-21 23:20:20,893 ERROR: ^
2025-07-21 23:20:20,893 ERROR: ^
2025-07-21 23:20:20,893 ERROR: ^
2025-07-21 23:20:20,893 ERROR: ^
2025-07-21 23:20:20,893 ERROR: ^
2025-07-21 23:20:20,893 ERROR: ^
2025-07-21 23:20:20,893 ERROR: ^
2025-07-21 23:20:20,893 ERROR: ^
2025-07-21 23:20:20,893 ERROR: ^
2025-07-21 23:20:20,893 ERROR: ^
2025-07-21 23:20:20,893 ERROR: ^
2025-07-21 23:20:20,893 ERROR: ^
2025-07-21 23:20:20,893 ERROR: ^
2025-07-21 23:20:20,893 ERROR: ^
2025-07-21 23:20:20,893 ERROR: ^
2025-07-21 23:20:20,893 ERROR: ^
2025-07-21 23:20:20,893 ERROR: ^
2025-07-21 23:20:20,893 ERROR: ^
2025-07-21 23:20:20,893 ERROR: ^
2025-07-21 23:20:20,893 ERROR: ^
2025-07-21 23:20:20,893 ERROR: File "/home/matteo/PycharmProjects/Tesi/.venv1/lib/python3.12/site-packages/pyspark/core/context.py", line 523, in getOrCreate
2025-07-21 23:20:20,894 ERROR: SparkContext(conf=conf or SparkConf())
2025-07-21 23:20:20,894 ERROR: File "/home/matteo/PycharmProjects/Tesi/.venv1/lib/python3.12/site-packages/pyspark/core/context.py", line 205, in __init__
2025-07-21 23:20:20,894 ERROR: SparkContext._ensure_initialized(self, gateway=gateway, conf=conf)
2025-07-21 23:20:20,894 ERROR: File "/home/matteo/PycharmProjects/Tesi/.venv1/lib/python3.12/site-packages/pyspark/core/context.py", line 444, in _ensure_initialized
2025-07-21 23:20:20,894 ERROR: SparkContext._gateway = gateway or launch_gateway(conf)
2025-07-21 23:20:20,894 ERROR: ^
2025-07-21 23:20:20,894 ERROR: ^
2025-07-21 23:20:20,894 ERROR: ^
2025-07-21 23:20:20,894 ERROR: ^
2025-07-21 23:20:20,894 ERROR: ^
2025-07-21 23:20:20,894 ERROR: ^
2025-07-21 23:20:20,894 ERROR: ^
2025-07-21 23:20:20,894 ERROR: ^
2025-07-21 23:20:20,894 ERROR: ^
2025-07-21 23:20:20,894 ERROR: ^
2025-07-21 23:20:20,894 ERROR: ^
2025-07-21 23:20:20,894 ERROR: ^
2025-07-21 23:20:20,894 ERROR: ^
2025-07-21 23:20:20,894 ERROR: ^
2025-07-21 23:20:20,894 ERROR: ^
2025-07-21 23:20:20,894 ERROR: ^
2025-07-21 23:20:20,894 ERROR: ^
2025-07-21 23:20:20,894 ERROR: ^
2025-07-21 23:20:20,894 ERROR: ^
2025-07-21 23:20:20,894 ERROR: ^
2025-07-21 23:20:20,894 ERROR: File "/home/matteo/PycharmProjects/Tesi/.venv1/lib/python3.12/site-packages/pyspark/java_gateway.py", line 111, in launch_gateway
2025-07-21 23:20:20,895 ERROR: raise PySparkRuntimeError(
2025-07-21 23:20:20,895 ERROR: pyspark.errors.exceptions.base
2025-07-21 23:20:20,895 ERROR: .
2025-07-21 23:20:20,895 ERROR: PySparkRuntimeError
2025-07-21 23:20:20,895 ERROR: :
2025-07-21 23:20:20,895 ERROR: [JAVA_GATEWAY_EXITED] Java gateway process exited before sending its port number.
2025-07-21 23:55:39,646 INFO: Errore durante la scrittura nella tabella fact_trip: [COLUMN_NOT_DEFINED_IN_TABLE] "STRING" column `trip_id` is not defined in table `fact_trip`, defined table columns are: `trip_id`, `contract`, `start_datetime`, `end_datetime`, `duration`, `total_meters`, `total_seconds`, `speeding`, `acceleration`, `brake`, `cornering`, `lateral_movement`, `ingestion_date`, `province_code`. SQLSTATE: 42703
2025-07-21 23:55:39,749 INFO: Closing down clientserver connection
2025-07-22 00:05:14,886 INFO: Scrittura completata per tabella: fact_trip
2025-07-22 00:05:14,941 INFO: Closing down clientserver connection
2025-07-22 21:21:16,929 INFO: Script /home/matteo/PycharmProjects/Tesi/BDA_Scripts/druid/druid_ingestion.sh eseguito con successo.
2025-07-22 21:21:17,140 INFO: Closing down clientserver connection
2025-07-22 21:22:42,399 INFO: Script /home/matteo/PycharmProjects/Tesi/BDA_Scripts/druid/druid_ingestion.sh eseguito con successo.
2025-07-22 21:22:42,604 INFO: Closing down clientserver connection
2025-07-22 21:26:07,389 INFO: Script /home/matteo/PycharmProjects/Tesi/BDA_Scripts/druid/druid_ingestion.sh eseguito con successo.
2025-07-22 21:26:07,585 INFO: Closing down clientserver connection
2025-07-22 21:28:39,468 INFO: Script /home/matteo/PycharmProjects/Tesi/BDA_Scripts/druid/druid_ingestion.sh eseguito con successo.
2025-07-22 21:28:39,606 INFO: Closing down clientserver connection
2025-07-22 21:36:16,499 INFO: Script /home/matteo/PycharmProjects/Tesi/BDA_Scripts/druid/druid_ingestion.sh eseguito con successo.
2025-07-22 21:36:16,626 INFO: Closing down clientserver connection
2025-07-22 21:36:49,546 INFO: Script /home/matteo/PycharmProjects/Tesi/BDA_Scripts/druid/druid_ingestion.sh eseguito con successo.
2025-07-22 21:36:49,613 INFO: Closing down clientserver connection
2025-07-22 21:40:00,371 INFO: Script /home/matteo/PycharmProjects/Tesi/BDA_Scripts/druid/druid_ingestion.sh eseguito con successo.
2025-07-22 21:40:00,478 INFO: Closing down clientserver connection
2025-07-22 21:53:58,550 INFO: Script /home/matteo/PycharmProjects/Tesi/BDA_Scripts/druid/druid_ingestion.sh eseguito con successo.
2025-07-22 21:53:59,000 INFO: Closing down clientserver connection
2025-07-22 22:03:05,724 INFO: Script /home/matteo/PycharmProjects/Tesi/BDA_Scripts/druid/druid_ingestion.sh eseguito con successo.
2025-07-22 22:03:05,778 INFO: Closing down clientserver connection
2025-07-22 22:09:15,938 INFO: Script /home/matteo/PycharmProjects/Tesi/BDA_Scripts/druid/druid_ingestion.sh eseguito con successo.
2025-07-22 22:09:16,043 INFO: Closing down clientserver connection
2025-07-22 22:10:20,874 INFO: Script /home/matteo/PycharmProjects/Tesi/BDA_Scripts/druid/druid_ingestion.sh eseguito con successo.
2025-07-22 22:10:21,366 INFO: Closing down clientserver connection
2025-07-22 22:13:14,818 INFO: Script /home/matteo/PycharmProjects/Tesi/BDA_Scripts/druid/druid_ingestion.sh eseguito con successo.
2025-07-22 22:13:14,884 INFO: Closing down clientserver connection
2025-07-22 22:14:12,094 INFO: Script /home/matteo/PycharmProjects/Tesi/BDA_Scripts/druid/druid_ingestion.sh eseguito con successo.
2025-07-22 22:14:12,205 INFO: Closing down clientserver connection
2025-07-22 22:15:19,663 INFO: Script /home/matteo/PycharmProjects/Tesi/BDA_Scripts/druid/druid_ingestion.sh eseguito con successo.
2025-07-22 22:15:19,728 INFO: Closing down clientserver connection
2025-07-22 22:18:54,823 INFO: Script /home/matteo/PycharmProjects/Tesi/BDA_Scripts/druid/druid_ingestion.sh eseguito con successo.
2025-07-22 22:18:54,921 INFO: Closing down clientserver connection
2025-07-22 22:23:49,091 INFO: Script /home/matteo/PycharmProjects/Tesi/BDA_Scripts/druid/druid_ingestion.sh eseguito con successo.
2025-07-22 22:23:49,157 INFO: Closing down clientserver connection
2025-07-22 22:29:08,892 INFO: Script /home/matteo/PycharmProjects/Tesi/BDA_Scripts/druid/druid_ingestion.sh eseguito con successo.
2025-07-22 22:29:08,983 INFO: Closing down clientserver connection
2025-07-22 22:33:15,520 INFO: Script /home/matteo/PycharmProjects/Tesi/BDA_Scripts/druid/druid_ingestion.sh eseguito con successo.
2025-07-22 22:33:15,563 INFO: Closing down clientserver connection
2025-07-22 22:37:12,327 INFO: Script /home/matteo/PycharmProjects/Tesi/BDA_Scripts/druid/druid_ingestion.sh eseguito con successo.
2025-07-22 22:37:12,380 INFO: Closing down clientserver connection
2025-07-22 22:42:05,422 INFO: Script /home/matteo/PycharmProjects/Tesi/BDA_Scripts/druid/druid_ingestion.sh eseguito con successo.
2025-07-22 22:42:05,510 INFO: Closing down clientserver connection
2025-07-22 22:44:19,578 INFO: Script /home/matteo/PycharmProjects/Tesi/BDA_Scripts/druid/druid_ingestion.sh eseguito con successo.
2025-07-22 22:44:19,670 INFO: Closing down clientserver connection
2025-07-22 22:50:10,599 INFO: Script /home/matteo/PycharmProjects/Tesi/BDA_Scripts/druid/druid_ingestion.sh eseguito con successo.
2025-07-22 22:50:10,671 INFO: Closing down clientserver connection
2025-07-23 00:04:22,350 INFO: Script /home/matteo/PycharmProjects/Tesi/BDA_Scripts/druid/druid_ingestion.sh eseguito con successo.
2025-07-23 00:04:22,863 INFO: Closing down clientserver connection
2025-07-23 01:09:58,046 ERROR: Traceback (most recent call last):
2025-07-23 01:09:58,048 ERROR: File "/home/matteo/PycharmProjects/Tesi/BDA_Scripts/retention_area/8_dw_feeder.py", line 229, in <module>
2025-07-23 01:09:58,048 ERROR: backup_bronze(spark, claims, access_key, secret_key, endpoint, s3_path)
2025-07-23 01:09:58,048 ERROR: File "/home/matteo/PycharmProjects/Tesi/BDA_Scripts/retention_area/8_dw_feeder.py", line 45, in backup_bronze
2025-07-23 01:09:58,048 ERROR: df_current.write.format("delta").mode("append").partitionBy("ingestion_date").save(s3_path)
2025-07-23 01:09:58,048 ERROR: File "/home/matteo/PycharmProjects/Tesi/.venv1/lib/python3.12/site-packages/pyspark/sql/readwriter.py", line 1745, in save
2025-07-23 01:09:58,049 ERROR: self._jwrite.save(path)
2025-07-23 01:09:58,049 ERROR: File "/home/matteo/PycharmProjects/Tesi/.venv1/lib/python3.12/site-packages/py4j/java_gateway.py", line 1362, in __call__
2025-07-23 01:09:58,050 ERROR: return_value = get_return_value(
2025-07-23 01:09:58,050 ERROR: ^
2025-07-23 01:09:58,050 ERROR: ^
2025-07-23 01:09:58,050 ERROR: ^
2025-07-23 01:09:58,050 ERROR: ^
2025-07-23 01:09:58,050 ERROR: ^
2025-07-23 01:09:58,050 ERROR: ^
2025-07-23 01:09:58,050 ERROR: ^
2025-07-23 01:09:58,050 ERROR: ^
2025-07-23 01:09:58,050 ERROR: ^
2025-07-23 01:09:58,050 ERROR: ^
2025-07-23 01:09:58,050 ERROR: ^
2025-07-23 01:09:58,050 ERROR: ^
2025-07-23 01:09:58,050 ERROR: ^
2025-07-23 01:09:58,050 ERROR: ^
2025-07-23 01:09:58,050 ERROR: ^
2025-07-23 01:09:58,050 ERROR: ^
2025-07-23 01:09:58,050 ERROR: ^
2025-07-23 01:09:58,050 ERROR: File "/home/matteo/PycharmProjects/Tesi/.venv1/lib/python3.12/site-packages/pyspark/errors/exceptions/captured.py", line 282, in deco
2025-07-23 01:09:58,050 ERROR: return f(*a, **kw)
2025-07-23 01:09:58,051 ERROR: ^
2025-07-23 01:09:58,051 ERROR: ^
2025-07-23 01:09:58,051 ERROR: ^
2025-07-23 01:09:58,051 ERROR: ^
2025-07-23 01:09:58,051 ERROR: ^
2025-07-23 01:09:58,051 ERROR: ^
2025-07-23 01:09:58,051 ERROR: ^
2025-07-23 01:09:58,051 ERROR: ^
2025-07-23 01:09:58,051 ERROR: ^
2025-07-23 01:09:58,051 ERROR: ^
2025-07-23 01:09:58,051 ERROR: ^
2025-07-23 01:09:58,051 ERROR: File "/home/matteo/PycharmProjects/Tesi/.venv1/lib/python3.12/site-packages/py4j/protocol.py", line 327, in get_return_value
2025-07-23 01:09:58,051 ERROR: raise Py4JJavaError(
2025-07-23 01:09:58,051 ERROR: py4j.protocol
2025-07-23 01:09:58,051 ERROR: .
2025-07-23 01:09:58,051 ERROR: Py4JJavaError
2025-07-23 01:09:58,092 ERROR: :
2025-07-23 01:09:58,092 ERROR: An error occurred while calling o53.save.
: java.lang.RuntimeException: java.lang.ClassNotFoundException: Class org.apache.hadoop.fs.s3a.S3AFileSystem not found
	at org.apache.hadoop.conf.Configuration.getClass(Configuration.java:2737)
	at org.apache.hadoop.fs.FileSystem.getFileSystemClass(FileSystem.java:3569)
	at org.apache.hadoop.fs.FileSystem.createFileSystem(FileSystem.java:3612)
	at org.apache.hadoop.fs.FileSystem.access$300(FileSystem.java:172)
	at org.apache.hadoop.fs.FileSystem$Cache.getInternal(FileSystem.java:3716)
	at org.apache.hadoop.fs.FileSystem$Cache.get(FileSystem.java:3667)
	at org.apache.hadoop.fs.FileSystem.get(FileSystem.java:557)
	at org.apache.hadoop.fs.Path.getFileSystem(Path.java:366)
	at org.apache.spark.sql.delta.DeltaLog$.apply(DeltaLog.scala:966)
	at org.apache.spark.sql.delta.DeltaLog$.forTable(DeltaLog.scala:801)
	at org.apache.spark.sql.delta.sources.DeltaDataSource.createRelation(DeltaDataSource.scala:197)
	at org.apache.spark.sql.execution.datasources.SaveIntoDataSourceCommand.run(SaveIntoDataSourceCommand.scala:55)
	at org.apache.spark.sql.execution.command.ExecutedCommandExec.sideEffectResult$lzycompute(commands.scala:79)
	at org.apache.spark.sql.execution.command.ExecutedCommandExec.sideEffectResult(commands.scala:77)
	at org.apache.spark.sql.execution.command.ExecutedCommandExec.executeCollect(commands.scala:88)
	at org.apache.spark.sql.execution.QueryExecution.$anonfun$eagerlyExecuteCommands$2(QueryExecution.scala:155)
	at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId0$8(SQLExecution.scala:162)
	at org.apache.spark.sql.execution.SQLExecution$.withSessionTagsApplied(SQLExecution.scala:268)
	at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId0$7(SQLExecution.scala:124)
	at org.apache.spark.JobArtifactSet$.withActiveJobArtifactState(JobArtifactSet.scala:94)
	at org.apache.spark.sql.artifact.ArtifactManager.$anonfun$withResources$1(ArtifactManager.scala:112)
	at org.apache.spark.sql.artifact.ArtifactManager.withClassLoaderIfNeeded(ArtifactManager.scala:106)
	at org.apache.spark.sql.artifact.ArtifactManager.withResources(ArtifactManager.scala:111)
	at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId0$6(SQLExecution.scala:124)
	at org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:291)
	at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId0$1(SQLExecution.scala:123)
	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:804)
	at org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId0(SQLExecution.scala:77)
	at org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:233)
	at org.apache.spark.sql.execution.QueryExecution.$anonfun$eagerlyExecuteCommands$1(QueryExecution.scala:155)
	at org.apache.spark.sql.execution.QueryExecution$.withInternalError(QueryExecution.scala:654)
	at org.apache.spark.sql.execution.QueryExecution.org$apache$spark$sql$execution$QueryExecution$$eagerlyExecute$1(QueryExecution.scala:154)
	at org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$3.applyOrElse(QueryExecution.scala:169)
	at org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$3.applyOrElse(QueryExecution.scala:164)
	at org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$transformDownWithPruning$1(TreeNode.scala:470)
	at org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(origin.scala:86)
	at org.apache.spark.sql.catalyst.trees.TreeNode.transformDownWithPruning(TreeNode.scala:470)
	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.org$apache$spark$sql$catalyst$plans$logical$AnalysisHelper$$super$transformDownWithPruning(LogicalPlan.scala:37)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning(AnalysisHelper.scala:360)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning$(AnalysisHelper.scala:356)
	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:37)
	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:37)
	at org.apache.spark.sql.catalyst.trees.TreeNode.transformDown(TreeNode.scala:446)
	at org.apache.spark.sql.execution.QueryExecution.eagerlyExecuteCommands(QueryExecution.scala:164)
	at org.apache.spark.sql.execution.QueryExecution.$anonfun$lazyCommandExecuted$1(QueryExecution.scala:126)
	at scala.util.Try$.apply(Try.scala:217)
	at org.apache.spark.util.Utils$.doTryWithCallerStacktrace(Utils.scala:1378)
	at org.apache.spark.util.Utils$.getTryWithCallerStacktrace(Utils.scala:1439)
	at org.apache.spark.util.LazyTry.get(LazyTry.scala:58)
	at org.apache.spark.sql.execution.QueryExecution.commandExecuted(QueryExecution.scala:131)
	at org.apache.spark.sql.execution.QueryExecution.assertCommandExecuted(QueryExecution.scala:192)
	at org.apache.spark.sql.classic.DataFrameWriter.runCommand(DataFrameWriter.scala:622)
	at org.apache.spark.sql.classic.DataFrameWriter.saveToV1Source(DataFrameWriter.scala:273)
	at org.apache.spark.sql.classic.DataFrameWriter.saveInternal(DataFrameWriter.scala:182)
	at org.apache.spark.sql.classic.DataFrameWriter.save(DataFrameWriter.scala:118)
	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:77)
	at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.base/java.lang.reflect.Method.invoke(Method.java:569)
	at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)
	at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)
	at py4j.Gateway.invoke(Gateway.java:282)
	at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)
	at py4j.commands.CallCommand.execute(CallCommand.java:79)
	at py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:184)
	at py4j.ClientServerConnection.run(ClientServerConnection.java:108)
	at java.base/java.lang.Thread.run(Thread.java:840)
	Suppressed: org.apache.spark.util.Utils$OriginalTryStackTraceException: Full stacktrace of original doTryWithCallerStacktrace caller
		at org.apache.hadoop.conf.Configuration.getClass(Configuration.java:2737)
		at org.apache.hadoop.fs.FileSystem.getFileSystemClass(FileSystem.java:3569)
		at org.apache.hadoop.fs.FileSystem.createFileSystem(FileSystem.java:3612)
		at org.apache.hadoop.fs.FileSystem.access$300(FileSystem.java:172)
		at org.apache.hadoop.fs.FileSystem$Cache.getInternal(FileSystem.java:3716)
		at org.apache.hadoop.fs.FileSystem$Cache.get(FileSystem.java:3667)
		at org.apache.hadoop.fs.FileSystem.get(FileSystem.java:557)
		at org.apache.hadoop.fs.Path.getFileSystem(Path.java:366)
		at org.apache.spark.sql.delta.DeltaLog$.apply(DeltaLog.scala:966)
		at org.apache.spark.sql.delta.DeltaLog$.forTable(DeltaLog.scala:801)
		at org.apache.spark.sql.delta.sources.DeltaDataSource.createRelation(DeltaDataSource.scala:197)
		at org.apache.spark.sql.execution.datasources.SaveIntoDataSourceCommand.run(SaveIntoDataSourceCommand.scala:55)
		at org.apache.spark.sql.execution.command.ExecutedCommandExec.sideEffectResult$lzycompute(commands.scala:79)
		at org.apache.spark.sql.execution.command.ExecutedCommandExec.sideEffectResult(commands.scala:77)
		at org.apache.spark.sql.execution.command.ExecutedCommandExec.executeCollect(commands.scala:88)
		at org.apache.spark.sql.execution.QueryExecution.$anonfun$eagerlyExecuteCommands$2(QueryExecution.scala:155)
		at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId0$8(SQLExecution.scala:162)
		at org.apache.spark.sql.execution.SQLExecution$.withSessionTagsApplied(SQLExecution.scala:268)
		at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId0$7(SQLExecution.scala:124)
		at org.apache.spark.JobArtifactSet$.withActiveJobArtifactState(JobArtifactSet.scala:94)
		at org.apache.spark.sql.artifact.ArtifactManager.$anonfun$withResources$1(ArtifactManager.scala:112)
		at org.apache.spark.sql.artifact.ArtifactManager.withClassLoaderIfNeeded(ArtifactManager.scala:106)
		at org.apache.spark.sql.artifact.ArtifactManager.withResources(ArtifactManager.scala:111)
		at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId0$6(SQLExecution.scala:124)
		at org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:291)
		at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId0$1(SQLExecution.scala:123)
		at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:804)
		at org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId0(SQLExecution.scala:77)
		at org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:233)
		at org.apache.spark.sql.execution.QueryExecution.$anonfun$eagerlyExecuteCommands$1(QueryExecution.scala:155)
		at org.apache.spark.sql.execution.QueryExecution$.withInternalError(QueryExecution.scala:654)
		at org.apache.spark.sql.execution.QueryExecution.org$apache$spark$sql$execution$QueryExecution$$eagerlyExecute$1(QueryExecution.scala:154)
		at org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$3.applyOrElse(QueryExecution.scala:169)
		at org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$3.applyOrElse(QueryExecution.scala:164)
		at org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$transformDownWithPruning$1(TreeNode.scala:470)
		at org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(origin.scala:86)
		at org.apache.spark.sql.catalyst.trees.TreeNode.transformDownWithPruning(TreeNode.scala:470)
		at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.org$apache$spark$sql$catalyst$plans$logical$AnalysisHelper$$super$transformDownWithPruning(LogicalPlan.scala:37)
		at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning(AnalysisHelper.scala:360)
		at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning$(AnalysisHelper.scala:356)
		at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:37)
		at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:37)
		at org.apache.spark.sql.catalyst.trees.TreeNode.transformDown(TreeNode.scala:446)
		at org.apache.spark.sql.execution.QueryExecution.eagerlyExecuteCommands(QueryExecution.scala:164)
		at org.apache.spark.sql.execution.QueryExecution.$anonfun$lazyCommandExecuted$1(QueryExecution.scala:126)
		at scala.util.Try$.apply(Try.scala:217)
		at org.apache.spark.util.Utils$.doTryWithCallerStacktrace(Utils.scala:1378)
		at org.apache.spark.util.LazyTry.tryT$lzycompute(LazyTry.scala:46)
		at org.apache.spark.util.LazyTry.tryT(LazyTry.scala:46)
		... 19 more
Caused by: java.lang.ClassNotFoundException: Class org.apache.hadoop.fs.s3a.S3AFileSystem not found
	at org.apache.hadoop.conf.Configuration.getClassByName(Configuration.java:2641)
	at org.apache.hadoop.conf.Configuration.getClass(Configuration.java:2735)
	at org.apache.hadoop.fs.FileSystem.getFileSystemClass(FileSystem.java:3569)
	at org.apache.hadoop.fs.FileSystem.createFileSystem(FileSystem.java:3612)
	at org.apache.hadoop.fs.FileSystem.access$300(FileSystem.java:172)
	at org.apache.hadoop.fs.FileSystem$Cache.getInternal(FileSystem.java:3716)
	at org.apache.hadoop.fs.FileSystem$Cache.get(FileSystem.java:3667)
	at org.apache.hadoop.fs.FileSystem.get(FileSystem.java:557)
	at org.apache.hadoop.fs.Path.getFileSystem(Path.java:366)
	at org.apache.spark.sql.delta.DeltaLog$.apply(DeltaLog.scala:966)
	at org.apache.spark.sql.delta.DeltaLog$.forTable(DeltaLog.scala:801)
	at org.apache.spark.sql.delta.sources.DeltaDataSource.createRelation(DeltaDataSource.scala:197)
	at org.apache.spark.sql.execution.datasources.SaveIntoDataSourceCommand.run(SaveIntoDataSourceCommand.scala:55)
	at org.apache.spark.sql.execution.command.ExecutedCommandExec.sideEffectResult$lzycompute(commands.scala:79)
	at org.apache.spark.sql.execution.command.ExecutedCommandExec.sideEffectResult(commands.scala:77)
	at org.apache.spark.sql.execution.command.ExecutedCommandExec.executeCollect(commands.scala:88)
	at org.apache.spark.sql.execution.QueryExecution.$anonfun$eagerlyExecuteCommands$2(QueryExecution.scala:155)
	at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId0$8(SQLExecution.scala:162)
	at org.apache.spark.sql.execution.SQLExecution$.withSessionTagsApplied(SQLExecution.scala:268)
	at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId0$7(SQLExecution.scala:124)
	at org.apache.spark.JobArtifactSet$.withActiveJobArtifactState(JobArtifactSet.scala:94)
	at org.apache.spark.sql.artifact.ArtifactManager.$anonfun$withResources$1(ArtifactManager.scala:112)
	at org.apache.spark.sql.artifact.ArtifactManager.withClassLoaderIfNeeded(ArtifactManager.scala:106)
	at org.apache.spark.sql.artifact.ArtifactManager.withResources(ArtifactManager.scala:111)
	at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId0$6(SQLExecution.scala:124)
	at org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:291)
	at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId0$1(SQLExecution.scala:123)
	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:804)
	at org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId0(SQLExecution.scala:77)
	at org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:233)
	at org.apache.spark.sql.execution.QueryExecution.$anonfun$eagerlyExecuteCommands$1(QueryExecution.scala:155)
	at org.apache.spark.sql.execution.QueryExecution$.withInternalError(QueryExecution.scala:654)
	at org.apache.spark.sql.execution.QueryExecution.org$apache$spark$sql$execution$QueryExecution$$eagerlyExecute$1(QueryExecution.scala:154)
	at org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$3.applyOrElse(QueryExecution.scala:169)
	at org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$3.applyOrElse(QueryExecution.scala:164)
	at org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$transformDownWithPruning$1(TreeNode.scala:470)
	at org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(origin.scala:86)
	at org.apache.spark.sql.catalyst.trees.TreeNode.transformDownWithPruning(TreeNode.scala:470)
	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.org$apache$spark$sql$catalyst$plans$logical$AnalysisHelper$$super$transformDownWithPruning(LogicalPlan.scala:37)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning(AnalysisHelper.scala:360)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning$(AnalysisHelper.scala:356)
	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:37)
	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:37)
	at org.apache.spark.sql.catalyst.trees.TreeNode.transformDown(TreeNode.scala:446)
	at org.apache.spark.sql.execution.QueryExecution.eagerlyExecuteCommands(QueryExecution.scala:164)
	at org.apache.spark.sql.execution.QueryExecution.$anonfun$lazyCommandExecuted$1(QueryExecution.scala:126)
	at scala.util.Try$.apply(Try.scala:217)
	at org.apache.spark.util.Utils$.doTryWithCallerStacktrace(Utils.scala:1378)
	at org.apache.spark.util.LazyTry.tryT$lzycompute(LazyTry.scala:46)
	at org.apache.spark.util.LazyTry.tryT(LazyTry.scala:46)
	... 19 more
2025-07-23 01:09:58,092 INFO: Closing down clientserver connection
2025-07-23 01:12:27,111 ERROR: Traceback (most recent call last):
2025-07-23 01:12:27,111 ERROR: File "/home/matteo/PycharmProjects/Tesi/BDA_Scripts/retention_area/8_dw_feeder.py", line 230, in <module>
2025-07-23 01:12:27,111 ERROR: backup_bronze(spark, claims, access_key, secret_key, endpoint, s3_path)
2025-07-23 01:12:27,111 ERROR: File "/home/matteo/PycharmProjects/Tesi/BDA_Scripts/retention_area/8_dw_feeder.py", line 45, in backup_bronze
2025-07-23 01:12:27,111 ERROR: df_current.write.format("delta").mode("append").partitionBy("ingestion_date").save(s3_path)
2025-07-23 01:12:27,112 ERROR: File "/home/matteo/PycharmProjects/Tesi/.venv1/lib/python3.12/site-packages/pyspark/sql/readwriter.py", line 1745, in save
2025-07-23 01:12:27,112 ERROR: self._jwrite.save(path)
2025-07-23 01:12:27,112 ERROR: File "/home/matteo/PycharmProjects/Tesi/.venv1/lib/python3.12/site-packages/py4j/java_gateway.py", line 1362, in __call__
2025-07-23 01:12:27,112 ERROR: return_value = get_return_value(
2025-07-23 01:12:27,112 ERROR: ^
2025-07-23 01:12:27,112 ERROR: ^
2025-07-23 01:12:27,112 ERROR: ^
2025-07-23 01:12:27,112 ERROR: ^
2025-07-23 01:12:27,112 ERROR: ^
2025-07-23 01:12:27,112 ERROR: ^
2025-07-23 01:12:27,112 ERROR: ^
2025-07-23 01:12:27,112 ERROR: ^
2025-07-23 01:12:27,112 ERROR: ^
2025-07-23 01:12:27,112 ERROR: ^
2025-07-23 01:12:27,112 ERROR: ^
2025-07-23 01:12:27,112 ERROR: ^
2025-07-23 01:12:27,112 ERROR: ^
2025-07-23 01:12:27,112 ERROR: ^
2025-07-23 01:12:27,112 ERROR: ^
2025-07-23 01:12:27,112 ERROR: ^
2025-07-23 01:12:27,112 ERROR: ^
2025-07-23 01:12:27,112 ERROR: File "/home/matteo/PycharmProjects/Tesi/.venv1/lib/python3.12/site-packages/pyspark/errors/exceptions/captured.py", line 282, in deco
2025-07-23 01:12:27,112 ERROR: return f(*a, **kw)
2025-07-23 01:12:27,112 ERROR: ^
2025-07-23 01:12:27,112 ERROR: ^
2025-07-23 01:12:27,112 ERROR: ^
2025-07-23 01:12:27,112 ERROR: ^
2025-07-23 01:12:27,112 ERROR: ^
2025-07-23 01:12:27,112 ERROR: ^
2025-07-23 01:12:27,112 ERROR: ^
2025-07-23 01:12:27,112 ERROR: ^
2025-07-23 01:12:27,112 ERROR: ^
2025-07-23 01:12:27,112 ERROR: ^
2025-07-23 01:12:27,112 ERROR: ^
2025-07-23 01:12:27,112 ERROR: File "/home/matteo/PycharmProjects/Tesi/.venv1/lib/python3.12/site-packages/py4j/protocol.py", line 327, in get_return_value
2025-07-23 01:12:27,112 ERROR: raise Py4JJavaError(
2025-07-23 01:12:27,112 ERROR: py4j.protocol
2025-07-23 01:12:27,113 ERROR: .
2025-07-23 01:12:27,113 ERROR: Py4JJavaError
2025-07-23 01:12:27,153 ERROR: :
2025-07-23 01:12:27,154 ERROR: An error occurred while calling o53.save.
: java.lang.RuntimeException: java.lang.ClassNotFoundException: Class org.apache.hadoop.fs.s3a.S3AFileSystem not found
	at org.apache.hadoop.conf.Configuration.getClass(Configuration.java:2737)
	at org.apache.hadoop.fs.FileSystem.getFileSystemClass(FileSystem.java:3569)
	at org.apache.hadoop.fs.FileSystem.createFileSystem(FileSystem.java:3612)
	at org.apache.hadoop.fs.FileSystem.access$300(FileSystem.java:172)
	at org.apache.hadoop.fs.FileSystem$Cache.getInternal(FileSystem.java:3716)
	at org.apache.hadoop.fs.FileSystem$Cache.get(FileSystem.java:3667)
	at org.apache.hadoop.fs.FileSystem.get(FileSystem.java:557)
	at org.apache.hadoop.fs.Path.getFileSystem(Path.java:366)
	at org.apache.spark.sql.delta.DeltaLog$.apply(DeltaLog.scala:966)
	at org.apache.spark.sql.delta.DeltaLog$.forTable(DeltaLog.scala:801)
	at org.apache.spark.sql.delta.sources.DeltaDataSource.createRelation(DeltaDataSource.scala:197)
	at org.apache.spark.sql.execution.datasources.SaveIntoDataSourceCommand.run(SaveIntoDataSourceCommand.scala:55)
	at org.apache.spark.sql.execution.command.ExecutedCommandExec.sideEffectResult$lzycompute(commands.scala:79)
	at org.apache.spark.sql.execution.command.ExecutedCommandExec.sideEffectResult(commands.scala:77)
	at org.apache.spark.sql.execution.command.ExecutedCommandExec.executeCollect(commands.scala:88)
	at org.apache.spark.sql.execution.QueryExecution.$anonfun$eagerlyExecuteCommands$2(QueryExecution.scala:155)
	at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId0$8(SQLExecution.scala:162)
	at org.apache.spark.sql.execution.SQLExecution$.withSessionTagsApplied(SQLExecution.scala:268)
	at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId0$7(SQLExecution.scala:124)
	at org.apache.spark.JobArtifactSet$.withActiveJobArtifactState(JobArtifactSet.scala:94)
	at org.apache.spark.sql.artifact.ArtifactManager.$anonfun$withResources$1(ArtifactManager.scala:112)
	at org.apache.spark.sql.artifact.ArtifactManager.withClassLoaderIfNeeded(ArtifactManager.scala:106)
	at org.apache.spark.sql.artifact.ArtifactManager.withResources(ArtifactManager.scala:111)
	at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId0$6(SQLExecution.scala:124)
	at org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:291)
	at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId0$1(SQLExecution.scala:123)
	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:804)
	at org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId0(SQLExecution.scala:77)
	at org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:233)
	at org.apache.spark.sql.execution.QueryExecution.$anonfun$eagerlyExecuteCommands$1(QueryExecution.scala:155)
	at org.apache.spark.sql.execution.QueryExecution$.withInternalError(QueryExecution.scala:654)
	at org.apache.spark.sql.execution.QueryExecution.org$apache$spark$sql$execution$QueryExecution$$eagerlyExecute$1(QueryExecution.scala:154)
	at org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$3.applyOrElse(QueryExecution.scala:169)
	at org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$3.applyOrElse(QueryExecution.scala:164)
	at org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$transformDownWithPruning$1(TreeNode.scala:470)
	at org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(origin.scala:86)
	at org.apache.spark.sql.catalyst.trees.TreeNode.transformDownWithPruning(TreeNode.scala:470)
	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.org$apache$spark$sql$catalyst$plans$logical$AnalysisHelper$$super$transformDownWithPruning(LogicalPlan.scala:37)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning(AnalysisHelper.scala:360)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning$(AnalysisHelper.scala:356)
	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:37)
	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:37)
	at org.apache.spark.sql.catalyst.trees.TreeNode.transformDown(TreeNode.scala:446)
	at org.apache.spark.sql.execution.QueryExecution.eagerlyExecuteCommands(QueryExecution.scala:164)
	at org.apache.spark.sql.execution.QueryExecution.$anonfun$lazyCommandExecuted$1(QueryExecution.scala:126)
	at scala.util.Try$.apply(Try.scala:217)
	at org.apache.spark.util.Utils$.doTryWithCallerStacktrace(Utils.scala:1378)
	at org.apache.spark.util.Utils$.getTryWithCallerStacktrace(Utils.scala:1439)
	at org.apache.spark.util.LazyTry.get(LazyTry.scala:58)
	at org.apache.spark.sql.execution.QueryExecution.commandExecuted(QueryExecution.scala:131)
	at org.apache.spark.sql.execution.QueryExecution.assertCommandExecuted(QueryExecution.scala:192)
	at org.apache.spark.sql.classic.DataFrameWriter.runCommand(DataFrameWriter.scala:622)
	at org.apache.spark.sql.classic.DataFrameWriter.saveToV1Source(DataFrameWriter.scala:273)
	at org.apache.spark.sql.classic.DataFrameWriter.saveInternal(DataFrameWriter.scala:182)
	at org.apache.spark.sql.classic.DataFrameWriter.save(DataFrameWriter.scala:118)
	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:77)
	at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.base/java.lang.reflect.Method.invoke(Method.java:569)
	at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)
	at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)
	at py4j.Gateway.invoke(Gateway.java:282)
	at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)
	at py4j.commands.CallCommand.execute(CallCommand.java:79)
	at py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:184)
	at py4j.ClientServerConnection.run(ClientServerConnection.java:108)
	at java.base/java.lang.Thread.run(Thread.java:840)
	Suppressed: org.apache.spark.util.Utils$OriginalTryStackTraceException: Full stacktrace of original doTryWithCallerStacktrace caller
		at org.apache.hadoop.conf.Configuration.getClass(Configuration.java:2737)
		at org.apache.hadoop.fs.FileSystem.getFileSystemClass(FileSystem.java:3569)
		at org.apache.hadoop.fs.FileSystem.createFileSystem(FileSystem.java:3612)
		at org.apache.hadoop.fs.FileSystem.access$300(FileSystem.java:172)
		at org.apache.hadoop.fs.FileSystem$Cache.getInternal(FileSystem.java:3716)
		at org.apache.hadoop.fs.FileSystem$Cache.get(FileSystem.java:3667)
		at org.apache.hadoop.fs.FileSystem.get(FileSystem.java:557)
		at org.apache.hadoop.fs.Path.getFileSystem(Path.java:366)
		at org.apache.spark.sql.delta.DeltaLog$.apply(DeltaLog.scala:966)
		at org.apache.spark.sql.delta.DeltaLog$.forTable(DeltaLog.scala:801)
		at org.apache.spark.sql.delta.sources.DeltaDataSource.createRelation(DeltaDataSource.scala:197)
		at org.apache.spark.sql.execution.datasources.SaveIntoDataSourceCommand.run(SaveIntoDataSourceCommand.scala:55)
		at org.apache.spark.sql.execution.command.ExecutedCommandExec.sideEffectResult$lzycompute(commands.scala:79)
		at org.apache.spark.sql.execution.command.ExecutedCommandExec.sideEffectResult(commands.scala:77)
		at org.apache.spark.sql.execution.command.ExecutedCommandExec.executeCollect(commands.scala:88)
		at org.apache.spark.sql.execution.QueryExecution.$anonfun$eagerlyExecuteCommands$2(QueryExecution.scala:155)
		at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId0$8(SQLExecution.scala:162)
		at org.apache.spark.sql.execution.SQLExecution$.withSessionTagsApplied(SQLExecution.scala:268)
		at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId0$7(SQLExecution.scala:124)
		at org.apache.spark.JobArtifactSet$.withActiveJobArtifactState(JobArtifactSet.scala:94)
		at org.apache.spark.sql.artifact.ArtifactManager.$anonfun$withResources$1(ArtifactManager.scala:112)
		at org.apache.spark.sql.artifact.ArtifactManager.withClassLoaderIfNeeded(ArtifactManager.scala:106)
		at org.apache.spark.sql.artifact.ArtifactManager.withResources(ArtifactManager.scala:111)
		at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId0$6(SQLExecution.scala:124)
		at org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:291)
		at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId0$1(SQLExecution.scala:123)
		at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:804)
		at org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId0(SQLExecution.scala:77)
		at org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:233)
		at org.apache.spark.sql.execution.QueryExecution.$anonfun$eagerlyExecuteCommands$1(QueryExecution.scala:155)
		at org.apache.spark.sql.execution.QueryExecution$.withInternalError(QueryExecution.scala:654)
		at org.apache.spark.sql.execution.QueryExecution.org$apache$spark$sql$execution$QueryExecution$$eagerlyExecute$1(QueryExecution.scala:154)
		at org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$3.applyOrElse(QueryExecution.scala:169)
		at org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$3.applyOrElse(QueryExecution.scala:164)
		at org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$transformDownWithPruning$1(TreeNode.scala:470)
		at org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(origin.scala:86)
		at org.apache.spark.sql.catalyst.trees.TreeNode.transformDownWithPruning(TreeNode.scala:470)
		at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.org$apache$spark$sql$catalyst$plans$logical$AnalysisHelper$$super$transformDownWithPruning(LogicalPlan.scala:37)
		at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning(AnalysisHelper.scala:360)
		at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning$(AnalysisHelper.scala:356)
		at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:37)
		at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:37)
		at org.apache.spark.sql.catalyst.trees.TreeNode.transformDown(TreeNode.scala:446)
		at org.apache.spark.sql.execution.QueryExecution.eagerlyExecuteCommands(QueryExecution.scala:164)
		at org.apache.spark.sql.execution.QueryExecution.$anonfun$lazyCommandExecuted$1(QueryExecution.scala:126)
		at scala.util.Try$.apply(Try.scala:217)
		at org.apache.spark.util.Utils$.doTryWithCallerStacktrace(Utils.scala:1378)
		at org.apache.spark.util.LazyTry.tryT$lzycompute(LazyTry.scala:46)
		at org.apache.spark.util.LazyTry.tryT(LazyTry.scala:46)
		... 19 more
Caused by: java.lang.ClassNotFoundException: Class org.apache.hadoop.fs.s3a.S3AFileSystem not found
	at org.apache.hadoop.conf.Configuration.getClassByName(Configuration.java:2641)
	at org.apache.hadoop.conf.Configuration.getClass(Configuration.java:2735)
	at org.apache.hadoop.fs.FileSystem.getFileSystemClass(FileSystem.java:3569)
	at org.apache.hadoop.fs.FileSystem.createFileSystem(FileSystem.java:3612)
	at org.apache.hadoop.fs.FileSystem.access$300(FileSystem.java:172)
	at org.apache.hadoop.fs.FileSystem$Cache.getInternal(FileSystem.java:3716)
	at org.apache.hadoop.fs.FileSystem$Cache.get(FileSystem.java:3667)
	at org.apache.hadoop.fs.FileSystem.get(FileSystem.java:557)
	at org.apache.hadoop.fs.Path.getFileSystem(Path.java:366)
	at org.apache.spark.sql.delta.DeltaLog$.apply(DeltaLog.scala:966)
	at org.apache.spark.sql.delta.DeltaLog$.forTable(DeltaLog.scala:801)
	at org.apache.spark.sql.delta.sources.DeltaDataSource.createRelation(DeltaDataSource.scala:197)
	at org.apache.spark.sql.execution.datasources.SaveIntoDataSourceCommand.run(SaveIntoDataSourceCommand.scala:55)
	at org.apache.spark.sql.execution.command.ExecutedCommandExec.sideEffectResult$lzycompute(commands.scala:79)
	at org.apache.spark.sql.execution.command.ExecutedCommandExec.sideEffectResult(commands.scala:77)
	at org.apache.spark.sql.execution.command.ExecutedCommandExec.executeCollect(commands.scala:88)
	at org.apache.spark.sql.execution.QueryExecution.$anonfun$eagerlyExecuteCommands$2(QueryExecution.scala:155)
	at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId0$8(SQLExecution.scala:162)
	at org.apache.spark.sql.execution.SQLExecution$.withSessionTagsApplied(SQLExecution.scala:268)
	at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId0$7(SQLExecution.scala:124)
	at org.apache.spark.JobArtifactSet$.withActiveJobArtifactState(JobArtifactSet.scala:94)
	at org.apache.spark.sql.artifact.ArtifactManager.$anonfun$withResources$1(ArtifactManager.scala:112)
	at org.apache.spark.sql.artifact.ArtifactManager.withClassLoaderIfNeeded(ArtifactManager.scala:106)
	at org.apache.spark.sql.artifact.ArtifactManager.withResources(ArtifactManager.scala:111)
	at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId0$6(SQLExecution.scala:124)
	at org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:291)
	at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId0$1(SQLExecution.scala:123)
	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:804)
	at org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId0(SQLExecution.scala:77)
	at org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:233)
	at org.apache.spark.sql.execution.QueryExecution.$anonfun$eagerlyExecuteCommands$1(QueryExecution.scala:155)
	at org.apache.spark.sql.execution.QueryExecution$.withInternalError(QueryExecution.scala:654)
	at org.apache.spark.sql.execution.QueryExecution.org$apache$spark$sql$execution$QueryExecution$$eagerlyExecute$1(QueryExecution.scala:154)
	at org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$3.applyOrElse(QueryExecution.scala:169)
	at org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$3.applyOrElse(QueryExecution.scala:164)
	at org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$transformDownWithPruning$1(TreeNode.scala:470)
	at org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(origin.scala:86)
	at org.apache.spark.sql.catalyst.trees.TreeNode.transformDownWithPruning(TreeNode.scala:470)
	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.org$apache$spark$sql$catalyst$plans$logical$AnalysisHelper$$super$transformDownWithPruning(LogicalPlan.scala:37)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning(AnalysisHelper.scala:360)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning$(AnalysisHelper.scala:356)
	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:37)
	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:37)
	at org.apache.spark.sql.catalyst.trees.TreeNode.transformDown(TreeNode.scala:446)
	at org.apache.spark.sql.execution.QueryExecution.eagerlyExecuteCommands(QueryExecution.scala:164)
	at org.apache.spark.sql.execution.QueryExecution.$anonfun$lazyCommandExecuted$1(QueryExecution.scala:126)
	at scala.util.Try$.apply(Try.scala:217)
	at org.apache.spark.util.Utils$.doTryWithCallerStacktrace(Utils.scala:1378)
	at org.apache.spark.util.LazyTry.tryT$lzycompute(LazyTry.scala:46)
	at org.apache.spark.util.LazyTry.tryT(LazyTry.scala:46)
	... 19 more
2025-07-23 01:12:27,154 INFO: Closing down clientserver connection
2025-07-23 01:15:00,804 ERROR: Traceback (most recent call last):
2025-07-23 01:15:00,804 ERROR: File "/home/matteo/PycharmProjects/Tesi/BDA_Scripts/retention_area/8_dw_feeder.py", line 234, in <module>
2025-07-23 01:15:00,804 ERROR: backup_bronze(spark, claims, access_key, secret_key, endpoint, s3_path)
2025-07-23 01:15:00,804 ERROR: File "/home/matteo/PycharmProjects/Tesi/BDA_Scripts/retention_area/8_dw_feeder.py", line 45, in backup_bronze
2025-07-23 01:15:00,804 ERROR: df_current.write.format("delta").mode("append").partitionBy("ingestion_date").save(s3_path)
2025-07-23 01:15:00,804 ERROR: File "/home/matteo/PycharmProjects/Tesi/.venv1/lib/python3.12/site-packages/pyspark/sql/readwriter.py", line 1745, in save
2025-07-23 01:15:00,805 ERROR: self._jwrite.save(path)
2025-07-23 01:15:00,805 ERROR: File "/home/matteo/PycharmProjects/Tesi/.venv1/lib/python3.12/site-packages/py4j/java_gateway.py", line 1362, in __call__
2025-07-23 01:15:00,805 ERROR: return_value = get_return_value(
2025-07-23 01:15:00,805 ERROR: ^
2025-07-23 01:15:00,805 ERROR: ^
2025-07-23 01:15:00,805 ERROR: ^
2025-07-23 01:15:00,805 ERROR: ^
2025-07-23 01:15:00,805 ERROR: ^
2025-07-23 01:15:00,805 ERROR: ^
2025-07-23 01:15:00,805 ERROR: ^
2025-07-23 01:15:00,805 ERROR: ^
2025-07-23 01:15:00,805 ERROR: ^
2025-07-23 01:15:00,805 ERROR: ^
2025-07-23 01:15:00,805 ERROR: ^
2025-07-23 01:15:00,805 ERROR: ^
2025-07-23 01:15:00,805 ERROR: ^
2025-07-23 01:15:00,805 ERROR: ^
2025-07-23 01:15:00,805 ERROR: ^
2025-07-23 01:15:00,805 ERROR: ^
2025-07-23 01:15:00,805 ERROR: ^
2025-07-23 01:15:00,805 ERROR: File "/home/matteo/PycharmProjects/Tesi/.venv1/lib/python3.12/site-packages/pyspark/errors/exceptions/captured.py", line 282, in deco
2025-07-23 01:15:00,805 ERROR: return f(*a, **kw)
2025-07-23 01:15:00,805 ERROR: ^
2025-07-23 01:15:00,805 ERROR: ^
2025-07-23 01:15:00,805 ERROR: ^
2025-07-23 01:15:00,805 ERROR: ^
2025-07-23 01:15:00,805 ERROR: ^
2025-07-23 01:15:00,805 ERROR: ^
2025-07-23 01:15:00,805 ERROR: ^
2025-07-23 01:15:00,805 ERROR: ^
2025-07-23 01:15:00,805 ERROR: ^
2025-07-23 01:15:00,805 ERROR: ^
2025-07-23 01:15:00,805 ERROR: ^
2025-07-23 01:15:00,805 ERROR: File "/home/matteo/PycharmProjects/Tesi/.venv1/lib/python3.12/site-packages/py4j/protocol.py", line 327, in get_return_value
2025-07-23 01:15:00,805 ERROR: raise Py4JJavaError(
2025-07-23 01:15:00,805 ERROR: py4j.protocol
2025-07-23 01:15:00,805 ERROR: .
2025-07-23 01:15:00,805 ERROR: Py4JJavaError
2025-07-23 01:15:00,847 ERROR: :
2025-07-23 01:15:00,847 ERROR: An error occurred while calling o60.save.
: java.lang.RuntimeException: java.lang.ClassNotFoundException: Class org.apache.hadoop.fs.s3a.S3AFileSystem not found
	at org.apache.hadoop.conf.Configuration.getClass(Configuration.java:2737)
	at org.apache.hadoop.fs.FileSystem.getFileSystemClass(FileSystem.java:3569)
	at org.apache.hadoop.fs.FileSystem.createFileSystem(FileSystem.java:3612)
	at org.apache.hadoop.fs.FileSystem.access$300(FileSystem.java:172)
	at org.apache.hadoop.fs.FileSystem$Cache.getInternal(FileSystem.java:3716)
	at org.apache.hadoop.fs.FileSystem$Cache.get(FileSystem.java:3667)
	at org.apache.hadoop.fs.FileSystem.get(FileSystem.java:557)
	at org.apache.hadoop.fs.Path.getFileSystem(Path.java:366)
	at org.apache.spark.sql.delta.DeltaLog$.apply(DeltaLog.scala:966)
	at org.apache.spark.sql.delta.DeltaLog$.forTable(DeltaLog.scala:801)
	at org.apache.spark.sql.delta.sources.DeltaDataSource.createRelation(DeltaDataSource.scala:197)
	at org.apache.spark.sql.execution.datasources.SaveIntoDataSourceCommand.run(SaveIntoDataSourceCommand.scala:55)
	at org.apache.spark.sql.execution.command.ExecutedCommandExec.sideEffectResult$lzycompute(commands.scala:79)
	at org.apache.spark.sql.execution.command.ExecutedCommandExec.sideEffectResult(commands.scala:77)
	at org.apache.spark.sql.execution.command.ExecutedCommandExec.executeCollect(commands.scala:88)
	at org.apache.spark.sql.execution.QueryExecution.$anonfun$eagerlyExecuteCommands$2(QueryExecution.scala:155)
	at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId0$8(SQLExecution.scala:162)
	at org.apache.spark.sql.execution.SQLExecution$.withSessionTagsApplied(SQLExecution.scala:268)
	at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId0$7(SQLExecution.scala:124)
	at org.apache.spark.JobArtifactSet$.withActiveJobArtifactState(JobArtifactSet.scala:94)
	at org.apache.spark.sql.artifact.ArtifactManager.$anonfun$withResources$1(ArtifactManager.scala:112)
	at org.apache.spark.sql.artifact.ArtifactManager.withClassLoaderIfNeeded(ArtifactManager.scala:106)
	at org.apache.spark.sql.artifact.ArtifactManager.withResources(ArtifactManager.scala:111)
	at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId0$6(SQLExecution.scala:124)
	at org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:291)
	at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId0$1(SQLExecution.scala:123)
	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:804)
	at org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId0(SQLExecution.scala:77)
	at org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:233)
	at org.apache.spark.sql.execution.QueryExecution.$anonfun$eagerlyExecuteCommands$1(QueryExecution.scala:155)
	at org.apache.spark.sql.execution.QueryExecution$.withInternalError(QueryExecution.scala:654)
	at org.apache.spark.sql.execution.QueryExecution.org$apache$spark$sql$execution$QueryExecution$$eagerlyExecute$1(QueryExecution.scala:154)
	at org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$3.applyOrElse(QueryExecution.scala:169)
	at org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$3.applyOrElse(QueryExecution.scala:164)
	at org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$transformDownWithPruning$1(TreeNode.scala:470)
	at org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(origin.scala:86)
	at org.apache.spark.sql.catalyst.trees.TreeNode.transformDownWithPruning(TreeNode.scala:470)
	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.org$apache$spark$sql$catalyst$plans$logical$AnalysisHelper$$super$transformDownWithPruning(LogicalPlan.scala:37)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning(AnalysisHelper.scala:360)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning$(AnalysisHelper.scala:356)
	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:37)
	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:37)
	at org.apache.spark.sql.catalyst.trees.TreeNode.transformDown(TreeNode.scala:446)
	at org.apache.spark.sql.execution.QueryExecution.eagerlyExecuteCommands(QueryExecution.scala:164)
	at org.apache.spark.sql.execution.QueryExecution.$anonfun$lazyCommandExecuted$1(QueryExecution.scala:126)
	at scala.util.Try$.apply(Try.scala:217)
	at org.apache.spark.util.Utils$.doTryWithCallerStacktrace(Utils.scala:1378)
	at org.apache.spark.util.Utils$.getTryWithCallerStacktrace(Utils.scala:1439)
	at org.apache.spark.util.LazyTry.get(LazyTry.scala:58)
	at org.apache.spark.sql.execution.QueryExecution.commandExecuted(QueryExecution.scala:131)
	at org.apache.spark.sql.execution.QueryExecution.assertCommandExecuted(QueryExecution.scala:192)
	at org.apache.spark.sql.classic.DataFrameWriter.runCommand(DataFrameWriter.scala:622)
	at org.apache.spark.sql.classic.DataFrameWriter.saveToV1Source(DataFrameWriter.scala:273)
	at org.apache.spark.sql.classic.DataFrameWriter.saveInternal(DataFrameWriter.scala:182)
	at org.apache.spark.sql.classic.DataFrameWriter.save(DataFrameWriter.scala:118)
	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:77)
	at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.base/java.lang.reflect.Method.invoke(Method.java:569)
	at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)
	at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)
	at py4j.Gateway.invoke(Gateway.java:282)
	at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)
	at py4j.commands.CallCommand.execute(CallCommand.java:79)
	at py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:184)
	at py4j.ClientServerConnection.run(ClientServerConnection.java:108)
	at java.base/java.lang.Thread.run(Thread.java:840)
	Suppressed: org.apache.spark.util.Utils$OriginalTryStackTraceException: Full stacktrace of original doTryWithCallerStacktrace caller
		at org.apache.hadoop.conf.Configuration.getClass(Configuration.java:2737)
		at org.apache.hadoop.fs.FileSystem.getFileSystemClass(FileSystem.java:3569)
		at org.apache.hadoop.fs.FileSystem.createFileSystem(FileSystem.java:3612)
		at org.apache.hadoop.fs.FileSystem.access$300(FileSystem.java:172)
		at org.apache.hadoop.fs.FileSystem$Cache.getInternal(FileSystem.java:3716)
		at org.apache.hadoop.fs.FileSystem$Cache.get(FileSystem.java:3667)
		at org.apache.hadoop.fs.FileSystem.get(FileSystem.java:557)
		at org.apache.hadoop.fs.Path.getFileSystem(Path.java:366)
		at org.apache.spark.sql.delta.DeltaLog$.apply(DeltaLog.scala:966)
		at org.apache.spark.sql.delta.DeltaLog$.forTable(DeltaLog.scala:801)
		at org.apache.spark.sql.delta.sources.DeltaDataSource.createRelation(DeltaDataSource.scala:197)
		at org.apache.spark.sql.execution.datasources.SaveIntoDataSourceCommand.run(SaveIntoDataSourceCommand.scala:55)
		at org.apache.spark.sql.execution.command.ExecutedCommandExec.sideEffectResult$lzycompute(commands.scala:79)
		at org.apache.spark.sql.execution.command.ExecutedCommandExec.sideEffectResult(commands.scala:77)
		at org.apache.spark.sql.execution.command.ExecutedCommandExec.executeCollect(commands.scala:88)
		at org.apache.spark.sql.execution.QueryExecution.$anonfun$eagerlyExecuteCommands$2(QueryExecution.scala:155)
		at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId0$8(SQLExecution.scala:162)
		at org.apache.spark.sql.execution.SQLExecution$.withSessionTagsApplied(SQLExecution.scala:268)
		at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId0$7(SQLExecution.scala:124)
		at org.apache.spark.JobArtifactSet$.withActiveJobArtifactState(JobArtifactSet.scala:94)
		at org.apache.spark.sql.artifact.ArtifactManager.$anonfun$withResources$1(ArtifactManager.scala:112)
		at org.apache.spark.sql.artifact.ArtifactManager.withClassLoaderIfNeeded(ArtifactManager.scala:106)
		at org.apache.spark.sql.artifact.ArtifactManager.withResources(ArtifactManager.scala:111)
		at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId0$6(SQLExecution.scala:124)
		at org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:291)
		at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId0$1(SQLExecution.scala:123)
		at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:804)
		at org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId0(SQLExecution.scala:77)
		at org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:233)
		at org.apache.spark.sql.execution.QueryExecution.$anonfun$eagerlyExecuteCommands$1(QueryExecution.scala:155)
		at org.apache.spark.sql.execution.QueryExecution$.withInternalError(QueryExecution.scala:654)
		at org.apache.spark.sql.execution.QueryExecution.org$apache$spark$sql$execution$QueryExecution$$eagerlyExecute$1(QueryExecution.scala:154)
		at org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$3.applyOrElse(QueryExecution.scala:169)
		at org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$3.applyOrElse(QueryExecution.scala:164)
		at org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$transformDownWithPruning$1(TreeNode.scala:470)
		at org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(origin.scala:86)
		at org.apache.spark.sql.catalyst.trees.TreeNode.transformDownWithPruning(TreeNode.scala:470)
		at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.org$apache$spark$sql$catalyst$plans$logical$AnalysisHelper$$super$transformDownWithPruning(LogicalPlan.scala:37)
		at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning(AnalysisHelper.scala:360)
		at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning$(AnalysisHelper.scala:356)
		at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:37)
		at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:37)
		at org.apache.spark.sql.catalyst.trees.TreeNode.transformDown(TreeNode.scala:446)
		at org.apache.spark.sql.execution.QueryExecution.eagerlyExecuteCommands(QueryExecution.scala:164)
		at org.apache.spark.sql.execution.QueryExecution.$anonfun$lazyCommandExecuted$1(QueryExecution.scala:126)
		at scala.util.Try$.apply(Try.scala:217)
		at org.apache.spark.util.Utils$.doTryWithCallerStacktrace(Utils.scala:1378)
		at org.apache.spark.util.LazyTry.tryT$lzycompute(LazyTry.scala:46)
		at org.apache.spark.util.LazyTry.tryT(LazyTry.scala:46)
		... 19 more
Caused by: java.lang.ClassNotFoundException: Class org.apache.hadoop.fs.s3a.S3AFileSystem not found
	at org.apache.hadoop.conf.Configuration.getClassByName(Configuration.java:2641)
	at org.apache.hadoop.conf.Configuration.getClass(Configuration.java:2735)
	at org.apache.hadoop.fs.FileSystem.getFileSystemClass(FileSystem.java:3569)
	at org.apache.hadoop.fs.FileSystem.createFileSystem(FileSystem.java:3612)
	at org.apache.hadoop.fs.FileSystem.access$300(FileSystem.java:172)
	at org.apache.hadoop.fs.FileSystem$Cache.getInternal(FileSystem.java:3716)
	at org.apache.hadoop.fs.FileSystem$Cache.get(FileSystem.java:3667)
	at org.apache.hadoop.fs.FileSystem.get(FileSystem.java:557)
	at org.apache.hadoop.fs.Path.getFileSystem(Path.java:366)
	at org.apache.spark.sql.delta.DeltaLog$.apply(DeltaLog.scala:966)
	at org.apache.spark.sql.delta.DeltaLog$.forTable(DeltaLog.scala:801)
	at org.apache.spark.sql.delta.sources.DeltaDataSource.createRelation(DeltaDataSource.scala:197)
	at org.apache.spark.sql.execution.datasources.SaveIntoDataSourceCommand.run(SaveIntoDataSourceCommand.scala:55)
	at org.apache.spark.sql.execution.command.ExecutedCommandExec.sideEffectResult$lzycompute(commands.scala:79)
	at org.apache.spark.sql.execution.command.ExecutedCommandExec.sideEffectResult(commands.scala:77)
	at org.apache.spark.sql.execution.command.ExecutedCommandExec.executeCollect(commands.scala:88)
	at org.apache.spark.sql.execution.QueryExecution.$anonfun$eagerlyExecuteCommands$2(QueryExecution.scala:155)
	at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId0$8(SQLExecution.scala:162)
	at org.apache.spark.sql.execution.SQLExecution$.withSessionTagsApplied(SQLExecution.scala:268)
	at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId0$7(SQLExecution.scala:124)
	at org.apache.spark.JobArtifactSet$.withActiveJobArtifactState(JobArtifactSet.scala:94)
	at org.apache.spark.sql.artifact.ArtifactManager.$anonfun$withResources$1(ArtifactManager.scala:112)
	at org.apache.spark.sql.artifact.ArtifactManager.withClassLoaderIfNeeded(ArtifactManager.scala:106)
	at org.apache.spark.sql.artifact.ArtifactManager.withResources(ArtifactManager.scala:111)
	at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId0$6(SQLExecution.scala:124)
	at org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:291)
	at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId0$1(SQLExecution.scala:123)
	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:804)
	at org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId0(SQLExecution.scala:77)
	at org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:233)
	at org.apache.spark.sql.execution.QueryExecution.$anonfun$eagerlyExecuteCommands$1(QueryExecution.scala:155)
	at org.apache.spark.sql.execution.QueryExecution$.withInternalError(QueryExecution.scala:654)
	at org.apache.spark.sql.execution.QueryExecution.org$apache$spark$sql$execution$QueryExecution$$eagerlyExecute$1(QueryExecution.scala:154)
	at org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$3.applyOrElse(QueryExecution.scala:169)
	at org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$3.applyOrElse(QueryExecution.scala:164)
	at org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$transformDownWithPruning$1(TreeNode.scala:470)
	at org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(origin.scala:86)
	at org.apache.spark.sql.catalyst.trees.TreeNode.transformDownWithPruning(TreeNode.scala:470)
	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.org$apache$spark$sql$catalyst$plans$logical$AnalysisHelper$$super$transformDownWithPruning(LogicalPlan.scala:37)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning(AnalysisHelper.scala:360)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning$(AnalysisHelper.scala:356)
	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:37)
	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:37)
	at org.apache.spark.sql.catalyst.trees.TreeNode.transformDown(TreeNode.scala:446)
	at org.apache.spark.sql.execution.QueryExecution.eagerlyExecuteCommands(QueryExecution.scala:164)
	at org.apache.spark.sql.execution.QueryExecution.$anonfun$lazyCommandExecuted$1(QueryExecution.scala:126)
	at scala.util.Try$.apply(Try.scala:217)
	at org.apache.spark.util.Utils$.doTryWithCallerStacktrace(Utils.scala:1378)
	at org.apache.spark.util.LazyTry.tryT$lzycompute(LazyTry.scala:46)
	at org.apache.spark.util.LazyTry.tryT(LazyTry.scala:46)
	... 19 more
2025-07-23 01:15:00,848 INFO: Closing down clientserver connection
2025-07-23 01:18:06,782 ERROR: Traceback (most recent call last):
2025-07-23 01:18:06,782 ERROR: File "/home/matteo/PycharmProjects/Tesi/BDA_Scripts/retention_area/8_dw_feeder.py", line 234, in <module>
2025-07-23 01:18:06,783 ERROR: backup_bronze(spark, claims, access_key, secret_key, endpoint, s3_path)
2025-07-23 01:18:06,783 ERROR: File "/home/matteo/PycharmProjects/Tesi/BDA_Scripts/retention_area/8_dw_feeder.py", line 45, in backup_bronze
2025-07-23 01:18:06,783 ERROR: df_current.write.format("delta").mode("append").partitionBy("ingestion_date").save(s3_path)
2025-07-23 01:18:06,783 ERROR: File "/home/matteo/PycharmProjects/Tesi/.venv1/lib/python3.12/site-packages/pyspark/sql/readwriter.py", line 1745, in save
2025-07-23 01:18:06,783 ERROR: self._jwrite.save(path)
2025-07-23 01:18:06,783 ERROR: File "/home/matteo/PycharmProjects/Tesi/.venv1/lib/python3.12/site-packages/py4j/java_gateway.py", line 1362, in __call__
2025-07-23 01:18:06,783 ERROR: return_value = get_return_value(
2025-07-23 01:18:06,783 ERROR: ^
2025-07-23 01:18:06,783 ERROR: ^
2025-07-23 01:18:06,783 ERROR: ^
2025-07-23 01:18:06,783 ERROR: ^
2025-07-23 01:18:06,783 ERROR: ^
2025-07-23 01:18:06,783 ERROR: ^
2025-07-23 01:18:06,783 ERROR: ^
2025-07-23 01:18:06,783 ERROR: ^
2025-07-23 01:18:06,783 ERROR: ^
2025-07-23 01:18:06,783 ERROR: ^
2025-07-23 01:18:06,783 ERROR: ^
2025-07-23 01:18:06,783 ERROR: ^
2025-07-23 01:18:06,783 ERROR: ^
2025-07-23 01:18:06,783 ERROR: ^
2025-07-23 01:18:06,783 ERROR: ^
2025-07-23 01:18:06,783 ERROR: ^
2025-07-23 01:18:06,783 ERROR: ^
2025-07-23 01:18:06,783 ERROR: File "/home/matteo/PycharmProjects/Tesi/.venv1/lib/python3.12/site-packages/pyspark/errors/exceptions/captured.py", line 282, in deco
2025-07-23 01:18:06,783 ERROR: return f(*a, **kw)
2025-07-23 01:18:06,783 ERROR: ^
2025-07-23 01:18:06,783 ERROR: ^
2025-07-23 01:18:06,783 ERROR: ^
2025-07-23 01:18:06,783 ERROR: ^
2025-07-23 01:18:06,783 ERROR: ^
2025-07-23 01:18:06,783 ERROR: ^
2025-07-23 01:18:06,783 ERROR: ^
2025-07-23 01:18:06,783 ERROR: ^
2025-07-23 01:18:06,783 ERROR: ^
2025-07-23 01:18:06,783 ERROR: ^
2025-07-23 01:18:06,783 ERROR: ^
2025-07-23 01:18:06,783 ERROR: File "/home/matteo/PycharmProjects/Tesi/.venv1/lib/python3.12/site-packages/py4j/protocol.py", line 327, in get_return_value
2025-07-23 01:18:06,783 ERROR: raise Py4JJavaError(
2025-07-23 01:18:06,783 ERROR: py4j.protocol
2025-07-23 01:18:06,783 ERROR: .
2025-07-23 01:18:06,783 ERROR: Py4JJavaError
2025-07-23 01:18:06,825 ERROR: :
2025-07-23 01:18:06,825 ERROR: An error occurred while calling o60.save.
: java.lang.RuntimeException: java.lang.ClassNotFoundException: Class org.apache.hadoop.fs.s3a.S3AFileSystem not found
	at org.apache.hadoop.conf.Configuration.getClass(Configuration.java:2737)
	at org.apache.hadoop.fs.FileSystem.getFileSystemClass(FileSystem.java:3569)
	at org.apache.hadoop.fs.FileSystem.createFileSystem(FileSystem.java:3612)
	at org.apache.hadoop.fs.FileSystem.access$300(FileSystem.java:172)
	at org.apache.hadoop.fs.FileSystem$Cache.getInternal(FileSystem.java:3716)
	at org.apache.hadoop.fs.FileSystem$Cache.get(FileSystem.java:3667)
	at org.apache.hadoop.fs.FileSystem.get(FileSystem.java:557)
	at org.apache.hadoop.fs.Path.getFileSystem(Path.java:366)
	at org.apache.spark.sql.delta.DeltaLog$.apply(DeltaLog.scala:966)
	at org.apache.spark.sql.delta.DeltaLog$.forTable(DeltaLog.scala:801)
	at org.apache.spark.sql.delta.sources.DeltaDataSource.createRelation(DeltaDataSource.scala:197)
	at org.apache.spark.sql.execution.datasources.SaveIntoDataSourceCommand.run(SaveIntoDataSourceCommand.scala:55)
	at org.apache.spark.sql.execution.command.ExecutedCommandExec.sideEffectResult$lzycompute(commands.scala:79)
	at org.apache.spark.sql.execution.command.ExecutedCommandExec.sideEffectResult(commands.scala:77)
	at org.apache.spark.sql.execution.command.ExecutedCommandExec.executeCollect(commands.scala:88)
	at org.apache.spark.sql.execution.QueryExecution.$anonfun$eagerlyExecuteCommands$2(QueryExecution.scala:155)
	at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId0$8(SQLExecution.scala:162)
	at org.apache.spark.sql.execution.SQLExecution$.withSessionTagsApplied(SQLExecution.scala:268)
	at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId0$7(SQLExecution.scala:124)
	at org.apache.spark.JobArtifactSet$.withActiveJobArtifactState(JobArtifactSet.scala:94)
	at org.apache.spark.sql.artifact.ArtifactManager.$anonfun$withResources$1(ArtifactManager.scala:112)
	at org.apache.spark.sql.artifact.ArtifactManager.withClassLoaderIfNeeded(ArtifactManager.scala:106)
	at org.apache.spark.sql.artifact.ArtifactManager.withResources(ArtifactManager.scala:111)
	at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId0$6(SQLExecution.scala:124)
	at org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:291)
	at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId0$1(SQLExecution.scala:123)
	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:804)
	at org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId0(SQLExecution.scala:77)
	at org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:233)
	at org.apache.spark.sql.execution.QueryExecution.$anonfun$eagerlyExecuteCommands$1(QueryExecution.scala:155)
	at org.apache.spark.sql.execution.QueryExecution$.withInternalError(QueryExecution.scala:654)
	at org.apache.spark.sql.execution.QueryExecution.org$apache$spark$sql$execution$QueryExecution$$eagerlyExecute$1(QueryExecution.scala:154)
	at org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$3.applyOrElse(QueryExecution.scala:169)
	at org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$3.applyOrElse(QueryExecution.scala:164)
	at org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$transformDownWithPruning$1(TreeNode.scala:470)
	at org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(origin.scala:86)
	at org.apache.spark.sql.catalyst.trees.TreeNode.transformDownWithPruning(TreeNode.scala:470)
	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.org$apache$spark$sql$catalyst$plans$logical$AnalysisHelper$$super$transformDownWithPruning(LogicalPlan.scala:37)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning(AnalysisHelper.scala:360)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning$(AnalysisHelper.scala:356)
	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:37)
	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:37)
	at org.apache.spark.sql.catalyst.trees.TreeNode.transformDown(TreeNode.scala:446)
	at org.apache.spark.sql.execution.QueryExecution.eagerlyExecuteCommands(QueryExecution.scala:164)
	at org.apache.spark.sql.execution.QueryExecution.$anonfun$lazyCommandExecuted$1(QueryExecution.scala:126)
	at scala.util.Try$.apply(Try.scala:217)
	at org.apache.spark.util.Utils$.doTryWithCallerStacktrace(Utils.scala:1378)
	at org.apache.spark.util.Utils$.getTryWithCallerStacktrace(Utils.scala:1439)
	at org.apache.spark.util.LazyTry.get(LazyTry.scala:58)
	at org.apache.spark.sql.execution.QueryExecution.commandExecuted(QueryExecution.scala:131)
	at org.apache.spark.sql.execution.QueryExecution.assertCommandExecuted(QueryExecution.scala:192)
	at org.apache.spark.sql.classic.DataFrameWriter.runCommand(DataFrameWriter.scala:622)
	at org.apache.spark.sql.classic.DataFrameWriter.saveToV1Source(DataFrameWriter.scala:273)
	at org.apache.spark.sql.classic.DataFrameWriter.saveInternal(DataFrameWriter.scala:182)
	at org.apache.spark.sql.classic.DataFrameWriter.save(DataFrameWriter.scala:118)
	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:77)
	at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.base/java.lang.reflect.Method.invoke(Method.java:569)
	at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)
	at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)
	at py4j.Gateway.invoke(Gateway.java:282)
	at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)
	at py4j.commands.CallCommand.execute(CallCommand.java:79)
	at py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:184)
	at py4j.ClientServerConnection.run(ClientServerConnection.java:108)
	at java.base/java.lang.Thread.run(Thread.java:840)
	Suppressed: org.apache.spark.util.Utils$OriginalTryStackTraceException: Full stacktrace of original doTryWithCallerStacktrace caller
		at org.apache.hadoop.conf.Configuration.getClass(Configuration.java:2737)
		at org.apache.hadoop.fs.FileSystem.getFileSystemClass(FileSystem.java:3569)
		at org.apache.hadoop.fs.FileSystem.createFileSystem(FileSystem.java:3612)
		at org.apache.hadoop.fs.FileSystem.access$300(FileSystem.java:172)
		at org.apache.hadoop.fs.FileSystem$Cache.getInternal(FileSystem.java:3716)
		at org.apache.hadoop.fs.FileSystem$Cache.get(FileSystem.java:3667)
		at org.apache.hadoop.fs.FileSystem.get(FileSystem.java:557)
		at org.apache.hadoop.fs.Path.getFileSystem(Path.java:366)
		at org.apache.spark.sql.delta.DeltaLog$.apply(DeltaLog.scala:966)
		at org.apache.spark.sql.delta.DeltaLog$.forTable(DeltaLog.scala:801)
		at org.apache.spark.sql.delta.sources.DeltaDataSource.createRelation(DeltaDataSource.scala:197)
		at org.apache.spark.sql.execution.datasources.SaveIntoDataSourceCommand.run(SaveIntoDataSourceCommand.scala:55)
		at org.apache.spark.sql.execution.command.ExecutedCommandExec.sideEffectResult$lzycompute(commands.scala:79)
		at org.apache.spark.sql.execution.command.ExecutedCommandExec.sideEffectResult(commands.scala:77)
		at org.apache.spark.sql.execution.command.ExecutedCommandExec.executeCollect(commands.scala:88)
		at org.apache.spark.sql.execution.QueryExecution.$anonfun$eagerlyExecuteCommands$2(QueryExecution.scala:155)
		at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId0$8(SQLExecution.scala:162)
		at org.apache.spark.sql.execution.SQLExecution$.withSessionTagsApplied(SQLExecution.scala:268)
		at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId0$7(SQLExecution.scala:124)
		at org.apache.spark.JobArtifactSet$.withActiveJobArtifactState(JobArtifactSet.scala:94)
		at org.apache.spark.sql.artifact.ArtifactManager.$anonfun$withResources$1(ArtifactManager.scala:112)
		at org.apache.spark.sql.artifact.ArtifactManager.withClassLoaderIfNeeded(ArtifactManager.scala:106)
		at org.apache.spark.sql.artifact.ArtifactManager.withResources(ArtifactManager.scala:111)
		at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId0$6(SQLExecution.scala:124)
		at org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:291)
		at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId0$1(SQLExecution.scala:123)
		at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:804)
		at org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId0(SQLExecution.scala:77)
		at org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:233)
		at org.apache.spark.sql.execution.QueryExecution.$anonfun$eagerlyExecuteCommands$1(QueryExecution.scala:155)
		at org.apache.spark.sql.execution.QueryExecution$.withInternalError(QueryExecution.scala:654)
		at org.apache.spark.sql.execution.QueryExecution.org$apache$spark$sql$execution$QueryExecution$$eagerlyExecute$1(QueryExecution.scala:154)
		at org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$3.applyOrElse(QueryExecution.scala:169)
		at org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$3.applyOrElse(QueryExecution.scala:164)
		at org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$transformDownWithPruning$1(TreeNode.scala:470)
		at org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(origin.scala:86)
		at org.apache.spark.sql.catalyst.trees.TreeNode.transformDownWithPruning(TreeNode.scala:470)
		at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.org$apache$spark$sql$catalyst$plans$logical$AnalysisHelper$$super$transformDownWithPruning(LogicalPlan.scala:37)
		at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning(AnalysisHelper.scala:360)
		at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning$(AnalysisHelper.scala:356)
		at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:37)
		at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:37)
		at org.apache.spark.sql.catalyst.trees.TreeNode.transformDown(TreeNode.scala:446)
		at org.apache.spark.sql.execution.QueryExecution.eagerlyExecuteCommands(QueryExecution.scala:164)
		at org.apache.spark.sql.execution.QueryExecution.$anonfun$lazyCommandExecuted$1(QueryExecution.scala:126)
		at scala.util.Try$.apply(Try.scala:217)
		at org.apache.spark.util.Utils$.doTryWithCallerStacktrace(Utils.scala:1378)
		at org.apache.spark.util.LazyTry.tryT$lzycompute(LazyTry.scala:46)
		at org.apache.spark.util.LazyTry.tryT(LazyTry.scala:46)
		... 19 more
Caused by: java.lang.ClassNotFoundException: Class org.apache.hadoop.fs.s3a.S3AFileSystem not found
	at org.apache.hadoop.conf.Configuration.getClassByName(Configuration.java:2641)
	at org.apache.hadoop.conf.Configuration.getClass(Configuration.java:2735)
	at org.apache.hadoop.fs.FileSystem.getFileSystemClass(FileSystem.java:3569)
	at org.apache.hadoop.fs.FileSystem.createFileSystem(FileSystem.java:3612)
	at org.apache.hadoop.fs.FileSystem.access$300(FileSystem.java:172)
	at org.apache.hadoop.fs.FileSystem$Cache.getInternal(FileSystem.java:3716)
	at org.apache.hadoop.fs.FileSystem$Cache.get(FileSystem.java:3667)
	at org.apache.hadoop.fs.FileSystem.get(FileSystem.java:557)
	at org.apache.hadoop.fs.Path.getFileSystem(Path.java:366)
	at org.apache.spark.sql.delta.DeltaLog$.apply(DeltaLog.scala:966)
	at org.apache.spark.sql.delta.DeltaLog$.forTable(DeltaLog.scala:801)
	at org.apache.spark.sql.delta.sources.DeltaDataSource.createRelation(DeltaDataSource.scala:197)
	at org.apache.spark.sql.execution.datasources.SaveIntoDataSourceCommand.run(SaveIntoDataSourceCommand.scala:55)
	at org.apache.spark.sql.execution.command.ExecutedCommandExec.sideEffectResult$lzycompute(commands.scala:79)
	at org.apache.spark.sql.execution.command.ExecutedCommandExec.sideEffectResult(commands.scala:77)
	at org.apache.spark.sql.execution.command.ExecutedCommandExec.executeCollect(commands.scala:88)
	at org.apache.spark.sql.execution.QueryExecution.$anonfun$eagerlyExecuteCommands$2(QueryExecution.scala:155)
	at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId0$8(SQLExecution.scala:162)
	at org.apache.spark.sql.execution.SQLExecution$.withSessionTagsApplied(SQLExecution.scala:268)
	at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId0$7(SQLExecution.scala:124)
	at org.apache.spark.JobArtifactSet$.withActiveJobArtifactState(JobArtifactSet.scala:94)
	at org.apache.spark.sql.artifact.ArtifactManager.$anonfun$withResources$1(ArtifactManager.scala:112)
	at org.apache.spark.sql.artifact.ArtifactManager.withClassLoaderIfNeeded(ArtifactManager.scala:106)
	at org.apache.spark.sql.artifact.ArtifactManager.withResources(ArtifactManager.scala:111)
	at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId0$6(SQLExecution.scala:124)
	at org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:291)
	at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId0$1(SQLExecution.scala:123)
	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:804)
	at org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId0(SQLExecution.scala:77)
	at org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:233)
	at org.apache.spark.sql.execution.QueryExecution.$anonfun$eagerlyExecuteCommands$1(QueryExecution.scala:155)
	at org.apache.spark.sql.execution.QueryExecution$.withInternalError(QueryExecution.scala:654)
	at org.apache.spark.sql.execution.QueryExecution.org$apache$spark$sql$execution$QueryExecution$$eagerlyExecute$1(QueryExecution.scala:154)
	at org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$3.applyOrElse(QueryExecution.scala:169)
	at org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$3.applyOrElse(QueryExecution.scala:164)
	at org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$transformDownWithPruning$1(TreeNode.scala:470)
	at org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(origin.scala:86)
	at org.apache.spark.sql.catalyst.trees.TreeNode.transformDownWithPruning(TreeNode.scala:470)
	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.org$apache$spark$sql$catalyst$plans$logical$AnalysisHelper$$super$transformDownWithPruning(LogicalPlan.scala:37)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning(AnalysisHelper.scala:360)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning$(AnalysisHelper.scala:356)
	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:37)
	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:37)
	at org.apache.spark.sql.catalyst.trees.TreeNode.transformDown(TreeNode.scala:446)
	at org.apache.spark.sql.execution.QueryExecution.eagerlyExecuteCommands(QueryExecution.scala:164)
	at org.apache.spark.sql.execution.QueryExecution.$anonfun$lazyCommandExecuted$1(QueryExecution.scala:126)
	at scala.util.Try$.apply(Try.scala:217)
	at org.apache.spark.util.Utils$.doTryWithCallerStacktrace(Utils.scala:1378)
	at org.apache.spark.util.LazyTry.tryT$lzycompute(LazyTry.scala:46)
	at org.apache.spark.util.LazyTry.tryT(LazyTry.scala:46)
	... 19 more
2025-07-23 01:18:06,825 INFO: Closing down clientserver connection
2025-07-23 10:06:19,711 ERROR: Traceback (most recent call last):
2025-07-23 10:06:19,712 ERROR: File "/home/matteo/PycharmProjects/Tesi/BDA_Scripts/retention_area/8_dw_feeder.py", line 234, in <module>
2025-07-23 10:06:19,712 ERROR: backup_bronze(spark, claims, access_key, secret_key, endpoint, s3_path)
2025-07-23 10:06:19,712 ERROR: File "/home/matteo/PycharmProjects/Tesi/BDA_Scripts/retention_area/8_dw_feeder.py", line 45, in backup_bronze
2025-07-23 10:06:19,712 ERROR: df_current.write.format("delta").mode("append").partitionBy("ingestion_date").save(s3_path)
2025-07-23 10:06:19,712 ERROR: File "/home/matteo/PycharmProjects/Tesi/.venv1/lib/python3.12/site-packages/pyspark/sql/readwriter.py", line 1745, in save
2025-07-23 10:06:19,713 ERROR: self._jwrite.save(path)
2025-07-23 10:06:19,713 ERROR: File "/home/matteo/PycharmProjects/Tesi/.venv1/lib/python3.12/site-packages/py4j/java_gateway.py", line 1362, in __call__
2025-07-23 10:06:19,714 ERROR: return_value = get_return_value(
2025-07-23 10:06:19,714 ERROR: ^
2025-07-23 10:06:19,714 ERROR: ^
2025-07-23 10:06:19,714 ERROR: ^
2025-07-23 10:06:19,714 ERROR: ^
2025-07-23 10:06:19,714 ERROR: ^
2025-07-23 10:06:19,714 ERROR: ^
2025-07-23 10:06:19,714 ERROR: ^
2025-07-23 10:06:19,714 ERROR: ^
2025-07-23 10:06:19,714 ERROR: ^
2025-07-23 10:06:19,714 ERROR: ^
2025-07-23 10:06:19,714 ERROR: ^
2025-07-23 10:06:19,714 ERROR: ^
2025-07-23 10:06:19,714 ERROR: ^
2025-07-23 10:06:19,714 ERROR: ^
2025-07-23 10:06:19,714 ERROR: ^
2025-07-23 10:06:19,714 ERROR: ^
2025-07-23 10:06:19,714 ERROR: ^
2025-07-23 10:06:19,714 ERROR: File "/home/matteo/PycharmProjects/Tesi/.venv1/lib/python3.12/site-packages/pyspark/errors/exceptions/captured.py", line 282, in deco
2025-07-23 10:06:19,715 ERROR: return f(*a, **kw)
2025-07-23 10:06:19,715 ERROR: ^
2025-07-23 10:06:19,715 ERROR: ^
2025-07-23 10:06:19,715 ERROR: ^
2025-07-23 10:06:19,715 ERROR: ^
2025-07-23 10:06:19,715 ERROR: ^
2025-07-23 10:06:19,715 ERROR: ^
2025-07-23 10:06:19,715 ERROR: ^
2025-07-23 10:06:19,715 ERROR: ^
2025-07-23 10:06:19,715 ERROR: ^
2025-07-23 10:06:19,715 ERROR: ^
2025-07-23 10:06:19,715 ERROR: ^
2025-07-23 10:06:19,715 ERROR: File "/home/matteo/PycharmProjects/Tesi/.venv1/lib/python3.12/site-packages/py4j/protocol.py", line 327, in get_return_value
2025-07-23 10:06:19,715 ERROR: raise Py4JJavaError(
2025-07-23 10:06:19,715 ERROR: py4j.protocol
2025-07-23 10:06:19,715 ERROR: .
2025-07-23 10:06:19,715 ERROR: Py4JJavaError
2025-07-23 10:06:19,756 ERROR: :
2025-07-23 10:06:19,756 ERROR: An error occurred while calling o60.save.
: java.lang.RuntimeException: java.lang.ClassNotFoundException: Class org.apache.hadoop.fs.s3a.S3AFileSystem not found
	at org.apache.hadoop.conf.Configuration.getClass(Configuration.java:2737)
	at org.apache.hadoop.fs.FileSystem.getFileSystemClass(FileSystem.java:3569)
	at org.apache.hadoop.fs.FileSystem.createFileSystem(FileSystem.java:3612)
	at org.apache.hadoop.fs.FileSystem.access$300(FileSystem.java:172)
	at org.apache.hadoop.fs.FileSystem$Cache.getInternal(FileSystem.java:3716)
	at org.apache.hadoop.fs.FileSystem$Cache.get(FileSystem.java:3667)
	at org.apache.hadoop.fs.FileSystem.get(FileSystem.java:557)
	at org.apache.hadoop.fs.Path.getFileSystem(Path.java:366)
	at org.apache.spark.sql.delta.DeltaLog$.apply(DeltaLog.scala:966)
	at org.apache.spark.sql.delta.DeltaLog$.forTable(DeltaLog.scala:801)
	at org.apache.spark.sql.delta.sources.DeltaDataSource.createRelation(DeltaDataSource.scala:197)
	at org.apache.spark.sql.execution.datasources.SaveIntoDataSourceCommand.run(SaveIntoDataSourceCommand.scala:55)
	at org.apache.spark.sql.execution.command.ExecutedCommandExec.sideEffectResult$lzycompute(commands.scala:79)
	at org.apache.spark.sql.execution.command.ExecutedCommandExec.sideEffectResult(commands.scala:77)
	at org.apache.spark.sql.execution.command.ExecutedCommandExec.executeCollect(commands.scala:88)
	at org.apache.spark.sql.execution.QueryExecution.$anonfun$eagerlyExecuteCommands$2(QueryExecution.scala:155)
	at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId0$8(SQLExecution.scala:162)
	at org.apache.spark.sql.execution.SQLExecution$.withSessionTagsApplied(SQLExecution.scala:268)
	at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId0$7(SQLExecution.scala:124)
	at org.apache.spark.JobArtifactSet$.withActiveJobArtifactState(JobArtifactSet.scala:94)
	at org.apache.spark.sql.artifact.ArtifactManager.$anonfun$withResources$1(ArtifactManager.scala:112)
	at org.apache.spark.sql.artifact.ArtifactManager.withClassLoaderIfNeeded(ArtifactManager.scala:106)
	at org.apache.spark.sql.artifact.ArtifactManager.withResources(ArtifactManager.scala:111)
	at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId0$6(SQLExecution.scala:124)
	at org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:291)
	at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId0$1(SQLExecution.scala:123)
	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:804)
	at org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId0(SQLExecution.scala:77)
	at org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:233)
	at org.apache.spark.sql.execution.QueryExecution.$anonfun$eagerlyExecuteCommands$1(QueryExecution.scala:155)
	at org.apache.spark.sql.execution.QueryExecution$.withInternalError(QueryExecution.scala:654)
	at org.apache.spark.sql.execution.QueryExecution.org$apache$spark$sql$execution$QueryExecution$$eagerlyExecute$1(QueryExecution.scala:154)
	at org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$3.applyOrElse(QueryExecution.scala:169)
	at org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$3.applyOrElse(QueryExecution.scala:164)
	at org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$transformDownWithPruning$1(TreeNode.scala:470)
	at org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(origin.scala:86)
	at org.apache.spark.sql.catalyst.trees.TreeNode.transformDownWithPruning(TreeNode.scala:470)
	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.org$apache$spark$sql$catalyst$plans$logical$AnalysisHelper$$super$transformDownWithPruning(LogicalPlan.scala:37)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning(AnalysisHelper.scala:360)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning$(AnalysisHelper.scala:356)
	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:37)
	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:37)
	at org.apache.spark.sql.catalyst.trees.TreeNode.transformDown(TreeNode.scala:446)
	at org.apache.spark.sql.execution.QueryExecution.eagerlyExecuteCommands(QueryExecution.scala:164)
	at org.apache.spark.sql.execution.QueryExecution.$anonfun$lazyCommandExecuted$1(QueryExecution.scala:126)
	at scala.util.Try$.apply(Try.scala:217)
	at org.apache.spark.util.Utils$.doTryWithCallerStacktrace(Utils.scala:1378)
	at org.apache.spark.util.Utils$.getTryWithCallerStacktrace(Utils.scala:1439)
	at org.apache.spark.util.LazyTry.get(LazyTry.scala:58)
	at org.apache.spark.sql.execution.QueryExecution.commandExecuted(QueryExecution.scala:131)
	at org.apache.spark.sql.execution.QueryExecution.assertCommandExecuted(QueryExecution.scala:192)
	at org.apache.spark.sql.classic.DataFrameWriter.runCommand(DataFrameWriter.scala:622)
	at org.apache.spark.sql.classic.DataFrameWriter.saveToV1Source(DataFrameWriter.scala:273)
	at org.apache.spark.sql.classic.DataFrameWriter.saveInternal(DataFrameWriter.scala:182)
	at org.apache.spark.sql.classic.DataFrameWriter.save(DataFrameWriter.scala:118)
	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:77)
	at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.base/java.lang.reflect.Method.invoke(Method.java:569)
	at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)
	at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)
	at py4j.Gateway.invoke(Gateway.java:282)
	at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)
	at py4j.commands.CallCommand.execute(CallCommand.java:79)
	at py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:184)
	at py4j.ClientServerConnection.run(ClientServerConnection.java:108)
	at java.base/java.lang.Thread.run(Thread.java:840)
	Suppressed: org.apache.spark.util.Utils$OriginalTryStackTraceException: Full stacktrace of original doTryWithCallerStacktrace caller
		at org.apache.hadoop.conf.Configuration.getClass(Configuration.java:2737)
		at org.apache.hadoop.fs.FileSystem.getFileSystemClass(FileSystem.java:3569)
		at org.apache.hadoop.fs.FileSystem.createFileSystem(FileSystem.java:3612)
		at org.apache.hadoop.fs.FileSystem.access$300(FileSystem.java:172)
		at org.apache.hadoop.fs.FileSystem$Cache.getInternal(FileSystem.java:3716)
		at org.apache.hadoop.fs.FileSystem$Cache.get(FileSystem.java:3667)
		at org.apache.hadoop.fs.FileSystem.get(FileSystem.java:557)
		at org.apache.hadoop.fs.Path.getFileSystem(Path.java:366)
		at org.apache.spark.sql.delta.DeltaLog$.apply(DeltaLog.scala:966)
		at org.apache.spark.sql.delta.DeltaLog$.forTable(DeltaLog.scala:801)
		at org.apache.spark.sql.delta.sources.DeltaDataSource.createRelation(DeltaDataSource.scala:197)
		at org.apache.spark.sql.execution.datasources.SaveIntoDataSourceCommand.run(SaveIntoDataSourceCommand.scala:55)
		at org.apache.spark.sql.execution.command.ExecutedCommandExec.sideEffectResult$lzycompute(commands.scala:79)
		at org.apache.spark.sql.execution.command.ExecutedCommandExec.sideEffectResult(commands.scala:77)
		at org.apache.spark.sql.execution.command.ExecutedCommandExec.executeCollect(commands.scala:88)
		at org.apache.spark.sql.execution.QueryExecution.$anonfun$eagerlyExecuteCommands$2(QueryExecution.scala:155)
		at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId0$8(SQLExecution.scala:162)
		at org.apache.spark.sql.execution.SQLExecution$.withSessionTagsApplied(SQLExecution.scala:268)
		at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId0$7(SQLExecution.scala:124)
		at org.apache.spark.JobArtifactSet$.withActiveJobArtifactState(JobArtifactSet.scala:94)
		at org.apache.spark.sql.artifact.ArtifactManager.$anonfun$withResources$1(ArtifactManager.scala:112)
		at org.apache.spark.sql.artifact.ArtifactManager.withClassLoaderIfNeeded(ArtifactManager.scala:106)
		at org.apache.spark.sql.artifact.ArtifactManager.withResources(ArtifactManager.scala:111)
		at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId0$6(SQLExecution.scala:124)
		at org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:291)
		at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId0$1(SQLExecution.scala:123)
		at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:804)
		at org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId0(SQLExecution.scala:77)
		at org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:233)
		at org.apache.spark.sql.execution.QueryExecution.$anonfun$eagerlyExecuteCommands$1(QueryExecution.scala:155)
		at org.apache.spark.sql.execution.QueryExecution$.withInternalError(QueryExecution.scala:654)
		at org.apache.spark.sql.execution.QueryExecution.org$apache$spark$sql$execution$QueryExecution$$eagerlyExecute$1(QueryExecution.scala:154)
		at org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$3.applyOrElse(QueryExecution.scala:169)
		at org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$3.applyOrElse(QueryExecution.scala:164)
		at org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$transformDownWithPruning$1(TreeNode.scala:470)
		at org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(origin.scala:86)
		at org.apache.spark.sql.catalyst.trees.TreeNode.transformDownWithPruning(TreeNode.scala:470)
		at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.org$apache$spark$sql$catalyst$plans$logical$AnalysisHelper$$super$transformDownWithPruning(LogicalPlan.scala:37)
		at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning(AnalysisHelper.scala:360)
		at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning$(AnalysisHelper.scala:356)
		at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:37)
		at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:37)
		at org.apache.spark.sql.catalyst.trees.TreeNode.transformDown(TreeNode.scala:446)
		at org.apache.spark.sql.execution.QueryExecution.eagerlyExecuteCommands(QueryExecution.scala:164)
		at org.apache.spark.sql.execution.QueryExecution.$anonfun$lazyCommandExecuted$1(QueryExecution.scala:126)
		at scala.util.Try$.apply(Try.scala:217)
		at org.apache.spark.util.Utils$.doTryWithCallerStacktrace(Utils.scala:1378)
		at org.apache.spark.util.LazyTry.tryT$lzycompute(LazyTry.scala:46)
		at org.apache.spark.util.LazyTry.tryT(LazyTry.scala:46)
		... 19 more
Caused by: java.lang.ClassNotFoundException: Class org.apache.hadoop.fs.s3a.S3AFileSystem not found
	at org.apache.hadoop.conf.Configuration.getClassByName(Configuration.java:2641)
	at org.apache.hadoop.conf.Configuration.getClass(Configuration.java:2735)
	at org.apache.hadoop.fs.FileSystem.getFileSystemClass(FileSystem.java:3569)
	at org.apache.hadoop.fs.FileSystem.createFileSystem(FileSystem.java:3612)
	at org.apache.hadoop.fs.FileSystem.access$300(FileSystem.java:172)
	at org.apache.hadoop.fs.FileSystem$Cache.getInternal(FileSystem.java:3716)
	at org.apache.hadoop.fs.FileSystem$Cache.get(FileSystem.java:3667)
	at org.apache.hadoop.fs.FileSystem.get(FileSystem.java:557)
	at org.apache.hadoop.fs.Path.getFileSystem(Path.java:366)
	at org.apache.spark.sql.delta.DeltaLog$.apply(DeltaLog.scala:966)
	at org.apache.spark.sql.delta.DeltaLog$.forTable(DeltaLog.scala:801)
	at org.apache.spark.sql.delta.sources.DeltaDataSource.createRelation(DeltaDataSource.scala:197)
	at org.apache.spark.sql.execution.datasources.SaveIntoDataSourceCommand.run(SaveIntoDataSourceCommand.scala:55)
	at org.apache.spark.sql.execution.command.ExecutedCommandExec.sideEffectResult$lzycompute(commands.scala:79)
	at org.apache.spark.sql.execution.command.ExecutedCommandExec.sideEffectResult(commands.scala:77)
	at org.apache.spark.sql.execution.command.ExecutedCommandExec.executeCollect(commands.scala:88)
	at org.apache.spark.sql.execution.QueryExecution.$anonfun$eagerlyExecuteCommands$2(QueryExecution.scala:155)
	at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId0$8(SQLExecution.scala:162)
	at org.apache.spark.sql.execution.SQLExecution$.withSessionTagsApplied(SQLExecution.scala:268)
	at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId0$7(SQLExecution.scala:124)
	at org.apache.spark.JobArtifactSet$.withActiveJobArtifactState(JobArtifactSet.scala:94)
	at org.apache.spark.sql.artifact.ArtifactManager.$anonfun$withResources$1(ArtifactManager.scala:112)
	at org.apache.spark.sql.artifact.ArtifactManager.withClassLoaderIfNeeded(ArtifactManager.scala:106)
	at org.apache.spark.sql.artifact.ArtifactManager.withResources(ArtifactManager.scala:111)
	at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId0$6(SQLExecution.scala:124)
	at org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:291)
	at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId0$1(SQLExecution.scala:123)
	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:804)
	at org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId0(SQLExecution.scala:77)
	at org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:233)
	at org.apache.spark.sql.execution.QueryExecution.$anonfun$eagerlyExecuteCommands$1(QueryExecution.scala:155)
	at org.apache.spark.sql.execution.QueryExecution$.withInternalError(QueryExecution.scala:654)
	at org.apache.spark.sql.execution.QueryExecution.org$apache$spark$sql$execution$QueryExecution$$eagerlyExecute$1(QueryExecution.scala:154)
	at org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$3.applyOrElse(QueryExecution.scala:169)
	at org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$3.applyOrElse(QueryExecution.scala:164)
	at org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$transformDownWithPruning$1(TreeNode.scala:470)
	at org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(origin.scala:86)
	at org.apache.spark.sql.catalyst.trees.TreeNode.transformDownWithPruning(TreeNode.scala:470)
	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.org$apache$spark$sql$catalyst$plans$logical$AnalysisHelper$$super$transformDownWithPruning(LogicalPlan.scala:37)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning(AnalysisHelper.scala:360)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning$(AnalysisHelper.scala:356)
	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:37)
	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:37)
	at org.apache.spark.sql.catalyst.trees.TreeNode.transformDown(TreeNode.scala:446)
	at org.apache.spark.sql.execution.QueryExecution.eagerlyExecuteCommands(QueryExecution.scala:164)
	at org.apache.spark.sql.execution.QueryExecution.$anonfun$lazyCommandExecuted$1(QueryExecution.scala:126)
	at scala.util.Try$.apply(Try.scala:217)
	at org.apache.spark.util.Utils$.doTryWithCallerStacktrace(Utils.scala:1378)
	at org.apache.spark.util.LazyTry.tryT$lzycompute(LazyTry.scala:46)
	at org.apache.spark.util.LazyTry.tryT(LazyTry.scala:46)
	... 19 more
2025-07-23 10:06:19,756 INFO: Closing down clientserver connection
2025-07-23 10:06:43,835 INFO: Script /home/matteo/PycharmProjects/Tesi/BDA_Scripts/druid/druid_ingestion.sh eseguito con successo.
2025-07-23 10:06:45,035 ERROR: Traceback (most recent call last):
2025-07-23 10:06:45,036 ERROR: File "/home/matteo/PycharmProjects/Tesi/BDA_Scripts/retention_area/8_dw_feeder.py", line 234, in <module>
2025-07-23 10:06:45,036 ERROR: backup_bronze(spark, claims, access_key, secret_key, endpoint, s3_path)
2025-07-23 10:06:45,036 ERROR: File "/home/matteo/PycharmProjects/Tesi/BDA_Scripts/retention_area/8_dw_feeder.py", line 45, in backup_bronze
2025-07-23 10:06:45,036 ERROR: df_current.write.format("delta").mode("append").partitionBy("ingestion_date").save(s3_path)
2025-07-23 10:06:45,036 ERROR: File "/home/matteo/PycharmProjects/Tesi/.venv1/lib/python3.12/site-packages/pyspark/sql/readwriter.py", line 1745, in save
2025-07-23 10:06:45,036 ERROR: self._jwrite.save(path)
2025-07-23 10:06:45,036 ERROR: File "/home/matteo/PycharmProjects/Tesi/.venv1/lib/python3.12/site-packages/py4j/java_gateway.py", line 1362, in __call__
2025-07-23 10:06:45,036 ERROR: return_value = get_return_value(
2025-07-23 10:06:45,036 ERROR: ^
2025-07-23 10:06:45,036 ERROR: ^
2025-07-23 10:06:45,036 ERROR: ^
2025-07-23 10:06:45,036 ERROR: ^
2025-07-23 10:06:45,036 ERROR: ^
2025-07-23 10:06:45,036 ERROR: ^
2025-07-23 10:06:45,036 ERROR: ^
2025-07-23 10:06:45,036 ERROR: ^
2025-07-23 10:06:45,036 ERROR: ^
2025-07-23 10:06:45,036 ERROR: ^
2025-07-23 10:06:45,036 ERROR: ^
2025-07-23 10:06:45,036 ERROR: ^
2025-07-23 10:06:45,036 ERROR: ^
2025-07-23 10:06:45,036 ERROR: ^
2025-07-23 10:06:45,036 ERROR: ^
2025-07-23 10:06:45,036 ERROR: ^
2025-07-23 10:06:45,036 ERROR: ^
2025-07-23 10:06:45,037 ERROR: File "/home/matteo/PycharmProjects/Tesi/.venv1/lib/python3.12/site-packages/pyspark/errors/exceptions/captured.py", line 282, in deco
2025-07-23 10:06:45,037 ERROR: return f(*a, **kw)
2025-07-23 10:06:45,037 ERROR: ^
2025-07-23 10:06:45,037 ERROR: ^
2025-07-23 10:06:45,037 ERROR: ^
2025-07-23 10:06:45,037 ERROR: ^
2025-07-23 10:06:45,037 ERROR: ^
2025-07-23 10:06:45,037 ERROR: ^
2025-07-23 10:06:45,037 ERROR: ^
2025-07-23 10:06:45,037 ERROR: ^
2025-07-23 10:06:45,037 ERROR: ^
2025-07-23 10:06:45,037 ERROR: ^
2025-07-23 10:06:45,037 ERROR: ^
2025-07-23 10:06:45,037 ERROR: File "/home/matteo/PycharmProjects/Tesi/.venv1/lib/python3.12/site-packages/py4j/protocol.py", line 327, in get_return_value
2025-07-23 10:06:45,037 ERROR: raise Py4JJavaError(
2025-07-23 10:06:45,037 ERROR: py4j.protocol
2025-07-23 10:06:45,037 ERROR: .
2025-07-23 10:06:45,037 ERROR: Py4JJavaError
2025-07-23 10:06:45,078 ERROR: :
2025-07-23 10:06:45,078 ERROR: An error occurred while calling o60.save.
: java.lang.RuntimeException: java.lang.ClassNotFoundException: Class org.apache.hadoop.fs.s3a.S3AFileSystem not found
	at org.apache.hadoop.conf.Configuration.getClass(Configuration.java:2737)
	at org.apache.hadoop.fs.FileSystem.getFileSystemClass(FileSystem.java:3569)
	at org.apache.hadoop.fs.FileSystem.createFileSystem(FileSystem.java:3612)
	at org.apache.hadoop.fs.FileSystem.access$300(FileSystem.java:172)
	at org.apache.hadoop.fs.FileSystem$Cache.getInternal(FileSystem.java:3716)
	at org.apache.hadoop.fs.FileSystem$Cache.get(FileSystem.java:3667)
	at org.apache.hadoop.fs.FileSystem.get(FileSystem.java:557)
	at org.apache.hadoop.fs.Path.getFileSystem(Path.java:366)
	at org.apache.spark.sql.delta.DeltaLog$.apply(DeltaLog.scala:966)
	at org.apache.spark.sql.delta.DeltaLog$.forTable(DeltaLog.scala:801)
	at org.apache.spark.sql.delta.sources.DeltaDataSource.createRelation(DeltaDataSource.scala:197)
	at org.apache.spark.sql.execution.datasources.SaveIntoDataSourceCommand.run(SaveIntoDataSourceCommand.scala:55)
	at org.apache.spark.sql.execution.command.ExecutedCommandExec.sideEffectResult$lzycompute(commands.scala:79)
	at org.apache.spark.sql.execution.command.ExecutedCommandExec.sideEffectResult(commands.scala:77)
	at org.apache.spark.sql.execution.command.ExecutedCommandExec.executeCollect(commands.scala:88)
	at org.apache.spark.sql.execution.QueryExecution.$anonfun$eagerlyExecuteCommands$2(QueryExecution.scala:155)
	at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId0$8(SQLExecution.scala:162)
	at org.apache.spark.sql.execution.SQLExecution$.withSessionTagsApplied(SQLExecution.scala:268)
	at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId0$7(SQLExecution.scala:124)
	at org.apache.spark.JobArtifactSet$.withActiveJobArtifactState(JobArtifactSet.scala:94)
	at org.apache.spark.sql.artifact.ArtifactManager.$anonfun$withResources$1(ArtifactManager.scala:112)
	at org.apache.spark.sql.artifact.ArtifactManager.withClassLoaderIfNeeded(ArtifactManager.scala:106)
	at org.apache.spark.sql.artifact.ArtifactManager.withResources(ArtifactManager.scala:111)
	at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId0$6(SQLExecution.scala:124)
	at org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:291)
	at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId0$1(SQLExecution.scala:123)
	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:804)
	at org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId0(SQLExecution.scala:77)
	at org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:233)
	at org.apache.spark.sql.execution.QueryExecution.$anonfun$eagerlyExecuteCommands$1(QueryExecution.scala:155)
	at org.apache.spark.sql.execution.QueryExecution$.withInternalError(QueryExecution.scala:654)
	at org.apache.spark.sql.execution.QueryExecution.org$apache$spark$sql$execution$QueryExecution$$eagerlyExecute$1(QueryExecution.scala:154)
	at org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$3.applyOrElse(QueryExecution.scala:169)
	at org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$3.applyOrElse(QueryExecution.scala:164)
	at org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$transformDownWithPruning$1(TreeNode.scala:470)
	at org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(origin.scala:86)
	at org.apache.spark.sql.catalyst.trees.TreeNode.transformDownWithPruning(TreeNode.scala:470)
	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.org$apache$spark$sql$catalyst$plans$logical$AnalysisHelper$$super$transformDownWithPruning(LogicalPlan.scala:37)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning(AnalysisHelper.scala:360)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning$(AnalysisHelper.scala:356)
	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:37)
	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:37)
	at org.apache.spark.sql.catalyst.trees.TreeNode.transformDown(TreeNode.scala:446)
	at org.apache.spark.sql.execution.QueryExecution.eagerlyExecuteCommands(QueryExecution.scala:164)
	at org.apache.spark.sql.execution.QueryExecution.$anonfun$lazyCommandExecuted$1(QueryExecution.scala:126)
	at scala.util.Try$.apply(Try.scala:217)
	at org.apache.spark.util.Utils$.doTryWithCallerStacktrace(Utils.scala:1378)
	at org.apache.spark.util.Utils$.getTryWithCallerStacktrace(Utils.scala:1439)
	at org.apache.spark.util.LazyTry.get(LazyTry.scala:58)
	at org.apache.spark.sql.execution.QueryExecution.commandExecuted(QueryExecution.scala:131)
	at org.apache.spark.sql.execution.QueryExecution.assertCommandExecuted(QueryExecution.scala:192)
	at org.apache.spark.sql.classic.DataFrameWriter.runCommand(DataFrameWriter.scala:622)
	at org.apache.spark.sql.classic.DataFrameWriter.saveToV1Source(DataFrameWriter.scala:273)
	at org.apache.spark.sql.classic.DataFrameWriter.saveInternal(DataFrameWriter.scala:182)
	at org.apache.spark.sql.classic.DataFrameWriter.save(DataFrameWriter.scala:118)
	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:77)
	at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.base/java.lang.reflect.Method.invoke(Method.java:569)
	at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)
	at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)
	at py4j.Gateway.invoke(Gateway.java:282)
	at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)
	at py4j.commands.CallCommand.execute(CallCommand.java:79)
	at py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:184)
	at py4j.ClientServerConnection.run(ClientServerConnection.java:108)
	at java.base/java.lang.Thread.run(Thread.java:840)
	Suppressed: org.apache.spark.util.Utils$OriginalTryStackTraceException: Full stacktrace of original doTryWithCallerStacktrace caller
		at org.apache.hadoop.conf.Configuration.getClass(Configuration.java:2737)
		at org.apache.hadoop.fs.FileSystem.getFileSystemClass(FileSystem.java:3569)
		at org.apache.hadoop.fs.FileSystem.createFileSystem(FileSystem.java:3612)
		at org.apache.hadoop.fs.FileSystem.access$300(FileSystem.java:172)
		at org.apache.hadoop.fs.FileSystem$Cache.getInternal(FileSystem.java:3716)
		at org.apache.hadoop.fs.FileSystem$Cache.get(FileSystem.java:3667)
		at org.apache.hadoop.fs.FileSystem.get(FileSystem.java:557)
		at org.apache.hadoop.fs.Path.getFileSystem(Path.java:366)
		at org.apache.spark.sql.delta.DeltaLog$.apply(DeltaLog.scala:966)
		at org.apache.spark.sql.delta.DeltaLog$.forTable(DeltaLog.scala:801)
		at org.apache.spark.sql.delta.sources.DeltaDataSource.createRelation(DeltaDataSource.scala:197)
		at org.apache.spark.sql.execution.datasources.SaveIntoDataSourceCommand.run(SaveIntoDataSourceCommand.scala:55)
		at org.apache.spark.sql.execution.command.ExecutedCommandExec.sideEffectResult$lzycompute(commands.scala:79)
		at org.apache.spark.sql.execution.command.ExecutedCommandExec.sideEffectResult(commands.scala:77)
		at org.apache.spark.sql.execution.command.ExecutedCommandExec.executeCollect(commands.scala:88)
		at org.apache.spark.sql.execution.QueryExecution.$anonfun$eagerlyExecuteCommands$2(QueryExecution.scala:155)
		at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId0$8(SQLExecution.scala:162)
		at org.apache.spark.sql.execution.SQLExecution$.withSessionTagsApplied(SQLExecution.scala:268)
		at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId0$7(SQLExecution.scala:124)
		at org.apache.spark.JobArtifactSet$.withActiveJobArtifactState(JobArtifactSet.scala:94)
		at org.apache.spark.sql.artifact.ArtifactManager.$anonfun$withResources$1(ArtifactManager.scala:112)
		at org.apache.spark.sql.artifact.ArtifactManager.withClassLoaderIfNeeded(ArtifactManager.scala:106)
		at org.apache.spark.sql.artifact.ArtifactManager.withResources(ArtifactManager.scala:111)
		at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId0$6(SQLExecution.scala:124)
		at org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:291)
		at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId0$1(SQLExecution.scala:123)
		at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:804)
		at org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId0(SQLExecution.scala:77)
		at org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:233)
		at org.apache.spark.sql.execution.QueryExecution.$anonfun$eagerlyExecuteCommands$1(QueryExecution.scala:155)
		at org.apache.spark.sql.execution.QueryExecution$.withInternalError(QueryExecution.scala:654)
		at org.apache.spark.sql.execution.QueryExecution.org$apache$spark$sql$execution$QueryExecution$$eagerlyExecute$1(QueryExecution.scala:154)
		at org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$3.applyOrElse(QueryExecution.scala:169)
		at org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$3.applyOrElse(QueryExecution.scala:164)
		at org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$transformDownWithPruning$1(TreeNode.scala:470)
		at org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(origin.scala:86)
		at org.apache.spark.sql.catalyst.trees.TreeNode.transformDownWithPruning(TreeNode.scala:470)
		at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.org$apache$spark$sql$catalyst$plans$logical$AnalysisHelper$$super$transformDownWithPruning(LogicalPlan.scala:37)
		at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning(AnalysisHelper.scala:360)
		at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning$(AnalysisHelper.scala:356)
		at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:37)
		at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:37)
		at org.apache.spark.sql.catalyst.trees.TreeNode.transformDown(TreeNode.scala:446)
		at org.apache.spark.sql.execution.QueryExecution.eagerlyExecuteCommands(QueryExecution.scala:164)
		at org.apache.spark.sql.execution.QueryExecution.$anonfun$lazyCommandExecuted$1(QueryExecution.scala:126)
		at scala.util.Try$.apply(Try.scala:217)
		at org.apache.spark.util.Utils$.doTryWithCallerStacktrace(Utils.scala:1378)
		at org.apache.spark.util.LazyTry.tryT$lzycompute(LazyTry.scala:46)
		at org.apache.spark.util.LazyTry.tryT(LazyTry.scala:46)
		... 19 more
Caused by: java.lang.ClassNotFoundException: Class org.apache.hadoop.fs.s3a.S3AFileSystem not found
	at org.apache.hadoop.conf.Configuration.getClassByName(Configuration.java:2641)
	at org.apache.hadoop.conf.Configuration.getClass(Configuration.java:2735)
	at org.apache.hadoop.fs.FileSystem.getFileSystemClass(FileSystem.java:3569)
	at org.apache.hadoop.fs.FileSystem.createFileSystem(FileSystem.java:3612)
	at org.apache.hadoop.fs.FileSystem.access$300(FileSystem.java:172)
	at org.apache.hadoop.fs.FileSystem$Cache.getInternal(FileSystem.java:3716)
	at org.apache.hadoop.fs.FileSystem$Cache.get(FileSystem.java:3667)
	at org.apache.hadoop.fs.FileSystem.get(FileSystem.java:557)
	at org.apache.hadoop.fs.Path.getFileSystem(Path.java:366)
	at org.apache.spark.sql.delta.DeltaLog$.apply(DeltaLog.scala:966)
	at org.apache.spark.sql.delta.DeltaLog$.forTable(DeltaLog.scala:801)
	at org.apache.spark.sql.delta.sources.DeltaDataSource.createRelation(DeltaDataSource.scala:197)
	at org.apache.spark.sql.execution.datasources.SaveIntoDataSourceCommand.run(SaveIntoDataSourceCommand.scala:55)
	at org.apache.spark.sql.execution.command.ExecutedCommandExec.sideEffectResult$lzycompute(commands.scala:79)
	at org.apache.spark.sql.execution.command.ExecutedCommandExec.sideEffectResult(commands.scala:77)
	at org.apache.spark.sql.execution.command.ExecutedCommandExec.executeCollect(commands.scala:88)
	at org.apache.spark.sql.execution.QueryExecution.$anonfun$eagerlyExecuteCommands$2(QueryExecution.scala:155)
	at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId0$8(SQLExecution.scala:162)
	at org.apache.spark.sql.execution.SQLExecution$.withSessionTagsApplied(SQLExecution.scala:268)
	at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId0$7(SQLExecution.scala:124)
	at org.apache.spark.JobArtifactSet$.withActiveJobArtifactState(JobArtifactSet.scala:94)
	at org.apache.spark.sql.artifact.ArtifactManager.$anonfun$withResources$1(ArtifactManager.scala:112)
	at org.apache.spark.sql.artifact.ArtifactManager.withClassLoaderIfNeeded(ArtifactManager.scala:106)
	at org.apache.spark.sql.artifact.ArtifactManager.withResources(ArtifactManager.scala:111)
	at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId0$6(SQLExecution.scala:124)
	at org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:291)
	at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId0$1(SQLExecution.scala:123)
	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:804)
	at org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId0(SQLExecution.scala:77)
	at org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:233)
	at org.apache.spark.sql.execution.QueryExecution.$anonfun$eagerlyExecuteCommands$1(QueryExecution.scala:155)
	at org.apache.spark.sql.execution.QueryExecution$.withInternalError(QueryExecution.scala:654)
	at org.apache.spark.sql.execution.QueryExecution.org$apache$spark$sql$execution$QueryExecution$$eagerlyExecute$1(QueryExecution.scala:154)
	at org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$3.applyOrElse(QueryExecution.scala:169)
	at org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$3.applyOrElse(QueryExecution.scala:164)
	at org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$transformDownWithPruning$1(TreeNode.scala:470)
	at org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(origin.scala:86)
	at org.apache.spark.sql.catalyst.trees.TreeNode.transformDownWithPruning(TreeNode.scala:470)
	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.org$apache$spark$sql$catalyst$plans$logical$AnalysisHelper$$super$transformDownWithPruning(LogicalPlan.scala:37)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning(AnalysisHelper.scala:360)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning$(AnalysisHelper.scala:356)
	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:37)
	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:37)
	at org.apache.spark.sql.catalyst.trees.TreeNode.transformDown(TreeNode.scala:446)
	at org.apache.spark.sql.execution.QueryExecution.eagerlyExecuteCommands(QueryExecution.scala:164)
	at org.apache.spark.sql.execution.QueryExecution.$anonfun$lazyCommandExecuted$1(QueryExecution.scala:126)
	at scala.util.Try$.apply(Try.scala:217)
	at org.apache.spark.util.Utils$.doTryWithCallerStacktrace(Utils.scala:1378)
	at org.apache.spark.util.LazyTry.tryT$lzycompute(LazyTry.scala:46)
	at org.apache.spark.util.LazyTry.tryT(LazyTry.scala:46)
	... 19 more
2025-07-23 10:06:45,078 INFO: Closing down clientserver connection
2025-07-23 10:36:37,672 INFO: Script /home/matteo/PycharmProjects/Tesi/BDA_Scripts/druid/druid_ingestion.sh eseguito con successo.
2025-07-23 10:36:38,798 ERROR: Traceback (most recent call last):
2025-07-23 10:36:38,798 ERROR: File "/home/matteo/PycharmProjects/Tesi/BDA_Scripts/retention_area/8_dw_feeder.py", line 234, in <module>
2025-07-23 10:36:38,798 ERROR: backup_bronze(spark, claims, access_key, secret_key, endpoint, s3_path)
2025-07-23 10:36:38,798 ERROR: File "/home/matteo/PycharmProjects/Tesi/BDA_Scripts/retention_area/8_dw_feeder.py", line 45, in backup_bronze
2025-07-23 10:36:38,798 ERROR: df_current.write.format("delta").mode("append").partitionBy("ingestion_date").save(s3_path)
2025-07-23 10:36:38,798 ERROR: File "/home/matteo/PycharmProjects/Tesi/.venv1/lib/python3.12/site-packages/pyspark/sql/readwriter.py", line 1745, in save
2025-07-23 10:36:38,798 ERROR: self._jwrite.save(path)
2025-07-23 10:36:38,798 ERROR: File "/home/matteo/PycharmProjects/Tesi/.venv1/lib/python3.12/site-packages/py4j/java_gateway.py", line 1362, in __call__
2025-07-23 10:36:38,799 ERROR: return_value = get_return_value(
2025-07-23 10:36:38,799 ERROR: ^
2025-07-23 10:36:38,799 ERROR: ^
2025-07-23 10:36:38,799 ERROR: ^
2025-07-23 10:36:38,799 ERROR: ^
2025-07-23 10:36:38,799 ERROR: ^
2025-07-23 10:36:38,799 ERROR: ^
2025-07-23 10:36:38,799 ERROR: ^
2025-07-23 10:36:38,799 ERROR: ^
2025-07-23 10:36:38,799 ERROR: ^
2025-07-23 10:36:38,799 ERROR: ^
2025-07-23 10:36:38,799 ERROR: ^
2025-07-23 10:36:38,799 ERROR: ^
2025-07-23 10:36:38,799 ERROR: ^
2025-07-23 10:36:38,799 ERROR: ^
2025-07-23 10:36:38,799 ERROR: ^
2025-07-23 10:36:38,799 ERROR: ^
2025-07-23 10:36:38,799 ERROR: ^
2025-07-23 10:36:38,799 ERROR: File "/home/matteo/PycharmProjects/Tesi/.venv1/lib/python3.12/site-packages/pyspark/errors/exceptions/captured.py", line 282, in deco
2025-07-23 10:36:38,799 ERROR: return f(*a, **kw)
2025-07-23 10:36:38,799 ERROR: ^
2025-07-23 10:36:38,799 ERROR: ^
2025-07-23 10:36:38,799 ERROR: ^
2025-07-23 10:36:38,799 ERROR: ^
2025-07-23 10:36:38,799 ERROR: ^
2025-07-23 10:36:38,799 ERROR: ^
2025-07-23 10:36:38,799 ERROR: ^
2025-07-23 10:36:38,799 ERROR: ^
2025-07-23 10:36:38,799 ERROR: ^
2025-07-23 10:36:38,799 ERROR: ^
2025-07-23 10:36:38,799 ERROR: ^
2025-07-23 10:36:38,799 ERROR: File "/home/matteo/PycharmProjects/Tesi/.venv1/lib/python3.12/site-packages/py4j/protocol.py", line 327, in get_return_value
2025-07-23 10:36:38,799 ERROR: raise Py4JJavaError(
2025-07-23 10:36:38,799 ERROR: py4j.protocol
2025-07-23 10:36:38,799 ERROR: .
2025-07-23 10:36:38,799 ERROR: Py4JJavaError
2025-07-23 10:36:38,840 ERROR: :
2025-07-23 10:36:38,840 ERROR: An error occurred while calling o60.save.
: java.lang.RuntimeException: java.lang.ClassNotFoundException: Class org.apache.hadoop.fs.s3a.S3AFileSystem not found
	at org.apache.hadoop.conf.Configuration.getClass(Configuration.java:2737)
	at org.apache.hadoop.fs.FileSystem.getFileSystemClass(FileSystem.java:3569)
	at org.apache.hadoop.fs.FileSystem.createFileSystem(FileSystem.java:3612)
	at org.apache.hadoop.fs.FileSystem.access$300(FileSystem.java:172)
	at org.apache.hadoop.fs.FileSystem$Cache.getInternal(FileSystem.java:3716)
	at org.apache.hadoop.fs.FileSystem$Cache.get(FileSystem.java:3667)
	at org.apache.hadoop.fs.FileSystem.get(FileSystem.java:557)
	at org.apache.hadoop.fs.Path.getFileSystem(Path.java:366)
	at org.apache.spark.sql.delta.DeltaLog$.apply(DeltaLog.scala:966)
	at org.apache.spark.sql.delta.DeltaLog$.forTable(DeltaLog.scala:801)
	at org.apache.spark.sql.delta.sources.DeltaDataSource.createRelation(DeltaDataSource.scala:197)
	at org.apache.spark.sql.execution.datasources.SaveIntoDataSourceCommand.run(SaveIntoDataSourceCommand.scala:55)
	at org.apache.spark.sql.execution.command.ExecutedCommandExec.sideEffectResult$lzycompute(commands.scala:79)
	at org.apache.spark.sql.execution.command.ExecutedCommandExec.sideEffectResult(commands.scala:77)
	at org.apache.spark.sql.execution.command.ExecutedCommandExec.executeCollect(commands.scala:88)
	at org.apache.spark.sql.execution.QueryExecution.$anonfun$eagerlyExecuteCommands$2(QueryExecution.scala:155)
	at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId0$8(SQLExecution.scala:162)
	at org.apache.spark.sql.execution.SQLExecution$.withSessionTagsApplied(SQLExecution.scala:268)
	at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId0$7(SQLExecution.scala:124)
	at org.apache.spark.JobArtifactSet$.withActiveJobArtifactState(JobArtifactSet.scala:94)
	at org.apache.spark.sql.artifact.ArtifactManager.$anonfun$withResources$1(ArtifactManager.scala:112)
	at org.apache.spark.sql.artifact.ArtifactManager.withClassLoaderIfNeeded(ArtifactManager.scala:106)
	at org.apache.spark.sql.artifact.ArtifactManager.withResources(ArtifactManager.scala:111)
	at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId0$6(SQLExecution.scala:124)
	at org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:291)
	at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId0$1(SQLExecution.scala:123)
	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:804)
	at org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId0(SQLExecution.scala:77)
	at org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:233)
	at org.apache.spark.sql.execution.QueryExecution.$anonfun$eagerlyExecuteCommands$1(QueryExecution.scala:155)
	at org.apache.spark.sql.execution.QueryExecution$.withInternalError(QueryExecution.scala:654)
	at org.apache.spark.sql.execution.QueryExecution.org$apache$spark$sql$execution$QueryExecution$$eagerlyExecute$1(QueryExecution.scala:154)
	at org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$3.applyOrElse(QueryExecution.scala:169)
	at org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$3.applyOrElse(QueryExecution.scala:164)
	at org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$transformDownWithPruning$1(TreeNode.scala:470)
	at org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(origin.scala:86)
	at org.apache.spark.sql.catalyst.trees.TreeNode.transformDownWithPruning(TreeNode.scala:470)
	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.org$apache$spark$sql$catalyst$plans$logical$AnalysisHelper$$super$transformDownWithPruning(LogicalPlan.scala:37)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning(AnalysisHelper.scala:360)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning$(AnalysisHelper.scala:356)
	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:37)
	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:37)
	at org.apache.spark.sql.catalyst.trees.TreeNode.transformDown(TreeNode.scala:446)
	at org.apache.spark.sql.execution.QueryExecution.eagerlyExecuteCommands(QueryExecution.scala:164)
	at org.apache.spark.sql.execution.QueryExecution.$anonfun$lazyCommandExecuted$1(QueryExecution.scala:126)
	at scala.util.Try$.apply(Try.scala:217)
	at org.apache.spark.util.Utils$.doTryWithCallerStacktrace(Utils.scala:1378)
	at org.apache.spark.util.Utils$.getTryWithCallerStacktrace(Utils.scala:1439)
	at org.apache.spark.util.LazyTry.get(LazyTry.scala:58)
	at org.apache.spark.sql.execution.QueryExecution.commandExecuted(QueryExecution.scala:131)
	at org.apache.spark.sql.execution.QueryExecution.assertCommandExecuted(QueryExecution.scala:192)
	at org.apache.spark.sql.classic.DataFrameWriter.runCommand(DataFrameWriter.scala:622)
	at org.apache.spark.sql.classic.DataFrameWriter.saveToV1Source(DataFrameWriter.scala:273)
	at org.apache.spark.sql.classic.DataFrameWriter.saveInternal(DataFrameWriter.scala:182)
	at org.apache.spark.sql.classic.DataFrameWriter.save(DataFrameWriter.scala:118)
	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:77)
	at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.base/java.lang.reflect.Method.invoke(Method.java:569)
	at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)
	at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)
	at py4j.Gateway.invoke(Gateway.java:282)
	at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)
	at py4j.commands.CallCommand.execute(CallCommand.java:79)
	at py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:184)
	at py4j.ClientServerConnection.run(ClientServerConnection.java:108)
	at java.base/java.lang.Thread.run(Thread.java:840)
	Suppressed: org.apache.spark.util.Utils$OriginalTryStackTraceException: Full stacktrace of original doTryWithCallerStacktrace caller
		at org.apache.hadoop.conf.Configuration.getClass(Configuration.java:2737)
		at org.apache.hadoop.fs.FileSystem.getFileSystemClass(FileSystem.java:3569)
		at org.apache.hadoop.fs.FileSystem.createFileSystem(FileSystem.java:3612)
		at org.apache.hadoop.fs.FileSystem.access$300(FileSystem.java:172)
		at org.apache.hadoop.fs.FileSystem$Cache.getInternal(FileSystem.java:3716)
		at org.apache.hadoop.fs.FileSystem$Cache.get(FileSystem.java:3667)
		at org.apache.hadoop.fs.FileSystem.get(FileSystem.java:557)
		at org.apache.hadoop.fs.Path.getFileSystem(Path.java:366)
		at org.apache.spark.sql.delta.DeltaLog$.apply(DeltaLog.scala:966)
		at org.apache.spark.sql.delta.DeltaLog$.forTable(DeltaLog.scala:801)
		at org.apache.spark.sql.delta.sources.DeltaDataSource.createRelation(DeltaDataSource.scala:197)
		at org.apache.spark.sql.execution.datasources.SaveIntoDataSourceCommand.run(SaveIntoDataSourceCommand.scala:55)
		at org.apache.spark.sql.execution.command.ExecutedCommandExec.sideEffectResult$lzycompute(commands.scala:79)
		at org.apache.spark.sql.execution.command.ExecutedCommandExec.sideEffectResult(commands.scala:77)
		at org.apache.spark.sql.execution.command.ExecutedCommandExec.executeCollect(commands.scala:88)
		at org.apache.spark.sql.execution.QueryExecution.$anonfun$eagerlyExecuteCommands$2(QueryExecution.scala:155)
		at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId0$8(SQLExecution.scala:162)
		at org.apache.spark.sql.execution.SQLExecution$.withSessionTagsApplied(SQLExecution.scala:268)
		at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId0$7(SQLExecution.scala:124)
		at org.apache.spark.JobArtifactSet$.withActiveJobArtifactState(JobArtifactSet.scala:94)
		at org.apache.spark.sql.artifact.ArtifactManager.$anonfun$withResources$1(ArtifactManager.scala:112)
		at org.apache.spark.sql.artifact.ArtifactManager.withClassLoaderIfNeeded(ArtifactManager.scala:106)
		at org.apache.spark.sql.artifact.ArtifactManager.withResources(ArtifactManager.scala:111)
		at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId0$6(SQLExecution.scala:124)
		at org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:291)
		at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId0$1(SQLExecution.scala:123)
		at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:804)
		at org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId0(SQLExecution.scala:77)
		at org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:233)
		at org.apache.spark.sql.execution.QueryExecution.$anonfun$eagerlyExecuteCommands$1(QueryExecution.scala:155)
		at org.apache.spark.sql.execution.QueryExecution$.withInternalError(QueryExecution.scala:654)
		at org.apache.spark.sql.execution.QueryExecution.org$apache$spark$sql$execution$QueryExecution$$eagerlyExecute$1(QueryExecution.scala:154)
		at org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$3.applyOrElse(QueryExecution.scala:169)
		at org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$3.applyOrElse(QueryExecution.scala:164)
		at org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$transformDownWithPruning$1(TreeNode.scala:470)
		at org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(origin.scala:86)
		at org.apache.spark.sql.catalyst.trees.TreeNode.transformDownWithPruning(TreeNode.scala:470)
		at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.org$apache$spark$sql$catalyst$plans$logical$AnalysisHelper$$super$transformDownWithPruning(LogicalPlan.scala:37)
		at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning(AnalysisHelper.scala:360)
		at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning$(AnalysisHelper.scala:356)
		at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:37)
		at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:37)
		at org.apache.spark.sql.catalyst.trees.TreeNode.transformDown(TreeNode.scala:446)
		at org.apache.spark.sql.execution.QueryExecution.eagerlyExecuteCommands(QueryExecution.scala:164)
		at org.apache.spark.sql.execution.QueryExecution.$anonfun$lazyCommandExecuted$1(QueryExecution.scala:126)
		at scala.util.Try$.apply(Try.scala:217)
		at org.apache.spark.util.Utils$.doTryWithCallerStacktrace(Utils.scala:1378)
		at org.apache.spark.util.LazyTry.tryT$lzycompute(LazyTry.scala:46)
		at org.apache.spark.util.LazyTry.tryT(LazyTry.scala:46)
		... 19 more
Caused by: java.lang.ClassNotFoundException: Class org.apache.hadoop.fs.s3a.S3AFileSystem not found
	at org.apache.hadoop.conf.Configuration.getClassByName(Configuration.java:2641)
	at org.apache.hadoop.conf.Configuration.getClass(Configuration.java:2735)
	at org.apache.hadoop.fs.FileSystem.getFileSystemClass(FileSystem.java:3569)
	at org.apache.hadoop.fs.FileSystem.createFileSystem(FileSystem.java:3612)
	at org.apache.hadoop.fs.FileSystem.access$300(FileSystem.java:172)
	at org.apache.hadoop.fs.FileSystem$Cache.getInternal(FileSystem.java:3716)
	at org.apache.hadoop.fs.FileSystem$Cache.get(FileSystem.java:3667)
	at org.apache.hadoop.fs.FileSystem.get(FileSystem.java:557)
	at org.apache.hadoop.fs.Path.getFileSystem(Path.java:366)
	at org.apache.spark.sql.delta.DeltaLog$.apply(DeltaLog.scala:966)
	at org.apache.spark.sql.delta.DeltaLog$.forTable(DeltaLog.scala:801)
	at org.apache.spark.sql.delta.sources.DeltaDataSource.createRelation(DeltaDataSource.scala:197)
	at org.apache.spark.sql.execution.datasources.SaveIntoDataSourceCommand.run(SaveIntoDataSourceCommand.scala:55)
	at org.apache.spark.sql.execution.command.ExecutedCommandExec.sideEffectResult$lzycompute(commands.scala:79)
	at org.apache.spark.sql.execution.command.ExecutedCommandExec.sideEffectResult(commands.scala:77)
	at org.apache.spark.sql.execution.command.ExecutedCommandExec.executeCollect(commands.scala:88)
	at org.apache.spark.sql.execution.QueryExecution.$anonfun$eagerlyExecuteCommands$2(QueryExecution.scala:155)
	at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId0$8(SQLExecution.scala:162)
	at org.apache.spark.sql.execution.SQLExecution$.withSessionTagsApplied(SQLExecution.scala:268)
	at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId0$7(SQLExecution.scala:124)
	at org.apache.spark.JobArtifactSet$.withActiveJobArtifactState(JobArtifactSet.scala:94)
	at org.apache.spark.sql.artifact.ArtifactManager.$anonfun$withResources$1(ArtifactManager.scala:112)
	at org.apache.spark.sql.artifact.ArtifactManager.withClassLoaderIfNeeded(ArtifactManager.scala:106)
	at org.apache.spark.sql.artifact.ArtifactManager.withResources(ArtifactManager.scala:111)
	at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId0$6(SQLExecution.scala:124)
	at org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:291)
	at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId0$1(SQLExecution.scala:123)
	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:804)
	at org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId0(SQLExecution.scala:77)
	at org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:233)
	at org.apache.spark.sql.execution.QueryExecution.$anonfun$eagerlyExecuteCommands$1(QueryExecution.scala:155)
	at org.apache.spark.sql.execution.QueryExecution$.withInternalError(QueryExecution.scala:654)
	at org.apache.spark.sql.execution.QueryExecution.org$apache$spark$sql$execution$QueryExecution$$eagerlyExecute$1(QueryExecution.scala:154)
	at org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$3.applyOrElse(QueryExecution.scala:169)
	at org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$3.applyOrElse(QueryExecution.scala:164)
	at org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$transformDownWithPruning$1(TreeNode.scala:470)
	at org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(origin.scala:86)
	at org.apache.spark.sql.catalyst.trees.TreeNode.transformDownWithPruning(TreeNode.scala:470)
	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.org$apache$spark$sql$catalyst$plans$logical$AnalysisHelper$$super$transformDownWithPruning(LogicalPlan.scala:37)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning(AnalysisHelper.scala:360)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning$(AnalysisHelper.scala:356)
	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:37)
	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:37)
	at org.apache.spark.sql.catalyst.trees.TreeNode.transformDown(TreeNode.scala:446)
	at org.apache.spark.sql.execution.QueryExecution.eagerlyExecuteCommands(QueryExecution.scala:164)
	at org.apache.spark.sql.execution.QueryExecution.$anonfun$lazyCommandExecuted$1(QueryExecution.scala:126)
	at scala.util.Try$.apply(Try.scala:217)
	at org.apache.spark.util.Utils$.doTryWithCallerStacktrace(Utils.scala:1378)
	at org.apache.spark.util.LazyTry.tryT$lzycompute(LazyTry.scala:46)
	at org.apache.spark.util.LazyTry.tryT(LazyTry.scala:46)
	... 19 more
2025-07-23 10:36:38,840 INFO: Closing down clientserver connection
2025-07-23 10:50:41,396 INFO: Script /home/matteo/PycharmProjects/Tesi/BDA_Scripts/druid/druid_ingestion.sh eseguito con successo.
2025-07-23 10:50:42,553 ERROR: Traceback (most recent call last):
2025-07-23 10:50:42,553 ERROR: File "/home/matteo/PycharmProjects/Tesi/BDA_Scripts/retention_area/8_dw_feeder.py", line 234, in <module>
2025-07-23 10:50:42,553 ERROR: backup_bronze(spark, claims, access_key, secret_key, endpoint, s3_path)
2025-07-23 10:50:42,553 ERROR: File "/home/matteo/PycharmProjects/Tesi/BDA_Scripts/retention_area/8_dw_feeder.py", line 45, in backup_bronze
2025-07-23 10:50:42,554 ERROR: df_current.write.format("delta").mode("append").partitionBy("ingestion_date").save(s3_path)
2025-07-23 10:50:42,554 ERROR: File "/home/matteo/PycharmProjects/Tesi/.venv1/lib/python3.12/site-packages/pyspark/sql/readwriter.py", line 1745, in save
2025-07-23 10:50:42,554 ERROR: self._jwrite.save(path)
2025-07-23 10:50:42,554 ERROR: File "/home/matteo/PycharmProjects/Tesi/.venv1/lib/python3.12/site-packages/py4j/java_gateway.py", line 1362, in __call__
2025-07-23 10:50:42,554 ERROR: return_value = get_return_value(
2025-07-23 10:50:42,554 ERROR: ^
2025-07-23 10:50:42,554 ERROR: ^
2025-07-23 10:50:42,554 ERROR: ^
2025-07-23 10:50:42,554 ERROR: ^
2025-07-23 10:50:42,554 ERROR: ^
2025-07-23 10:50:42,554 ERROR: ^
2025-07-23 10:50:42,554 ERROR: ^
2025-07-23 10:50:42,554 ERROR: ^
2025-07-23 10:50:42,554 ERROR: ^
2025-07-23 10:50:42,554 ERROR: ^
2025-07-23 10:50:42,554 ERROR: ^
2025-07-23 10:50:42,555 ERROR: ^
2025-07-23 10:50:42,555 ERROR: ^
2025-07-23 10:50:42,555 ERROR: ^
2025-07-23 10:50:42,555 ERROR: ^
2025-07-23 10:50:42,555 ERROR: ^
2025-07-23 10:50:42,555 ERROR: ^
2025-07-23 10:50:42,555 ERROR: File "/home/matteo/PycharmProjects/Tesi/.venv1/lib/python3.12/site-packages/pyspark/errors/exceptions/captured.py", line 282, in deco
2025-07-23 10:50:42,555 ERROR: return f(*a, **kw)
2025-07-23 10:50:42,555 ERROR: ^
2025-07-23 10:50:42,555 ERROR: ^
2025-07-23 10:50:42,555 ERROR: ^
2025-07-23 10:50:42,555 ERROR: ^
2025-07-23 10:50:42,555 ERROR: ^
2025-07-23 10:50:42,555 ERROR: ^
2025-07-23 10:50:42,555 ERROR: ^
2025-07-23 10:50:42,555 ERROR: ^
2025-07-23 10:50:42,555 ERROR: ^
2025-07-23 10:50:42,555 ERROR: ^
2025-07-23 10:50:42,555 ERROR: ^
2025-07-23 10:50:42,555 ERROR: File "/home/matteo/PycharmProjects/Tesi/.venv1/lib/python3.12/site-packages/py4j/protocol.py", line 327, in get_return_value
2025-07-23 10:50:42,555 ERROR: raise Py4JJavaError(
2025-07-23 10:50:42,555 ERROR: py4j.protocol
2025-07-23 10:50:42,555 ERROR: .
2025-07-23 10:50:42,555 ERROR: Py4JJavaError
2025-07-23 10:50:42,598 ERROR: :
2025-07-23 10:50:42,598 ERROR: An error occurred while calling o60.save.
: java.lang.RuntimeException: java.lang.ClassNotFoundException: Class org.apache.hadoop.fs.s3a.S3AFileSystem not found
	at org.apache.hadoop.conf.Configuration.getClass(Configuration.java:2737)
	at org.apache.hadoop.fs.FileSystem.getFileSystemClass(FileSystem.java:3569)
	at org.apache.hadoop.fs.FileSystem.createFileSystem(FileSystem.java:3612)
	at org.apache.hadoop.fs.FileSystem.access$300(FileSystem.java:172)
	at org.apache.hadoop.fs.FileSystem$Cache.getInternal(FileSystem.java:3716)
	at org.apache.hadoop.fs.FileSystem$Cache.get(FileSystem.java:3667)
	at org.apache.hadoop.fs.FileSystem.get(FileSystem.java:557)
	at org.apache.hadoop.fs.Path.getFileSystem(Path.java:366)
	at org.apache.spark.sql.delta.DeltaLog$.apply(DeltaLog.scala:966)
	at org.apache.spark.sql.delta.DeltaLog$.forTable(DeltaLog.scala:801)
	at org.apache.spark.sql.delta.sources.DeltaDataSource.createRelation(DeltaDataSource.scala:197)
	at org.apache.spark.sql.execution.datasources.SaveIntoDataSourceCommand.run(SaveIntoDataSourceCommand.scala:55)
	at org.apache.spark.sql.execution.command.ExecutedCommandExec.sideEffectResult$lzycompute(commands.scala:79)
	at org.apache.spark.sql.execution.command.ExecutedCommandExec.sideEffectResult(commands.scala:77)
	at org.apache.spark.sql.execution.command.ExecutedCommandExec.executeCollect(commands.scala:88)
	at org.apache.spark.sql.execution.QueryExecution.$anonfun$eagerlyExecuteCommands$2(QueryExecution.scala:155)
	at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId0$8(SQLExecution.scala:162)
	at org.apache.spark.sql.execution.SQLExecution$.withSessionTagsApplied(SQLExecution.scala:268)
	at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId0$7(SQLExecution.scala:124)
	at org.apache.spark.JobArtifactSet$.withActiveJobArtifactState(JobArtifactSet.scala:94)
	at org.apache.spark.sql.artifact.ArtifactManager.$anonfun$withResources$1(ArtifactManager.scala:112)
	at org.apache.spark.sql.artifact.ArtifactManager.withClassLoaderIfNeeded(ArtifactManager.scala:106)
	at org.apache.spark.sql.artifact.ArtifactManager.withResources(ArtifactManager.scala:111)
	at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId0$6(SQLExecution.scala:124)
	at org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:291)
	at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId0$1(SQLExecution.scala:123)
	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:804)
	at org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId0(SQLExecution.scala:77)
	at org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:233)
	at org.apache.spark.sql.execution.QueryExecution.$anonfun$eagerlyExecuteCommands$1(QueryExecution.scala:155)
	at org.apache.spark.sql.execution.QueryExecution$.withInternalError(QueryExecution.scala:654)
	at org.apache.spark.sql.execution.QueryExecution.org$apache$spark$sql$execution$QueryExecution$$eagerlyExecute$1(QueryExecution.scala:154)
	at org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$3.applyOrElse(QueryExecution.scala:169)
	at org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$3.applyOrElse(QueryExecution.scala:164)
	at org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$transformDownWithPruning$1(TreeNode.scala:470)
	at org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(origin.scala:86)
	at org.apache.spark.sql.catalyst.trees.TreeNode.transformDownWithPruning(TreeNode.scala:470)
	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.org$apache$spark$sql$catalyst$plans$logical$AnalysisHelper$$super$transformDownWithPruning(LogicalPlan.scala:37)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning(AnalysisHelper.scala:360)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning$(AnalysisHelper.scala:356)
	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:37)
	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:37)
	at org.apache.spark.sql.catalyst.trees.TreeNode.transformDown(TreeNode.scala:446)
	at org.apache.spark.sql.execution.QueryExecution.eagerlyExecuteCommands(QueryExecution.scala:164)
	at org.apache.spark.sql.execution.QueryExecution.$anonfun$lazyCommandExecuted$1(QueryExecution.scala:126)
	at scala.util.Try$.apply(Try.scala:217)
	at org.apache.spark.util.Utils$.doTryWithCallerStacktrace(Utils.scala:1378)
	at org.apache.spark.util.Utils$.getTryWithCallerStacktrace(Utils.scala:1439)
	at org.apache.spark.util.LazyTry.get(LazyTry.scala:58)
	at org.apache.spark.sql.execution.QueryExecution.commandExecuted(QueryExecution.scala:131)
	at org.apache.spark.sql.execution.QueryExecution.assertCommandExecuted(QueryExecution.scala:192)
	at org.apache.spark.sql.classic.DataFrameWriter.runCommand(DataFrameWriter.scala:622)
	at org.apache.spark.sql.classic.DataFrameWriter.saveToV1Source(DataFrameWriter.scala:273)
	at org.apache.spark.sql.classic.DataFrameWriter.saveInternal(DataFrameWriter.scala:182)
	at org.apache.spark.sql.classic.DataFrameWriter.save(DataFrameWriter.scala:118)
	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:77)
	at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.base/java.lang.reflect.Method.invoke(Method.java:569)
	at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)
	at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)
	at py4j.Gateway.invoke(Gateway.java:282)
	at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)
	at py4j.commands.CallCommand.execute(CallCommand.java:79)
	at py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:184)
	at py4j.ClientServerConnection.run(ClientServerConnection.java:108)
	at java.base/java.lang.Thread.run(Thread.java:840)
	Suppressed: org.apache.spark.util.Utils$OriginalTryStackTraceException: Full stacktrace of original doTryWithCallerStacktrace caller
		at org.apache.hadoop.conf.Configuration.getClass(Configuration.java:2737)
		at org.apache.hadoop.fs.FileSystem.getFileSystemClass(FileSystem.java:3569)
		at org.apache.hadoop.fs.FileSystem.createFileSystem(FileSystem.java:3612)
		at org.apache.hadoop.fs.FileSystem.access$300(FileSystem.java:172)
		at org.apache.hadoop.fs.FileSystem$Cache.getInternal(FileSystem.java:3716)
		at org.apache.hadoop.fs.FileSystem$Cache.get(FileSystem.java:3667)
		at org.apache.hadoop.fs.FileSystem.get(FileSystem.java:557)
		at org.apache.hadoop.fs.Path.getFileSystem(Path.java:366)
		at org.apache.spark.sql.delta.DeltaLog$.apply(DeltaLog.scala:966)
		at org.apache.spark.sql.delta.DeltaLog$.forTable(DeltaLog.scala:801)
		at org.apache.spark.sql.delta.sources.DeltaDataSource.createRelation(DeltaDataSource.scala:197)
		at org.apache.spark.sql.execution.datasources.SaveIntoDataSourceCommand.run(SaveIntoDataSourceCommand.scala:55)
		at org.apache.spark.sql.execution.command.ExecutedCommandExec.sideEffectResult$lzycompute(commands.scala:79)
		at org.apache.spark.sql.execution.command.ExecutedCommandExec.sideEffectResult(commands.scala:77)
		at org.apache.spark.sql.execution.command.ExecutedCommandExec.executeCollect(commands.scala:88)
		at org.apache.spark.sql.execution.QueryExecution.$anonfun$eagerlyExecuteCommands$2(QueryExecution.scala:155)
		at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId0$8(SQLExecution.scala:162)
		at org.apache.spark.sql.execution.SQLExecution$.withSessionTagsApplied(SQLExecution.scala:268)
		at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId0$7(SQLExecution.scala:124)
		at org.apache.spark.JobArtifactSet$.withActiveJobArtifactState(JobArtifactSet.scala:94)
		at org.apache.spark.sql.artifact.ArtifactManager.$anonfun$withResources$1(ArtifactManager.scala:112)
		at org.apache.spark.sql.artifact.ArtifactManager.withClassLoaderIfNeeded(ArtifactManager.scala:106)
		at org.apache.spark.sql.artifact.ArtifactManager.withResources(ArtifactManager.scala:111)
		at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId0$6(SQLExecution.scala:124)
		at org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:291)
		at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId0$1(SQLExecution.scala:123)
		at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:804)
		at org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId0(SQLExecution.scala:77)
		at org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:233)
		at org.apache.spark.sql.execution.QueryExecution.$anonfun$eagerlyExecuteCommands$1(QueryExecution.scala:155)
		at org.apache.spark.sql.execution.QueryExecution$.withInternalError(QueryExecution.scala:654)
		at org.apache.spark.sql.execution.QueryExecution.org$apache$spark$sql$execution$QueryExecution$$eagerlyExecute$1(QueryExecution.scala:154)
		at org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$3.applyOrElse(QueryExecution.scala:169)
		at org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$3.applyOrElse(QueryExecution.scala:164)
		at org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$transformDownWithPruning$1(TreeNode.scala:470)
		at org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(origin.scala:86)
		at org.apache.spark.sql.catalyst.trees.TreeNode.transformDownWithPruning(TreeNode.scala:470)
		at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.org$apache$spark$sql$catalyst$plans$logical$AnalysisHelper$$super$transformDownWithPruning(LogicalPlan.scala:37)
		at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning(AnalysisHelper.scala:360)
		at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning$(AnalysisHelper.scala:356)
		at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:37)
		at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:37)
		at org.apache.spark.sql.catalyst.trees.TreeNode.transformDown(TreeNode.scala:446)
		at org.apache.spark.sql.execution.QueryExecution.eagerlyExecuteCommands(QueryExecution.scala:164)
		at org.apache.spark.sql.execution.QueryExecution.$anonfun$lazyCommandExecuted$1(QueryExecution.scala:126)
		at scala.util.Try$.apply(Try.scala:217)
		at org.apache.spark.util.Utils$.doTryWithCallerStacktrace(Utils.scala:1378)
		at org.apache.spark.util.LazyTry.tryT$lzycompute(LazyTry.scala:46)
		at org.apache.spark.util.LazyTry.tryT(LazyTry.scala:46)
		... 19 more
Caused by: java.lang.ClassNotFoundException: Class org.apache.hadoop.fs.s3a.S3AFileSystem not found
	at org.apache.hadoop.conf.Configuration.getClassByName(Configuration.java:2641)
	at org.apache.hadoop.conf.Configuration.getClass(Configuration.java:2735)
	at org.apache.hadoop.fs.FileSystem.getFileSystemClass(FileSystem.java:3569)
	at org.apache.hadoop.fs.FileSystem.createFileSystem(FileSystem.java:3612)
	at org.apache.hadoop.fs.FileSystem.access$300(FileSystem.java:172)
	at org.apache.hadoop.fs.FileSystem$Cache.getInternal(FileSystem.java:3716)
	at org.apache.hadoop.fs.FileSystem$Cache.get(FileSystem.java:3667)
	at org.apache.hadoop.fs.FileSystem.get(FileSystem.java:557)
	at org.apache.hadoop.fs.Path.getFileSystem(Path.java:366)
	at org.apache.spark.sql.delta.DeltaLog$.apply(DeltaLog.scala:966)
	at org.apache.spark.sql.delta.DeltaLog$.forTable(DeltaLog.scala:801)
	at org.apache.spark.sql.delta.sources.DeltaDataSource.createRelation(DeltaDataSource.scala:197)
	at org.apache.spark.sql.execution.datasources.SaveIntoDataSourceCommand.run(SaveIntoDataSourceCommand.scala:55)
	at org.apache.spark.sql.execution.command.ExecutedCommandExec.sideEffectResult$lzycompute(commands.scala:79)
	at org.apache.spark.sql.execution.command.ExecutedCommandExec.sideEffectResult(commands.scala:77)
	at org.apache.spark.sql.execution.command.ExecutedCommandExec.executeCollect(commands.scala:88)
	at org.apache.spark.sql.execution.QueryExecution.$anonfun$eagerlyExecuteCommands$2(QueryExecution.scala:155)
	at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId0$8(SQLExecution.scala:162)
	at org.apache.spark.sql.execution.SQLExecution$.withSessionTagsApplied(SQLExecution.scala:268)
	at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId0$7(SQLExecution.scala:124)
	at org.apache.spark.JobArtifactSet$.withActiveJobArtifactState(JobArtifactSet.scala:94)
	at org.apache.spark.sql.artifact.ArtifactManager.$anonfun$withResources$1(ArtifactManager.scala:112)
	at org.apache.spark.sql.artifact.ArtifactManager.withClassLoaderIfNeeded(ArtifactManager.scala:106)
	at org.apache.spark.sql.artifact.ArtifactManager.withResources(ArtifactManager.scala:111)
	at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId0$6(SQLExecution.scala:124)
	at org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:291)
	at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId0$1(SQLExecution.scala:123)
	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:804)
	at org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId0(SQLExecution.scala:77)
	at org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:233)
	at org.apache.spark.sql.execution.QueryExecution.$anonfun$eagerlyExecuteCommands$1(QueryExecution.scala:155)
	at org.apache.spark.sql.execution.QueryExecution$.withInternalError(QueryExecution.scala:654)
	at org.apache.spark.sql.execution.QueryExecution.org$apache$spark$sql$execution$QueryExecution$$eagerlyExecute$1(QueryExecution.scala:154)
	at org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$3.applyOrElse(QueryExecution.scala:169)
	at org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$3.applyOrElse(QueryExecution.scala:164)
	at org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$transformDownWithPruning$1(TreeNode.scala:470)
	at org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(origin.scala:86)
	at org.apache.spark.sql.catalyst.trees.TreeNode.transformDownWithPruning(TreeNode.scala:470)
	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.org$apache$spark$sql$catalyst$plans$logical$AnalysisHelper$$super$transformDownWithPruning(LogicalPlan.scala:37)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning(AnalysisHelper.scala:360)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning$(AnalysisHelper.scala:356)
	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:37)
	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:37)
	at org.apache.spark.sql.catalyst.trees.TreeNode.transformDown(TreeNode.scala:446)
	at org.apache.spark.sql.execution.QueryExecution.eagerlyExecuteCommands(QueryExecution.scala:164)
	at org.apache.spark.sql.execution.QueryExecution.$anonfun$lazyCommandExecuted$1(QueryExecution.scala:126)
	at scala.util.Try$.apply(Try.scala:217)
	at org.apache.spark.util.Utils$.doTryWithCallerStacktrace(Utils.scala:1378)
	at org.apache.spark.util.LazyTry.tryT$lzycompute(LazyTry.scala:46)
	at org.apache.spark.util.LazyTry.tryT(LazyTry.scala:46)
	... 19 more
2025-07-23 10:50:42,598 INFO: Closing down clientserver connection
2025-07-23 11:02:29,967 INFO: Script /home/matteo/PycharmProjects/Tesi/BDA_Scripts/druid/druid_ingestion.sh eseguito con successo.
2025-07-23 11:02:31,090 ERROR: Traceback (most recent call last):
2025-07-23 11:02:31,090 ERROR: File "/home/matteo/PycharmProjects/Tesi/BDA_Scripts/retention_area/8_dw_feeder.py", line 234, in <module>
2025-07-23 11:02:31,090 ERROR: backup_bronze(spark, claims, access_key, secret_key, endpoint, s3_path)
2025-07-23 11:02:31,090 ERROR: File "/home/matteo/PycharmProjects/Tesi/BDA_Scripts/retention_area/8_dw_feeder.py", line 45, in backup_bronze
2025-07-23 11:02:31,090 ERROR: df_current.write.format("delta").mode("append").partitionBy("ingestion_date").save(s3_path)
2025-07-23 11:02:31,090 ERROR: File "/home/matteo/PycharmProjects/Tesi/.venv1/lib/python3.12/site-packages/pyspark/sql/readwriter.py", line 1745, in save
2025-07-23 11:02:31,091 ERROR: self._jwrite.save(path)
2025-07-23 11:02:31,091 ERROR: File "/home/matteo/PycharmProjects/Tesi/.venv1/lib/python3.12/site-packages/py4j/java_gateway.py", line 1362, in __call__
2025-07-23 11:02:31,091 ERROR: return_value = get_return_value(
2025-07-23 11:02:31,091 ERROR: ^
2025-07-23 11:02:31,091 ERROR: ^
2025-07-23 11:02:31,091 ERROR: ^
2025-07-23 11:02:31,091 ERROR: ^
2025-07-23 11:02:31,091 ERROR: ^
2025-07-23 11:02:31,091 ERROR: ^
2025-07-23 11:02:31,091 ERROR: ^
2025-07-23 11:02:31,091 ERROR: ^
2025-07-23 11:02:31,091 ERROR: ^
2025-07-23 11:02:31,091 ERROR: ^
2025-07-23 11:02:31,091 ERROR: ^
2025-07-23 11:02:31,091 ERROR: ^
2025-07-23 11:02:31,091 ERROR: ^
2025-07-23 11:02:31,091 ERROR: ^
2025-07-23 11:02:31,091 ERROR: ^
2025-07-23 11:02:31,091 ERROR: ^
2025-07-23 11:02:31,091 ERROR: ^
2025-07-23 11:02:31,091 ERROR: File "/home/matteo/PycharmProjects/Tesi/.venv1/lib/python3.12/site-packages/pyspark/errors/exceptions/captured.py", line 282, in deco
2025-07-23 11:02:31,092 ERROR: return f(*a, **kw)
2025-07-23 11:02:31,092 ERROR: ^
2025-07-23 11:02:31,092 ERROR: ^
2025-07-23 11:02:31,092 ERROR: ^
2025-07-23 11:02:31,092 ERROR: ^
2025-07-23 11:02:31,092 ERROR: ^
2025-07-23 11:02:31,092 ERROR: ^
2025-07-23 11:02:31,092 ERROR: ^
2025-07-23 11:02:31,092 ERROR: ^
2025-07-23 11:02:31,092 ERROR: ^
2025-07-23 11:02:31,092 ERROR: ^
2025-07-23 11:02:31,092 ERROR: ^
2025-07-23 11:02:31,092 ERROR: File "/home/matteo/PycharmProjects/Tesi/.venv1/lib/python3.12/site-packages/py4j/protocol.py", line 327, in get_return_value
2025-07-23 11:02:31,092 ERROR: raise Py4JJavaError(
2025-07-23 11:02:31,092 ERROR: py4j.protocol
2025-07-23 11:02:31,092 ERROR: .
2025-07-23 11:02:31,092 ERROR: Py4JJavaError
2025-07-23 11:02:31,133 ERROR: :
2025-07-23 11:02:31,133 ERROR: An error occurred while calling o60.save.
: java.lang.RuntimeException: java.lang.ClassNotFoundException: Class org.apache.hadoop.fs.s3a.S3AFileSystem not found
	at org.apache.hadoop.conf.Configuration.getClass(Configuration.java:2737)
	at org.apache.hadoop.fs.FileSystem.getFileSystemClass(FileSystem.java:3569)
	at org.apache.hadoop.fs.FileSystem.createFileSystem(FileSystem.java:3612)
	at org.apache.hadoop.fs.FileSystem.access$300(FileSystem.java:172)
	at org.apache.hadoop.fs.FileSystem$Cache.getInternal(FileSystem.java:3716)
	at org.apache.hadoop.fs.FileSystem$Cache.get(FileSystem.java:3667)
	at org.apache.hadoop.fs.FileSystem.get(FileSystem.java:557)
	at org.apache.hadoop.fs.Path.getFileSystem(Path.java:366)
	at org.apache.spark.sql.delta.DeltaLog$.apply(DeltaLog.scala:966)
	at org.apache.spark.sql.delta.DeltaLog$.forTable(DeltaLog.scala:801)
	at org.apache.spark.sql.delta.sources.DeltaDataSource.createRelation(DeltaDataSource.scala:197)
	at org.apache.spark.sql.execution.datasources.SaveIntoDataSourceCommand.run(SaveIntoDataSourceCommand.scala:55)
	at org.apache.spark.sql.execution.command.ExecutedCommandExec.sideEffectResult$lzycompute(commands.scala:79)
	at org.apache.spark.sql.execution.command.ExecutedCommandExec.sideEffectResult(commands.scala:77)
	at org.apache.spark.sql.execution.command.ExecutedCommandExec.executeCollect(commands.scala:88)
	at org.apache.spark.sql.execution.QueryExecution.$anonfun$eagerlyExecuteCommands$2(QueryExecution.scala:155)
	at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId0$8(SQLExecution.scala:162)
	at org.apache.spark.sql.execution.SQLExecution$.withSessionTagsApplied(SQLExecution.scala:268)
	at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId0$7(SQLExecution.scala:124)
	at org.apache.spark.JobArtifactSet$.withActiveJobArtifactState(JobArtifactSet.scala:94)
	at org.apache.spark.sql.artifact.ArtifactManager.$anonfun$withResources$1(ArtifactManager.scala:112)
	at org.apache.spark.sql.artifact.ArtifactManager.withClassLoaderIfNeeded(ArtifactManager.scala:106)
	at org.apache.spark.sql.artifact.ArtifactManager.withResources(ArtifactManager.scala:111)
	at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId0$6(SQLExecution.scala:124)
	at org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:291)
	at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId0$1(SQLExecution.scala:123)
	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:804)
	at org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId0(SQLExecution.scala:77)
	at org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:233)
	at org.apache.spark.sql.execution.QueryExecution.$anonfun$eagerlyExecuteCommands$1(QueryExecution.scala:155)
	at org.apache.spark.sql.execution.QueryExecution$.withInternalError(QueryExecution.scala:654)
	at org.apache.spark.sql.execution.QueryExecution.org$apache$spark$sql$execution$QueryExecution$$eagerlyExecute$1(QueryExecution.scala:154)
	at org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$3.applyOrElse(QueryExecution.scala:169)
	at org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$3.applyOrElse(QueryExecution.scala:164)
	at org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$transformDownWithPruning$1(TreeNode.scala:470)
	at org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(origin.scala:86)
	at org.apache.spark.sql.catalyst.trees.TreeNode.transformDownWithPruning(TreeNode.scala:470)
	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.org$apache$spark$sql$catalyst$plans$logical$AnalysisHelper$$super$transformDownWithPruning(LogicalPlan.scala:37)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning(AnalysisHelper.scala:360)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning$(AnalysisHelper.scala:356)
	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:37)
	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:37)
	at org.apache.spark.sql.catalyst.trees.TreeNode.transformDown(TreeNode.scala:446)
	at org.apache.spark.sql.execution.QueryExecution.eagerlyExecuteCommands(QueryExecution.scala:164)
	at org.apache.spark.sql.execution.QueryExecution.$anonfun$lazyCommandExecuted$1(QueryExecution.scala:126)
	at scala.util.Try$.apply(Try.scala:217)
	at org.apache.spark.util.Utils$.doTryWithCallerStacktrace(Utils.scala:1378)
	at org.apache.spark.util.Utils$.getTryWithCallerStacktrace(Utils.scala:1439)
	at org.apache.spark.util.LazyTry.get(LazyTry.scala:58)
	at org.apache.spark.sql.execution.QueryExecution.commandExecuted(QueryExecution.scala:131)
	at org.apache.spark.sql.execution.QueryExecution.assertCommandExecuted(QueryExecution.scala:192)
	at org.apache.spark.sql.classic.DataFrameWriter.runCommand(DataFrameWriter.scala:622)
	at org.apache.spark.sql.classic.DataFrameWriter.saveToV1Source(DataFrameWriter.scala:273)
	at org.apache.spark.sql.classic.DataFrameWriter.saveInternal(DataFrameWriter.scala:182)
	at org.apache.spark.sql.classic.DataFrameWriter.save(DataFrameWriter.scala:118)
	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:77)
	at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.base/java.lang.reflect.Method.invoke(Method.java:569)
	at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)
	at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)
	at py4j.Gateway.invoke(Gateway.java:282)
	at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)
	at py4j.commands.CallCommand.execute(CallCommand.java:79)
	at py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:184)
	at py4j.ClientServerConnection.run(ClientServerConnection.java:108)
	at java.base/java.lang.Thread.run(Thread.java:840)
	Suppressed: org.apache.spark.util.Utils$OriginalTryStackTraceException: Full stacktrace of original doTryWithCallerStacktrace caller
		at org.apache.hadoop.conf.Configuration.getClass(Configuration.java:2737)
		at org.apache.hadoop.fs.FileSystem.getFileSystemClass(FileSystem.java:3569)
		at org.apache.hadoop.fs.FileSystem.createFileSystem(FileSystem.java:3612)
		at org.apache.hadoop.fs.FileSystem.access$300(FileSystem.java:172)
		at org.apache.hadoop.fs.FileSystem$Cache.getInternal(FileSystem.java:3716)
		at org.apache.hadoop.fs.FileSystem$Cache.get(FileSystem.java:3667)
		at org.apache.hadoop.fs.FileSystem.get(FileSystem.java:557)
		at org.apache.hadoop.fs.Path.getFileSystem(Path.java:366)
		at org.apache.spark.sql.delta.DeltaLog$.apply(DeltaLog.scala:966)
		at org.apache.spark.sql.delta.DeltaLog$.forTable(DeltaLog.scala:801)
		at org.apache.spark.sql.delta.sources.DeltaDataSource.createRelation(DeltaDataSource.scala:197)
		at org.apache.spark.sql.execution.datasources.SaveIntoDataSourceCommand.run(SaveIntoDataSourceCommand.scala:55)
		at org.apache.spark.sql.execution.command.ExecutedCommandExec.sideEffectResult$lzycompute(commands.scala:79)
		at org.apache.spark.sql.execution.command.ExecutedCommandExec.sideEffectResult(commands.scala:77)
		at org.apache.spark.sql.execution.command.ExecutedCommandExec.executeCollect(commands.scala:88)
		at org.apache.spark.sql.execution.QueryExecution.$anonfun$eagerlyExecuteCommands$2(QueryExecution.scala:155)
		at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId0$8(SQLExecution.scala:162)
		at org.apache.spark.sql.execution.SQLExecution$.withSessionTagsApplied(SQLExecution.scala:268)
		at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId0$7(SQLExecution.scala:124)
		at org.apache.spark.JobArtifactSet$.withActiveJobArtifactState(JobArtifactSet.scala:94)
		at org.apache.spark.sql.artifact.ArtifactManager.$anonfun$withResources$1(ArtifactManager.scala:112)
		at org.apache.spark.sql.artifact.ArtifactManager.withClassLoaderIfNeeded(ArtifactManager.scala:106)
		at org.apache.spark.sql.artifact.ArtifactManager.withResources(ArtifactManager.scala:111)
		at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId0$6(SQLExecution.scala:124)
		at org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:291)
		at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId0$1(SQLExecution.scala:123)
		at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:804)
		at org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId0(SQLExecution.scala:77)
		at org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:233)
		at org.apache.spark.sql.execution.QueryExecution.$anonfun$eagerlyExecuteCommands$1(QueryExecution.scala:155)
		at org.apache.spark.sql.execution.QueryExecution$.withInternalError(QueryExecution.scala:654)
		at org.apache.spark.sql.execution.QueryExecution.org$apache$spark$sql$execution$QueryExecution$$eagerlyExecute$1(QueryExecution.scala:154)
		at org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$3.applyOrElse(QueryExecution.scala:169)
		at org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$3.applyOrElse(QueryExecution.scala:164)
		at org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$transformDownWithPruning$1(TreeNode.scala:470)
		at org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(origin.scala:86)
		at org.apache.spark.sql.catalyst.trees.TreeNode.transformDownWithPruning(TreeNode.scala:470)
		at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.org$apache$spark$sql$catalyst$plans$logical$AnalysisHelper$$super$transformDownWithPruning(LogicalPlan.scala:37)
		at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning(AnalysisHelper.scala:360)
		at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning$(AnalysisHelper.scala:356)
		at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:37)
		at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:37)
		at org.apache.spark.sql.catalyst.trees.TreeNode.transformDown(TreeNode.scala:446)
		at org.apache.spark.sql.execution.QueryExecution.eagerlyExecuteCommands(QueryExecution.scala:164)
		at org.apache.spark.sql.execution.QueryExecution.$anonfun$lazyCommandExecuted$1(QueryExecution.scala:126)
		at scala.util.Try$.apply(Try.scala:217)
		at org.apache.spark.util.Utils$.doTryWithCallerStacktrace(Utils.scala:1378)
		at org.apache.spark.util.LazyTry.tryT$lzycompute(LazyTry.scala:46)
		at org.apache.spark.util.LazyTry.tryT(LazyTry.scala:46)
		... 19 more
Caused by: java.lang.ClassNotFoundException: Class org.apache.hadoop.fs.s3a.S3AFileSystem not found
	at org.apache.hadoop.conf.Configuration.getClassByName(Configuration.java:2641)
	at org.apache.hadoop.conf.Configuration.getClass(Configuration.java:2735)
	at org.apache.hadoop.fs.FileSystem.getFileSystemClass(FileSystem.java:3569)
	at org.apache.hadoop.fs.FileSystem.createFileSystem(FileSystem.java:3612)
	at org.apache.hadoop.fs.FileSystem.access$300(FileSystem.java:172)
	at org.apache.hadoop.fs.FileSystem$Cache.getInternal(FileSystem.java:3716)
	at org.apache.hadoop.fs.FileSystem$Cache.get(FileSystem.java:3667)
	at org.apache.hadoop.fs.FileSystem.get(FileSystem.java:557)
	at org.apache.hadoop.fs.Path.getFileSystem(Path.java:366)
	at org.apache.spark.sql.delta.DeltaLog$.apply(DeltaLog.scala:966)
	at org.apache.spark.sql.delta.DeltaLog$.forTable(DeltaLog.scala:801)
	at org.apache.spark.sql.delta.sources.DeltaDataSource.createRelation(DeltaDataSource.scala:197)
	at org.apache.spark.sql.execution.datasources.SaveIntoDataSourceCommand.run(SaveIntoDataSourceCommand.scala:55)
	at org.apache.spark.sql.execution.command.ExecutedCommandExec.sideEffectResult$lzycompute(commands.scala:79)
	at org.apache.spark.sql.execution.command.ExecutedCommandExec.sideEffectResult(commands.scala:77)
	at org.apache.spark.sql.execution.command.ExecutedCommandExec.executeCollect(commands.scala:88)
	at org.apache.spark.sql.execution.QueryExecution.$anonfun$eagerlyExecuteCommands$2(QueryExecution.scala:155)
	at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId0$8(SQLExecution.scala:162)
	at org.apache.spark.sql.execution.SQLExecution$.withSessionTagsApplied(SQLExecution.scala:268)
	at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId0$7(SQLExecution.scala:124)
	at org.apache.spark.JobArtifactSet$.withActiveJobArtifactState(JobArtifactSet.scala:94)
	at org.apache.spark.sql.artifact.ArtifactManager.$anonfun$withResources$1(ArtifactManager.scala:112)
	at org.apache.spark.sql.artifact.ArtifactManager.withClassLoaderIfNeeded(ArtifactManager.scala:106)
	at org.apache.spark.sql.artifact.ArtifactManager.withResources(ArtifactManager.scala:111)
	at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId0$6(SQLExecution.scala:124)
	at org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:291)
	at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId0$1(SQLExecution.scala:123)
	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:804)
	at org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId0(SQLExecution.scala:77)
	at org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:233)
	at org.apache.spark.sql.execution.QueryExecution.$anonfun$eagerlyExecuteCommands$1(QueryExecution.scala:155)
	at org.apache.spark.sql.execution.QueryExecution$.withInternalError(QueryExecution.scala:654)
	at org.apache.spark.sql.execution.QueryExecution.org$apache$spark$sql$execution$QueryExecution$$eagerlyExecute$1(QueryExecution.scala:154)
	at org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$3.applyOrElse(QueryExecution.scala:169)
	at org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$3.applyOrElse(QueryExecution.scala:164)
	at org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$transformDownWithPruning$1(TreeNode.scala:470)
	at org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(origin.scala:86)
	at org.apache.spark.sql.catalyst.trees.TreeNode.transformDownWithPruning(TreeNode.scala:470)
	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.org$apache$spark$sql$catalyst$plans$logical$AnalysisHelper$$super$transformDownWithPruning(LogicalPlan.scala:37)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning(AnalysisHelper.scala:360)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning$(AnalysisHelper.scala:356)
	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:37)
	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:37)
	at org.apache.spark.sql.catalyst.trees.TreeNode.transformDown(TreeNode.scala:446)
	at org.apache.spark.sql.execution.QueryExecution.eagerlyExecuteCommands(QueryExecution.scala:164)
	at org.apache.spark.sql.execution.QueryExecution.$anonfun$lazyCommandExecuted$1(QueryExecution.scala:126)
	at scala.util.Try$.apply(Try.scala:217)
	at org.apache.spark.util.Utils$.doTryWithCallerStacktrace(Utils.scala:1378)
	at org.apache.spark.util.LazyTry.tryT$lzycompute(LazyTry.scala:46)
	at org.apache.spark.util.LazyTry.tryT(LazyTry.scala:46)
	... 19 more
2025-07-23 11:02:31,133 INFO: Closing down clientserver connection
2025-07-23 11:08:51,254 INFO: Script /home/matteo/PycharmProjects/Tesi/BDA_Scripts/druid/druid_ingestion.sh eseguito con successo.
2025-07-23 11:08:52,359 ERROR: Traceback (most recent call last):
2025-07-23 11:08:52,359 ERROR: File "/home/matteo/PycharmProjects/Tesi/BDA_Scripts/retention_area/8_dw_feeder.py", line 234, in <module>
2025-07-23 11:08:52,359 ERROR: backup_bronze(spark, claims, access_key, secret_key, endpoint, s3_path)
2025-07-23 11:08:52,359 ERROR: File "/home/matteo/PycharmProjects/Tesi/BDA_Scripts/retention_area/8_dw_feeder.py", line 45, in backup_bronze
2025-07-23 11:08:52,359 ERROR: df_current.write.format("delta").mode("append").partitionBy("ingestion_date").save(s3_path)
2025-07-23 11:08:52,359 ERROR: File "/home/matteo/PycharmProjects/Tesi/.venv1/lib/python3.12/site-packages/pyspark/sql/readwriter.py", line 1745, in save
2025-07-23 11:08:52,359 ERROR: self._jwrite.save(path)
2025-07-23 11:08:52,359 ERROR: File "/home/matteo/PycharmProjects/Tesi/.venv1/lib/python3.12/site-packages/py4j/java_gateway.py", line 1362, in __call__
2025-07-23 11:08:52,360 ERROR: return_value = get_return_value(
2025-07-23 11:08:52,360 ERROR: ^
2025-07-23 11:08:52,360 ERROR: ^
2025-07-23 11:08:52,360 ERROR: ^
2025-07-23 11:08:52,360 ERROR: ^
2025-07-23 11:08:52,360 ERROR: ^
2025-07-23 11:08:52,360 ERROR: ^
2025-07-23 11:08:52,360 ERROR: ^
2025-07-23 11:08:52,360 ERROR: ^
2025-07-23 11:08:52,360 ERROR: ^
2025-07-23 11:08:52,360 ERROR: ^
2025-07-23 11:08:52,360 ERROR: ^
2025-07-23 11:08:52,360 ERROR: ^
2025-07-23 11:08:52,360 ERROR: ^
2025-07-23 11:08:52,360 ERROR: ^
2025-07-23 11:08:52,360 ERROR: ^
2025-07-23 11:08:52,360 ERROR: ^
2025-07-23 11:08:52,360 ERROR: ^
2025-07-23 11:08:52,360 ERROR: File "/home/matteo/PycharmProjects/Tesi/.venv1/lib/python3.12/site-packages/pyspark/errors/exceptions/captured.py", line 282, in deco
2025-07-23 11:08:52,360 ERROR: return f(*a, **kw)
2025-07-23 11:08:52,360 ERROR: ^
2025-07-23 11:08:52,360 ERROR: ^
2025-07-23 11:08:52,360 ERROR: ^
2025-07-23 11:08:52,360 ERROR: ^
2025-07-23 11:08:52,360 ERROR: ^
2025-07-23 11:08:52,360 ERROR: ^
2025-07-23 11:08:52,360 ERROR: ^
2025-07-23 11:08:52,360 ERROR: ^
2025-07-23 11:08:52,360 ERROR: ^
2025-07-23 11:08:52,360 ERROR: ^
2025-07-23 11:08:52,360 ERROR: ^
2025-07-23 11:08:52,360 ERROR: File "/home/matteo/PycharmProjects/Tesi/.venv1/lib/python3.12/site-packages/py4j/protocol.py", line 327, in get_return_value
2025-07-23 11:08:52,360 ERROR: raise Py4JJavaError(
2025-07-23 11:08:52,360 ERROR: py4j.protocol
2025-07-23 11:08:52,360 ERROR: .
2025-07-23 11:08:52,360 ERROR: Py4JJavaError
2025-07-23 11:08:52,401 ERROR: :
2025-07-23 11:08:52,401 ERROR: An error occurred while calling o60.save.
: java.lang.RuntimeException: java.lang.ClassNotFoundException: Class org.apache.hadoop.fs.s3a.S3AFileSystem not found
	at org.apache.hadoop.conf.Configuration.getClass(Configuration.java:2737)
	at org.apache.hadoop.fs.FileSystem.getFileSystemClass(FileSystem.java:3569)
	at org.apache.hadoop.fs.FileSystem.createFileSystem(FileSystem.java:3612)
	at org.apache.hadoop.fs.FileSystem.access$300(FileSystem.java:172)
	at org.apache.hadoop.fs.FileSystem$Cache.getInternal(FileSystem.java:3716)
	at org.apache.hadoop.fs.FileSystem$Cache.get(FileSystem.java:3667)
	at org.apache.hadoop.fs.FileSystem.get(FileSystem.java:557)
	at org.apache.hadoop.fs.Path.getFileSystem(Path.java:366)
	at org.apache.spark.sql.delta.DeltaLog$.apply(DeltaLog.scala:966)
	at org.apache.spark.sql.delta.DeltaLog$.forTable(DeltaLog.scala:801)
	at org.apache.spark.sql.delta.sources.DeltaDataSource.createRelation(DeltaDataSource.scala:197)
	at org.apache.spark.sql.execution.datasources.SaveIntoDataSourceCommand.run(SaveIntoDataSourceCommand.scala:55)
	at org.apache.spark.sql.execution.command.ExecutedCommandExec.sideEffectResult$lzycompute(commands.scala:79)
	at org.apache.spark.sql.execution.command.ExecutedCommandExec.sideEffectResult(commands.scala:77)
	at org.apache.spark.sql.execution.command.ExecutedCommandExec.executeCollect(commands.scala:88)
	at org.apache.spark.sql.execution.QueryExecution.$anonfun$eagerlyExecuteCommands$2(QueryExecution.scala:155)
	at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId0$8(SQLExecution.scala:162)
	at org.apache.spark.sql.execution.SQLExecution$.withSessionTagsApplied(SQLExecution.scala:268)
	at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId0$7(SQLExecution.scala:124)
	at org.apache.spark.JobArtifactSet$.withActiveJobArtifactState(JobArtifactSet.scala:94)
	at org.apache.spark.sql.artifact.ArtifactManager.$anonfun$withResources$1(ArtifactManager.scala:112)
	at org.apache.spark.sql.artifact.ArtifactManager.withClassLoaderIfNeeded(ArtifactManager.scala:106)
	at org.apache.spark.sql.artifact.ArtifactManager.withResources(ArtifactManager.scala:111)
	at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId0$6(SQLExecution.scala:124)
	at org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:291)
	at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId0$1(SQLExecution.scala:123)
	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:804)
	at org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId0(SQLExecution.scala:77)
	at org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:233)
	at org.apache.spark.sql.execution.QueryExecution.$anonfun$eagerlyExecuteCommands$1(QueryExecution.scala:155)
	at org.apache.spark.sql.execution.QueryExecution$.withInternalError(QueryExecution.scala:654)
	at org.apache.spark.sql.execution.QueryExecution.org$apache$spark$sql$execution$QueryExecution$$eagerlyExecute$1(QueryExecution.scala:154)
	at org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$3.applyOrElse(QueryExecution.scala:169)
	at org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$3.applyOrElse(QueryExecution.scala:164)
	at org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$transformDownWithPruning$1(TreeNode.scala:470)
	at org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(origin.scala:86)
	at org.apache.spark.sql.catalyst.trees.TreeNode.transformDownWithPruning(TreeNode.scala:470)
	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.org$apache$spark$sql$catalyst$plans$logical$AnalysisHelper$$super$transformDownWithPruning(LogicalPlan.scala:37)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning(AnalysisHelper.scala:360)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning$(AnalysisHelper.scala:356)
	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:37)
	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:37)
	at org.apache.spark.sql.catalyst.trees.TreeNode.transformDown(TreeNode.scala:446)
	at org.apache.spark.sql.execution.QueryExecution.eagerlyExecuteCommands(QueryExecution.scala:164)
	at org.apache.spark.sql.execution.QueryExecution.$anonfun$lazyCommandExecuted$1(QueryExecution.scala:126)
	at scala.util.Try$.apply(Try.scala:217)
	at org.apache.spark.util.Utils$.doTryWithCallerStacktrace(Utils.scala:1378)
	at org.apache.spark.util.Utils$.getTryWithCallerStacktrace(Utils.scala:1439)
	at org.apache.spark.util.LazyTry.get(LazyTry.scala:58)
	at org.apache.spark.sql.execution.QueryExecution.commandExecuted(QueryExecution.scala:131)
	at org.apache.spark.sql.execution.QueryExecution.assertCommandExecuted(QueryExecution.scala:192)
	at org.apache.spark.sql.classic.DataFrameWriter.runCommand(DataFrameWriter.scala:622)
	at org.apache.spark.sql.classic.DataFrameWriter.saveToV1Source(DataFrameWriter.scala:273)
	at org.apache.spark.sql.classic.DataFrameWriter.saveInternal(DataFrameWriter.scala:182)
	at org.apache.spark.sql.classic.DataFrameWriter.save(DataFrameWriter.scala:118)
	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:77)
	at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.base/java.lang.reflect.Method.invoke(Method.java:569)
	at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)
	at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)
	at py4j.Gateway.invoke(Gateway.java:282)
	at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)
	at py4j.commands.CallCommand.execute(CallCommand.java:79)
	at py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:184)
	at py4j.ClientServerConnection.run(ClientServerConnection.java:108)
	at java.base/java.lang.Thread.run(Thread.java:840)
	Suppressed: org.apache.spark.util.Utils$OriginalTryStackTraceException: Full stacktrace of original doTryWithCallerStacktrace caller
		at org.apache.hadoop.conf.Configuration.getClass(Configuration.java:2737)
		at org.apache.hadoop.fs.FileSystem.getFileSystemClass(FileSystem.java:3569)
		at org.apache.hadoop.fs.FileSystem.createFileSystem(FileSystem.java:3612)
		at org.apache.hadoop.fs.FileSystem.access$300(FileSystem.java:172)
		at org.apache.hadoop.fs.FileSystem$Cache.getInternal(FileSystem.java:3716)
		at org.apache.hadoop.fs.FileSystem$Cache.get(FileSystem.java:3667)
		at org.apache.hadoop.fs.FileSystem.get(FileSystem.java:557)
		at org.apache.hadoop.fs.Path.getFileSystem(Path.java:366)
		at org.apache.spark.sql.delta.DeltaLog$.apply(DeltaLog.scala:966)
		at org.apache.spark.sql.delta.DeltaLog$.forTable(DeltaLog.scala:801)
		at org.apache.spark.sql.delta.sources.DeltaDataSource.createRelation(DeltaDataSource.scala:197)
		at org.apache.spark.sql.execution.datasources.SaveIntoDataSourceCommand.run(SaveIntoDataSourceCommand.scala:55)
		at org.apache.spark.sql.execution.command.ExecutedCommandExec.sideEffectResult$lzycompute(commands.scala:79)
		at org.apache.spark.sql.execution.command.ExecutedCommandExec.sideEffectResult(commands.scala:77)
		at org.apache.spark.sql.execution.command.ExecutedCommandExec.executeCollect(commands.scala:88)
		at org.apache.spark.sql.execution.QueryExecution.$anonfun$eagerlyExecuteCommands$2(QueryExecution.scala:155)
		at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId0$8(SQLExecution.scala:162)
		at org.apache.spark.sql.execution.SQLExecution$.withSessionTagsApplied(SQLExecution.scala:268)
		at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId0$7(SQLExecution.scala:124)
		at org.apache.spark.JobArtifactSet$.withActiveJobArtifactState(JobArtifactSet.scala:94)
		at org.apache.spark.sql.artifact.ArtifactManager.$anonfun$withResources$1(ArtifactManager.scala:112)
		at org.apache.spark.sql.artifact.ArtifactManager.withClassLoaderIfNeeded(ArtifactManager.scala:106)
		at org.apache.spark.sql.artifact.ArtifactManager.withResources(ArtifactManager.scala:111)
		at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId0$6(SQLExecution.scala:124)
		at org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:291)
		at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId0$1(SQLExecution.scala:123)
		at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:804)
		at org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId0(SQLExecution.scala:77)
		at org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:233)
		at org.apache.spark.sql.execution.QueryExecution.$anonfun$eagerlyExecuteCommands$1(QueryExecution.scala:155)
		at org.apache.spark.sql.execution.QueryExecution$.withInternalError(QueryExecution.scala:654)
		at org.apache.spark.sql.execution.QueryExecution.org$apache$spark$sql$execution$QueryExecution$$eagerlyExecute$1(QueryExecution.scala:154)
		at org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$3.applyOrElse(QueryExecution.scala:169)
		at org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$3.applyOrElse(QueryExecution.scala:164)
		at org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$transformDownWithPruning$1(TreeNode.scala:470)
		at org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(origin.scala:86)
		at org.apache.spark.sql.catalyst.trees.TreeNode.transformDownWithPruning(TreeNode.scala:470)
		at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.org$apache$spark$sql$catalyst$plans$logical$AnalysisHelper$$super$transformDownWithPruning(LogicalPlan.scala:37)
		at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning(AnalysisHelper.scala:360)
		at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning$(AnalysisHelper.scala:356)
		at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:37)
		at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:37)
		at org.apache.spark.sql.catalyst.trees.TreeNode.transformDown(TreeNode.scala:446)
		at org.apache.spark.sql.execution.QueryExecution.eagerlyExecuteCommands(QueryExecution.scala:164)
		at org.apache.spark.sql.execution.QueryExecution.$anonfun$lazyCommandExecuted$1(QueryExecution.scala:126)
		at scala.util.Try$.apply(Try.scala:217)
		at org.apache.spark.util.Utils$.doTryWithCallerStacktrace(Utils.scala:1378)
		at org.apache.spark.util.LazyTry.tryT$lzycompute(LazyTry.scala:46)
		at org.apache.spark.util.LazyTry.tryT(LazyTry.scala:46)
		... 19 more
Caused by: java.lang.ClassNotFoundException: Class org.apache.hadoop.fs.s3a.S3AFileSystem not found
	at org.apache.hadoop.conf.Configuration.getClassByName(Configuration.java:2641)
	at org.apache.hadoop.conf.Configuration.getClass(Configuration.java:2735)
	at org.apache.hadoop.fs.FileSystem.getFileSystemClass(FileSystem.java:3569)
	at org.apache.hadoop.fs.FileSystem.createFileSystem(FileSystem.java:3612)
	at org.apache.hadoop.fs.FileSystem.access$300(FileSystem.java:172)
	at org.apache.hadoop.fs.FileSystem$Cache.getInternal(FileSystem.java:3716)
	at org.apache.hadoop.fs.FileSystem$Cache.get(FileSystem.java:3667)
	at org.apache.hadoop.fs.FileSystem.get(FileSystem.java:557)
	at org.apache.hadoop.fs.Path.getFileSystem(Path.java:366)
	at org.apache.spark.sql.delta.DeltaLog$.apply(DeltaLog.scala:966)
	at org.apache.spark.sql.delta.DeltaLog$.forTable(DeltaLog.scala:801)
	at org.apache.spark.sql.delta.sources.DeltaDataSource.createRelation(DeltaDataSource.scala:197)
	at org.apache.spark.sql.execution.datasources.SaveIntoDataSourceCommand.run(SaveIntoDataSourceCommand.scala:55)
	at org.apache.spark.sql.execution.command.ExecutedCommandExec.sideEffectResult$lzycompute(commands.scala:79)
	at org.apache.spark.sql.execution.command.ExecutedCommandExec.sideEffectResult(commands.scala:77)
	at org.apache.spark.sql.execution.command.ExecutedCommandExec.executeCollect(commands.scala:88)
	at org.apache.spark.sql.execution.QueryExecution.$anonfun$eagerlyExecuteCommands$2(QueryExecution.scala:155)
	at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId0$8(SQLExecution.scala:162)
	at org.apache.spark.sql.execution.SQLExecution$.withSessionTagsApplied(SQLExecution.scala:268)
	at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId0$7(SQLExecution.scala:124)
	at org.apache.spark.JobArtifactSet$.withActiveJobArtifactState(JobArtifactSet.scala:94)
	at org.apache.spark.sql.artifact.ArtifactManager.$anonfun$withResources$1(ArtifactManager.scala:112)
	at org.apache.spark.sql.artifact.ArtifactManager.withClassLoaderIfNeeded(ArtifactManager.scala:106)
	at org.apache.spark.sql.artifact.ArtifactManager.withResources(ArtifactManager.scala:111)
	at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId0$6(SQLExecution.scala:124)
	at org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:291)
	at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId0$1(SQLExecution.scala:123)
	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:804)
	at org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId0(SQLExecution.scala:77)
	at org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:233)
	at org.apache.spark.sql.execution.QueryExecution.$anonfun$eagerlyExecuteCommands$1(QueryExecution.scala:155)
	at org.apache.spark.sql.execution.QueryExecution$.withInternalError(QueryExecution.scala:654)
	at org.apache.spark.sql.execution.QueryExecution.org$apache$spark$sql$execution$QueryExecution$$eagerlyExecute$1(QueryExecution.scala:154)
	at org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$3.applyOrElse(QueryExecution.scala:169)
	at org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$3.applyOrElse(QueryExecution.scala:164)
	at org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$transformDownWithPruning$1(TreeNode.scala:470)
	at org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(origin.scala:86)
	at org.apache.spark.sql.catalyst.trees.TreeNode.transformDownWithPruning(TreeNode.scala:470)
	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.org$apache$spark$sql$catalyst$plans$logical$AnalysisHelper$$super$transformDownWithPruning(LogicalPlan.scala:37)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning(AnalysisHelper.scala:360)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning$(AnalysisHelper.scala:356)
	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:37)
	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:37)
	at org.apache.spark.sql.catalyst.trees.TreeNode.transformDown(TreeNode.scala:446)
	at org.apache.spark.sql.execution.QueryExecution.eagerlyExecuteCommands(QueryExecution.scala:164)
	at org.apache.spark.sql.execution.QueryExecution.$anonfun$lazyCommandExecuted$1(QueryExecution.scala:126)
	at scala.util.Try$.apply(Try.scala:217)
	at org.apache.spark.util.Utils$.doTryWithCallerStacktrace(Utils.scala:1378)
	at org.apache.spark.util.LazyTry.tryT$lzycompute(LazyTry.scala:46)
	at org.apache.spark.util.LazyTry.tryT(LazyTry.scala:46)
	... 19 more
2025-07-23 11:08:52,401 INFO: Closing down clientserver connection
2025-07-23 11:13:16,859 INFO: Script /home/matteo/PycharmProjects/Tesi/BDA_Scripts/druid/druid_ingestion.sh eseguito con successo.
2025-07-23 11:13:17,966 ERROR: Traceback (most recent call last):
2025-07-23 11:13:17,966 ERROR: File "/home/matteo/PycharmProjects/Tesi/BDA_Scripts/retention_area/8_dw_feeder.py", line 234, in <module>
2025-07-23 11:13:17,966 ERROR: backup_bronze(spark, claims, access_key, secret_key, endpoint, s3_path)
2025-07-23 11:13:17,966 ERROR: File "/home/matteo/PycharmProjects/Tesi/BDA_Scripts/retention_area/8_dw_feeder.py", line 45, in backup_bronze
2025-07-23 11:13:17,966 ERROR: df_current.write.format("delta").mode("append").partitionBy("ingestion_date").save(s3_path)
2025-07-23 11:13:17,966 ERROR: File "/home/matteo/PycharmProjects/Tesi/.venv1/lib/python3.12/site-packages/pyspark/sql/readwriter.py", line 1745, in save
2025-07-23 11:13:17,967 ERROR: self._jwrite.save(path)
2025-07-23 11:13:17,967 ERROR: File "/home/matteo/PycharmProjects/Tesi/.venv1/lib/python3.12/site-packages/py4j/java_gateway.py", line 1362, in __call__
2025-07-23 11:13:17,967 ERROR: return_value = get_return_value(
2025-07-23 11:13:17,967 ERROR: ^
2025-07-23 11:13:17,967 ERROR: ^
2025-07-23 11:13:17,967 ERROR: ^
2025-07-23 11:13:17,967 ERROR: ^
2025-07-23 11:13:17,967 ERROR: ^
2025-07-23 11:13:17,967 ERROR: ^
2025-07-23 11:13:17,967 ERROR: ^
2025-07-23 11:13:17,967 ERROR: ^
2025-07-23 11:13:17,967 ERROR: ^
2025-07-23 11:13:17,967 ERROR: ^
2025-07-23 11:13:17,967 ERROR: ^
2025-07-23 11:13:17,967 ERROR: ^
2025-07-23 11:13:17,967 ERROR: ^
2025-07-23 11:13:17,967 ERROR: ^
2025-07-23 11:13:17,967 ERROR: ^
2025-07-23 11:13:17,967 ERROR: ^
2025-07-23 11:13:17,967 ERROR: ^
2025-07-23 11:13:17,967 ERROR: File "/home/matteo/PycharmProjects/Tesi/.venv1/lib/python3.12/site-packages/pyspark/errors/exceptions/captured.py", line 282, in deco
2025-07-23 11:13:17,967 ERROR: return f(*a, **kw)
2025-07-23 11:13:17,967 ERROR: ^
2025-07-23 11:13:17,968 ERROR: ^
2025-07-23 11:13:17,968 ERROR: ^
2025-07-23 11:13:17,968 ERROR: ^
2025-07-23 11:13:17,968 ERROR: ^
2025-07-23 11:13:17,968 ERROR: ^
2025-07-23 11:13:17,968 ERROR: ^
2025-07-23 11:13:17,968 ERROR: ^
2025-07-23 11:13:17,968 ERROR: ^
2025-07-23 11:13:17,968 ERROR: ^
2025-07-23 11:13:17,968 ERROR: ^
2025-07-23 11:13:17,968 ERROR: File "/home/matteo/PycharmProjects/Tesi/.venv1/lib/python3.12/site-packages/py4j/protocol.py", line 327, in get_return_value
2025-07-23 11:13:17,968 ERROR: raise Py4JJavaError(
2025-07-23 11:13:17,968 ERROR: py4j.protocol
2025-07-23 11:13:17,968 ERROR: .
2025-07-23 11:13:17,968 ERROR: Py4JJavaError
2025-07-23 11:13:18,009 ERROR: :
2025-07-23 11:13:18,009 ERROR: An error occurred while calling o60.save.
: java.lang.RuntimeException: java.lang.ClassNotFoundException: Class org.apache.hadoop.fs.s3a.S3AFileSystem not found
	at org.apache.hadoop.conf.Configuration.getClass(Configuration.java:2737)
	at org.apache.hadoop.fs.FileSystem.getFileSystemClass(FileSystem.java:3569)
	at org.apache.hadoop.fs.FileSystem.createFileSystem(FileSystem.java:3612)
	at org.apache.hadoop.fs.FileSystem.access$300(FileSystem.java:172)
	at org.apache.hadoop.fs.FileSystem$Cache.getInternal(FileSystem.java:3716)
	at org.apache.hadoop.fs.FileSystem$Cache.get(FileSystem.java:3667)
	at org.apache.hadoop.fs.FileSystem.get(FileSystem.java:557)
	at org.apache.hadoop.fs.Path.getFileSystem(Path.java:366)
	at org.apache.spark.sql.delta.DeltaLog$.apply(DeltaLog.scala:966)
	at org.apache.spark.sql.delta.DeltaLog$.forTable(DeltaLog.scala:801)
	at org.apache.spark.sql.delta.sources.DeltaDataSource.createRelation(DeltaDataSource.scala:197)
	at org.apache.spark.sql.execution.datasources.SaveIntoDataSourceCommand.run(SaveIntoDataSourceCommand.scala:55)
	at org.apache.spark.sql.execution.command.ExecutedCommandExec.sideEffectResult$lzycompute(commands.scala:79)
	at org.apache.spark.sql.execution.command.ExecutedCommandExec.sideEffectResult(commands.scala:77)
	at org.apache.spark.sql.execution.command.ExecutedCommandExec.executeCollect(commands.scala:88)
	at org.apache.spark.sql.execution.QueryExecution.$anonfun$eagerlyExecuteCommands$2(QueryExecution.scala:155)
	at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId0$8(SQLExecution.scala:162)
	at org.apache.spark.sql.execution.SQLExecution$.withSessionTagsApplied(SQLExecution.scala:268)
	at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId0$7(SQLExecution.scala:124)
	at org.apache.spark.JobArtifactSet$.withActiveJobArtifactState(JobArtifactSet.scala:94)
	at org.apache.spark.sql.artifact.ArtifactManager.$anonfun$withResources$1(ArtifactManager.scala:112)
	at org.apache.spark.sql.artifact.ArtifactManager.withClassLoaderIfNeeded(ArtifactManager.scala:106)
	at org.apache.spark.sql.artifact.ArtifactManager.withResources(ArtifactManager.scala:111)
	at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId0$6(SQLExecution.scala:124)
	at org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:291)
	at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId0$1(SQLExecution.scala:123)
	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:804)
	at org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId0(SQLExecution.scala:77)
	at org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:233)
	at org.apache.spark.sql.execution.QueryExecution.$anonfun$eagerlyExecuteCommands$1(QueryExecution.scala:155)
	at org.apache.spark.sql.execution.QueryExecution$.withInternalError(QueryExecution.scala:654)
	at org.apache.spark.sql.execution.QueryExecution.org$apache$spark$sql$execution$QueryExecution$$eagerlyExecute$1(QueryExecution.scala:154)
	at org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$3.applyOrElse(QueryExecution.scala:169)
	at org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$3.applyOrElse(QueryExecution.scala:164)
	at org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$transformDownWithPruning$1(TreeNode.scala:470)
	at org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(origin.scala:86)
	at org.apache.spark.sql.catalyst.trees.TreeNode.transformDownWithPruning(TreeNode.scala:470)
	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.org$apache$spark$sql$catalyst$plans$logical$AnalysisHelper$$super$transformDownWithPruning(LogicalPlan.scala:37)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning(AnalysisHelper.scala:360)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning$(AnalysisHelper.scala:356)
	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:37)
	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:37)
	at org.apache.spark.sql.catalyst.trees.TreeNode.transformDown(TreeNode.scala:446)
	at org.apache.spark.sql.execution.QueryExecution.eagerlyExecuteCommands(QueryExecution.scala:164)
	at org.apache.spark.sql.execution.QueryExecution.$anonfun$lazyCommandExecuted$1(QueryExecution.scala:126)
	at scala.util.Try$.apply(Try.scala:217)
	at org.apache.spark.util.Utils$.doTryWithCallerStacktrace(Utils.scala:1378)
	at org.apache.spark.util.Utils$.getTryWithCallerStacktrace(Utils.scala:1439)
	at org.apache.spark.util.LazyTry.get(LazyTry.scala:58)
	at org.apache.spark.sql.execution.QueryExecution.commandExecuted(QueryExecution.scala:131)
	at org.apache.spark.sql.execution.QueryExecution.assertCommandExecuted(QueryExecution.scala:192)
	at org.apache.spark.sql.classic.DataFrameWriter.runCommand(DataFrameWriter.scala:622)
	at org.apache.spark.sql.classic.DataFrameWriter.saveToV1Source(DataFrameWriter.scala:273)
	at org.apache.spark.sql.classic.DataFrameWriter.saveInternal(DataFrameWriter.scala:182)
	at org.apache.spark.sql.classic.DataFrameWriter.save(DataFrameWriter.scala:118)
	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:77)
	at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.base/java.lang.reflect.Method.invoke(Method.java:569)
	at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)
	at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)
	at py4j.Gateway.invoke(Gateway.java:282)
	at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)
	at py4j.commands.CallCommand.execute(CallCommand.java:79)
	at py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:184)
	at py4j.ClientServerConnection.run(ClientServerConnection.java:108)
	at java.base/java.lang.Thread.run(Thread.java:840)
	Suppressed: org.apache.spark.util.Utils$OriginalTryStackTraceException: Full stacktrace of original doTryWithCallerStacktrace caller
		at org.apache.hadoop.conf.Configuration.getClass(Configuration.java:2737)
		at org.apache.hadoop.fs.FileSystem.getFileSystemClass(FileSystem.java:3569)
		at org.apache.hadoop.fs.FileSystem.createFileSystem(FileSystem.java:3612)
		at org.apache.hadoop.fs.FileSystem.access$300(FileSystem.java:172)
		at org.apache.hadoop.fs.FileSystem$Cache.getInternal(FileSystem.java:3716)
		at org.apache.hadoop.fs.FileSystem$Cache.get(FileSystem.java:3667)
		at org.apache.hadoop.fs.FileSystem.get(FileSystem.java:557)
		at org.apache.hadoop.fs.Path.getFileSystem(Path.java:366)
		at org.apache.spark.sql.delta.DeltaLog$.apply(DeltaLog.scala:966)
		at org.apache.spark.sql.delta.DeltaLog$.forTable(DeltaLog.scala:801)
		at org.apache.spark.sql.delta.sources.DeltaDataSource.createRelation(DeltaDataSource.scala:197)
		at org.apache.spark.sql.execution.datasources.SaveIntoDataSourceCommand.run(SaveIntoDataSourceCommand.scala:55)
		at org.apache.spark.sql.execution.command.ExecutedCommandExec.sideEffectResult$lzycompute(commands.scala:79)
		at org.apache.spark.sql.execution.command.ExecutedCommandExec.sideEffectResult(commands.scala:77)
		at org.apache.spark.sql.execution.command.ExecutedCommandExec.executeCollect(commands.scala:88)
		at org.apache.spark.sql.execution.QueryExecution.$anonfun$eagerlyExecuteCommands$2(QueryExecution.scala:155)
		at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId0$8(SQLExecution.scala:162)
		at org.apache.spark.sql.execution.SQLExecution$.withSessionTagsApplied(SQLExecution.scala:268)
		at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId0$7(SQLExecution.scala:124)
		at org.apache.spark.JobArtifactSet$.withActiveJobArtifactState(JobArtifactSet.scala:94)
		at org.apache.spark.sql.artifact.ArtifactManager.$anonfun$withResources$1(ArtifactManager.scala:112)
		at org.apache.spark.sql.artifact.ArtifactManager.withClassLoaderIfNeeded(ArtifactManager.scala:106)
		at org.apache.spark.sql.artifact.ArtifactManager.withResources(ArtifactManager.scala:111)
		at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId0$6(SQLExecution.scala:124)
		at org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:291)
		at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId0$1(SQLExecution.scala:123)
		at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:804)
		at org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId0(SQLExecution.scala:77)
		at org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:233)
		at org.apache.spark.sql.execution.QueryExecution.$anonfun$eagerlyExecuteCommands$1(QueryExecution.scala:155)
		at org.apache.spark.sql.execution.QueryExecution$.withInternalError(QueryExecution.scala:654)
		at org.apache.spark.sql.execution.QueryExecution.org$apache$spark$sql$execution$QueryExecution$$eagerlyExecute$1(QueryExecution.scala:154)
		at org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$3.applyOrElse(QueryExecution.scala:169)
		at org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$3.applyOrElse(QueryExecution.scala:164)
		at org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$transformDownWithPruning$1(TreeNode.scala:470)
		at org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(origin.scala:86)
		at org.apache.spark.sql.catalyst.trees.TreeNode.transformDownWithPruning(TreeNode.scala:470)
		at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.org$apache$spark$sql$catalyst$plans$logical$AnalysisHelper$$super$transformDownWithPruning(LogicalPlan.scala:37)
		at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning(AnalysisHelper.scala:360)
		at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning$(AnalysisHelper.scala:356)
		at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:37)
		at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:37)
		at org.apache.spark.sql.catalyst.trees.TreeNode.transformDown(TreeNode.scala:446)
		at org.apache.spark.sql.execution.QueryExecution.eagerlyExecuteCommands(QueryExecution.scala:164)
		at org.apache.spark.sql.execution.QueryExecution.$anonfun$lazyCommandExecuted$1(QueryExecution.scala:126)
		at scala.util.Try$.apply(Try.scala:217)
		at org.apache.spark.util.Utils$.doTryWithCallerStacktrace(Utils.scala:1378)
		at org.apache.spark.util.LazyTry.tryT$lzycompute(LazyTry.scala:46)
		at org.apache.spark.util.LazyTry.tryT(LazyTry.scala:46)
		... 19 more
Caused by: java.lang.ClassNotFoundException: Class org.apache.hadoop.fs.s3a.S3AFileSystem not found
	at org.apache.hadoop.conf.Configuration.getClassByName(Configuration.java:2641)
	at org.apache.hadoop.conf.Configuration.getClass(Configuration.java:2735)
	at org.apache.hadoop.fs.FileSystem.getFileSystemClass(FileSystem.java:3569)
	at org.apache.hadoop.fs.FileSystem.createFileSystem(FileSystem.java:3612)
	at org.apache.hadoop.fs.FileSystem.access$300(FileSystem.java:172)
	at org.apache.hadoop.fs.FileSystem$Cache.getInternal(FileSystem.java:3716)
	at org.apache.hadoop.fs.FileSystem$Cache.get(FileSystem.java:3667)
	at org.apache.hadoop.fs.FileSystem.get(FileSystem.java:557)
	at org.apache.hadoop.fs.Path.getFileSystem(Path.java:366)
	at org.apache.spark.sql.delta.DeltaLog$.apply(DeltaLog.scala:966)
	at org.apache.spark.sql.delta.DeltaLog$.forTable(DeltaLog.scala:801)
	at org.apache.spark.sql.delta.sources.DeltaDataSource.createRelation(DeltaDataSource.scala:197)
	at org.apache.spark.sql.execution.datasources.SaveIntoDataSourceCommand.run(SaveIntoDataSourceCommand.scala:55)
	at org.apache.spark.sql.execution.command.ExecutedCommandExec.sideEffectResult$lzycompute(commands.scala:79)
	at org.apache.spark.sql.execution.command.ExecutedCommandExec.sideEffectResult(commands.scala:77)
	at org.apache.spark.sql.execution.command.ExecutedCommandExec.executeCollect(commands.scala:88)
	at org.apache.spark.sql.execution.QueryExecution.$anonfun$eagerlyExecuteCommands$2(QueryExecution.scala:155)
	at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId0$8(SQLExecution.scala:162)
	at org.apache.spark.sql.execution.SQLExecution$.withSessionTagsApplied(SQLExecution.scala:268)
	at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId0$7(SQLExecution.scala:124)
	at org.apache.spark.JobArtifactSet$.withActiveJobArtifactState(JobArtifactSet.scala:94)
	at org.apache.spark.sql.artifact.ArtifactManager.$anonfun$withResources$1(ArtifactManager.scala:112)
	at org.apache.spark.sql.artifact.ArtifactManager.withClassLoaderIfNeeded(ArtifactManager.scala:106)
	at org.apache.spark.sql.artifact.ArtifactManager.withResources(ArtifactManager.scala:111)
	at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId0$6(SQLExecution.scala:124)
	at org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:291)
	at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId0$1(SQLExecution.scala:123)
	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:804)
	at org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId0(SQLExecution.scala:77)
	at org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:233)
	at org.apache.spark.sql.execution.QueryExecution.$anonfun$eagerlyExecuteCommands$1(QueryExecution.scala:155)
	at org.apache.spark.sql.execution.QueryExecution$.withInternalError(QueryExecution.scala:654)
	at org.apache.spark.sql.execution.QueryExecution.org$apache$spark$sql$execution$QueryExecution$$eagerlyExecute$1(QueryExecution.scala:154)
	at org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$3.applyOrElse(QueryExecution.scala:169)
	at org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$3.applyOrElse(QueryExecution.scala:164)
	at org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$transformDownWithPruning$1(TreeNode.scala:470)
	at org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(origin.scala:86)
	at org.apache.spark.sql.catalyst.trees.TreeNode.transformDownWithPruning(TreeNode.scala:470)
	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.org$apache$spark$sql$catalyst$plans$logical$AnalysisHelper$$super$transformDownWithPruning(LogicalPlan.scala:37)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning(AnalysisHelper.scala:360)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning$(AnalysisHelper.scala:356)
	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:37)
	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:37)
	at org.apache.spark.sql.catalyst.trees.TreeNode.transformDown(TreeNode.scala:446)
	at org.apache.spark.sql.execution.QueryExecution.eagerlyExecuteCommands(QueryExecution.scala:164)
	at org.apache.spark.sql.execution.QueryExecution.$anonfun$lazyCommandExecuted$1(QueryExecution.scala:126)
	at scala.util.Try$.apply(Try.scala:217)
	at org.apache.spark.util.Utils$.doTryWithCallerStacktrace(Utils.scala:1378)
	at org.apache.spark.util.LazyTry.tryT$lzycompute(LazyTry.scala:46)
	at org.apache.spark.util.LazyTry.tryT(LazyTry.scala:46)
	... 19 more
2025-07-23 11:13:18,009 INFO: Closing down clientserver connection
2025-07-23 11:19:43,484 INFO: Script /home/matteo/PycharmProjects/Tesi/BDA_Scripts/druid/druid_ingestion.sh eseguito con successo.
2025-07-23 11:19:44,690 ERROR: Traceback (most recent call last):
2025-07-23 11:19:44,690 ERROR: File "/home/matteo/PycharmProjects/Tesi/BDA_Scripts/retention_area/8_dw_feeder.py", line 234, in <module>
2025-07-23 11:19:44,690 ERROR: backup_bronze(spark, claims, access_key, secret_key, endpoint, s3_path)
2025-07-23 11:19:44,690 ERROR: File "/home/matteo/PycharmProjects/Tesi/BDA_Scripts/retention_area/8_dw_feeder.py", line 45, in backup_bronze
2025-07-23 11:19:44,690 ERROR: df_current.write.format("delta").mode("append").partitionBy("ingestion_date").save(s3_path)
2025-07-23 11:19:44,690 ERROR: File "/home/matteo/PycharmProjects/Tesi/.venv1/lib/python3.12/site-packages/pyspark/sql/readwriter.py", line 1745, in save
2025-07-23 11:19:44,690 ERROR: self._jwrite.save(path)
2025-07-23 11:19:44,690 ERROR: File "/home/matteo/PycharmProjects/Tesi/.venv1/lib/python3.12/site-packages/py4j/java_gateway.py", line 1362, in __call__
2025-07-23 11:19:44,691 ERROR: return_value = get_return_value(
2025-07-23 11:19:44,691 ERROR: ^
2025-07-23 11:19:44,691 ERROR: ^
2025-07-23 11:19:44,691 ERROR: ^
2025-07-23 11:19:44,691 ERROR: ^
2025-07-23 11:19:44,691 ERROR: ^
2025-07-23 11:19:44,691 ERROR: ^
2025-07-23 11:19:44,691 ERROR: ^
2025-07-23 11:19:44,691 ERROR: ^
2025-07-23 11:19:44,691 ERROR: ^
2025-07-23 11:19:44,691 ERROR: ^
2025-07-23 11:19:44,691 ERROR: ^
2025-07-23 11:19:44,691 ERROR: ^
2025-07-23 11:19:44,691 ERROR: ^
2025-07-23 11:19:44,691 ERROR: ^
2025-07-23 11:19:44,691 ERROR: ^
2025-07-23 11:19:44,691 ERROR: ^
2025-07-23 11:19:44,691 ERROR: ^
2025-07-23 11:19:44,691 ERROR: File "/home/matteo/PycharmProjects/Tesi/.venv1/lib/python3.12/site-packages/pyspark/errors/exceptions/captured.py", line 282, in deco
2025-07-23 11:19:44,691 ERROR: return f(*a, **kw)
2025-07-23 11:19:44,691 ERROR: ^
2025-07-23 11:19:44,691 ERROR: ^
2025-07-23 11:19:44,691 ERROR: ^
2025-07-23 11:19:44,691 ERROR: ^
2025-07-23 11:19:44,691 ERROR: ^
2025-07-23 11:19:44,691 ERROR: ^
2025-07-23 11:19:44,691 ERROR: ^
2025-07-23 11:19:44,691 ERROR: ^
2025-07-23 11:19:44,691 ERROR: ^
2025-07-23 11:19:44,691 ERROR: ^
2025-07-23 11:19:44,691 ERROR: ^
2025-07-23 11:19:44,691 ERROR: File "/home/matteo/PycharmProjects/Tesi/.venv1/lib/python3.12/site-packages/py4j/protocol.py", line 327, in get_return_value
2025-07-23 11:19:44,691 ERROR: raise Py4JJavaError(
2025-07-23 11:19:44,692 ERROR: py4j.protocol
2025-07-23 11:19:44,692 ERROR: .
2025-07-23 11:19:44,692 ERROR: Py4JJavaError
2025-07-23 11:19:44,733 ERROR: :
2025-07-23 11:19:44,733 ERROR: An error occurred while calling o60.save.
: java.lang.RuntimeException: java.lang.ClassNotFoundException: Class org.apache.hadoop.fs.s3a.S3AFileSystem not found
	at org.apache.hadoop.conf.Configuration.getClass(Configuration.java:2737)
	at org.apache.hadoop.fs.FileSystem.getFileSystemClass(FileSystem.java:3569)
	at org.apache.hadoop.fs.FileSystem.createFileSystem(FileSystem.java:3612)
	at org.apache.hadoop.fs.FileSystem.access$300(FileSystem.java:172)
	at org.apache.hadoop.fs.FileSystem$Cache.getInternal(FileSystem.java:3716)
	at org.apache.hadoop.fs.FileSystem$Cache.get(FileSystem.java:3667)
	at org.apache.hadoop.fs.FileSystem.get(FileSystem.java:557)
	at org.apache.hadoop.fs.Path.getFileSystem(Path.java:366)
	at org.apache.spark.sql.delta.DeltaLog$.apply(DeltaLog.scala:966)
	at org.apache.spark.sql.delta.DeltaLog$.forTable(DeltaLog.scala:801)
	at org.apache.spark.sql.delta.sources.DeltaDataSource.createRelation(DeltaDataSource.scala:197)
	at org.apache.spark.sql.execution.datasources.SaveIntoDataSourceCommand.run(SaveIntoDataSourceCommand.scala:55)
	at org.apache.spark.sql.execution.command.ExecutedCommandExec.sideEffectResult$lzycompute(commands.scala:79)
	at org.apache.spark.sql.execution.command.ExecutedCommandExec.sideEffectResult(commands.scala:77)
	at org.apache.spark.sql.execution.command.ExecutedCommandExec.executeCollect(commands.scala:88)
	at org.apache.spark.sql.execution.QueryExecution.$anonfun$eagerlyExecuteCommands$2(QueryExecution.scala:155)
	at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId0$8(SQLExecution.scala:162)
	at org.apache.spark.sql.execution.SQLExecution$.withSessionTagsApplied(SQLExecution.scala:268)
	at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId0$7(SQLExecution.scala:124)
	at org.apache.spark.JobArtifactSet$.withActiveJobArtifactState(JobArtifactSet.scala:94)
	at org.apache.spark.sql.artifact.ArtifactManager.$anonfun$withResources$1(ArtifactManager.scala:112)
	at org.apache.spark.sql.artifact.ArtifactManager.withClassLoaderIfNeeded(ArtifactManager.scala:106)
	at org.apache.spark.sql.artifact.ArtifactManager.withResources(ArtifactManager.scala:111)
	at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId0$6(SQLExecution.scala:124)
	at org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:291)
	at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId0$1(SQLExecution.scala:123)
	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:804)
	at org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId0(SQLExecution.scala:77)
	at org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:233)
	at org.apache.spark.sql.execution.QueryExecution.$anonfun$eagerlyExecuteCommands$1(QueryExecution.scala:155)
	at org.apache.spark.sql.execution.QueryExecution$.withInternalError(QueryExecution.scala:654)
	at org.apache.spark.sql.execution.QueryExecution.org$apache$spark$sql$execution$QueryExecution$$eagerlyExecute$1(QueryExecution.scala:154)
	at org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$3.applyOrElse(QueryExecution.scala:169)
	at org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$3.applyOrElse(QueryExecution.scala:164)
	at org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$transformDownWithPruning$1(TreeNode.scala:470)
	at org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(origin.scala:86)
	at org.apache.spark.sql.catalyst.trees.TreeNode.transformDownWithPruning(TreeNode.scala:470)
	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.org$apache$spark$sql$catalyst$plans$logical$AnalysisHelper$$super$transformDownWithPruning(LogicalPlan.scala:37)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning(AnalysisHelper.scala:360)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning$(AnalysisHelper.scala:356)
	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:37)
	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:37)
	at org.apache.spark.sql.catalyst.trees.TreeNode.transformDown(TreeNode.scala:446)
	at org.apache.spark.sql.execution.QueryExecution.eagerlyExecuteCommands(QueryExecution.scala:164)
	at org.apache.spark.sql.execution.QueryExecution.$anonfun$lazyCommandExecuted$1(QueryExecution.scala:126)
	at scala.util.Try$.apply(Try.scala:217)
	at org.apache.spark.util.Utils$.doTryWithCallerStacktrace(Utils.scala:1378)
	at org.apache.spark.util.Utils$.getTryWithCallerStacktrace(Utils.scala:1439)
	at org.apache.spark.util.LazyTry.get(LazyTry.scala:58)
	at org.apache.spark.sql.execution.QueryExecution.commandExecuted(QueryExecution.scala:131)
	at org.apache.spark.sql.execution.QueryExecution.assertCommandExecuted(QueryExecution.scala:192)
	at org.apache.spark.sql.classic.DataFrameWriter.runCommand(DataFrameWriter.scala:622)
	at org.apache.spark.sql.classic.DataFrameWriter.saveToV1Source(DataFrameWriter.scala:273)
	at org.apache.spark.sql.classic.DataFrameWriter.saveInternal(DataFrameWriter.scala:182)
	at org.apache.spark.sql.classic.DataFrameWriter.save(DataFrameWriter.scala:118)
	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:77)
	at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.base/java.lang.reflect.Method.invoke(Method.java:569)
	at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)
	at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)
	at py4j.Gateway.invoke(Gateway.java:282)
	at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)
	at py4j.commands.CallCommand.execute(CallCommand.java:79)
	at py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:184)
	at py4j.ClientServerConnection.run(ClientServerConnection.java:108)
	at java.base/java.lang.Thread.run(Thread.java:840)
	Suppressed: org.apache.spark.util.Utils$OriginalTryStackTraceException: Full stacktrace of original doTryWithCallerStacktrace caller
		at org.apache.hadoop.conf.Configuration.getClass(Configuration.java:2737)
		at org.apache.hadoop.fs.FileSystem.getFileSystemClass(FileSystem.java:3569)
		at org.apache.hadoop.fs.FileSystem.createFileSystem(FileSystem.java:3612)
		at org.apache.hadoop.fs.FileSystem.access$300(FileSystem.java:172)
		at org.apache.hadoop.fs.FileSystem$Cache.getInternal(FileSystem.java:3716)
		at org.apache.hadoop.fs.FileSystem$Cache.get(FileSystem.java:3667)
		at org.apache.hadoop.fs.FileSystem.get(FileSystem.java:557)
		at org.apache.hadoop.fs.Path.getFileSystem(Path.java:366)
		at org.apache.spark.sql.delta.DeltaLog$.apply(DeltaLog.scala:966)
		at org.apache.spark.sql.delta.DeltaLog$.forTable(DeltaLog.scala:801)
		at org.apache.spark.sql.delta.sources.DeltaDataSource.createRelation(DeltaDataSource.scala:197)
		at org.apache.spark.sql.execution.datasources.SaveIntoDataSourceCommand.run(SaveIntoDataSourceCommand.scala:55)
		at org.apache.spark.sql.execution.command.ExecutedCommandExec.sideEffectResult$lzycompute(commands.scala:79)
		at org.apache.spark.sql.execution.command.ExecutedCommandExec.sideEffectResult(commands.scala:77)
		at org.apache.spark.sql.execution.command.ExecutedCommandExec.executeCollect(commands.scala:88)
		at org.apache.spark.sql.execution.QueryExecution.$anonfun$eagerlyExecuteCommands$2(QueryExecution.scala:155)
		at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId0$8(SQLExecution.scala:162)
		at org.apache.spark.sql.execution.SQLExecution$.withSessionTagsApplied(SQLExecution.scala:268)
		at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId0$7(SQLExecution.scala:124)
		at org.apache.spark.JobArtifactSet$.withActiveJobArtifactState(JobArtifactSet.scala:94)
		at org.apache.spark.sql.artifact.ArtifactManager.$anonfun$withResources$1(ArtifactManager.scala:112)
		at org.apache.spark.sql.artifact.ArtifactManager.withClassLoaderIfNeeded(ArtifactManager.scala:106)
		at org.apache.spark.sql.artifact.ArtifactManager.withResources(ArtifactManager.scala:111)
		at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId0$6(SQLExecution.scala:124)
		at org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:291)
		at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId0$1(SQLExecution.scala:123)
		at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:804)
		at org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId0(SQLExecution.scala:77)
		at org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:233)
		at org.apache.spark.sql.execution.QueryExecution.$anonfun$eagerlyExecuteCommands$1(QueryExecution.scala:155)
		at org.apache.spark.sql.execution.QueryExecution$.withInternalError(QueryExecution.scala:654)
		at org.apache.spark.sql.execution.QueryExecution.org$apache$spark$sql$execution$QueryExecution$$eagerlyExecute$1(QueryExecution.scala:154)
		at org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$3.applyOrElse(QueryExecution.scala:169)
		at org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$3.applyOrElse(QueryExecution.scala:164)
		at org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$transformDownWithPruning$1(TreeNode.scala:470)
		at org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(origin.scala:86)
		at org.apache.spark.sql.catalyst.trees.TreeNode.transformDownWithPruning(TreeNode.scala:470)
		at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.org$apache$spark$sql$catalyst$plans$logical$AnalysisHelper$$super$transformDownWithPruning(LogicalPlan.scala:37)
		at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning(AnalysisHelper.scala:360)
		at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning$(AnalysisHelper.scala:356)
		at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:37)
		at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:37)
		at org.apache.spark.sql.catalyst.trees.TreeNode.transformDown(TreeNode.scala:446)
		at org.apache.spark.sql.execution.QueryExecution.eagerlyExecuteCommands(QueryExecution.scala:164)
		at org.apache.spark.sql.execution.QueryExecution.$anonfun$lazyCommandExecuted$1(QueryExecution.scala:126)
		at scala.util.Try$.apply(Try.scala:217)
		at org.apache.spark.util.Utils$.doTryWithCallerStacktrace(Utils.scala:1378)
		at org.apache.spark.util.LazyTry.tryT$lzycompute(LazyTry.scala:46)
		at org.apache.spark.util.LazyTry.tryT(LazyTry.scala:46)
		... 19 more
Caused by: java.lang.ClassNotFoundException: Class org.apache.hadoop.fs.s3a.S3AFileSystem not found
	at org.apache.hadoop.conf.Configuration.getClassByName(Configuration.java:2641)
	at org.apache.hadoop.conf.Configuration.getClass(Configuration.java:2735)
	at org.apache.hadoop.fs.FileSystem.getFileSystemClass(FileSystem.java:3569)
	at org.apache.hadoop.fs.FileSystem.createFileSystem(FileSystem.java:3612)
	at org.apache.hadoop.fs.FileSystem.access$300(FileSystem.java:172)
	at org.apache.hadoop.fs.FileSystem$Cache.getInternal(FileSystem.java:3716)
	at org.apache.hadoop.fs.FileSystem$Cache.get(FileSystem.java:3667)
	at org.apache.hadoop.fs.FileSystem.get(FileSystem.java:557)
	at org.apache.hadoop.fs.Path.getFileSystem(Path.java:366)
	at org.apache.spark.sql.delta.DeltaLog$.apply(DeltaLog.scala:966)
	at org.apache.spark.sql.delta.DeltaLog$.forTable(DeltaLog.scala:801)
	at org.apache.spark.sql.delta.sources.DeltaDataSource.createRelation(DeltaDataSource.scala:197)
	at org.apache.spark.sql.execution.datasources.SaveIntoDataSourceCommand.run(SaveIntoDataSourceCommand.scala:55)
	at org.apache.spark.sql.execution.command.ExecutedCommandExec.sideEffectResult$lzycompute(commands.scala:79)
	at org.apache.spark.sql.execution.command.ExecutedCommandExec.sideEffectResult(commands.scala:77)
	at org.apache.spark.sql.execution.command.ExecutedCommandExec.executeCollect(commands.scala:88)
	at org.apache.spark.sql.execution.QueryExecution.$anonfun$eagerlyExecuteCommands$2(QueryExecution.scala:155)
	at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId0$8(SQLExecution.scala:162)
	at org.apache.spark.sql.execution.SQLExecution$.withSessionTagsApplied(SQLExecution.scala:268)
	at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId0$7(SQLExecution.scala:124)
	at org.apache.spark.JobArtifactSet$.withActiveJobArtifactState(JobArtifactSet.scala:94)
	at org.apache.spark.sql.artifact.ArtifactManager.$anonfun$withResources$1(ArtifactManager.scala:112)
	at org.apache.spark.sql.artifact.ArtifactManager.withClassLoaderIfNeeded(ArtifactManager.scala:106)
	at org.apache.spark.sql.artifact.ArtifactManager.withResources(ArtifactManager.scala:111)
	at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId0$6(SQLExecution.scala:124)
	at org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:291)
	at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId0$1(SQLExecution.scala:123)
	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:804)
	at org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId0(SQLExecution.scala:77)
	at org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:233)
	at org.apache.spark.sql.execution.QueryExecution.$anonfun$eagerlyExecuteCommands$1(QueryExecution.scala:155)
	at org.apache.spark.sql.execution.QueryExecution$.withInternalError(QueryExecution.scala:654)
	at org.apache.spark.sql.execution.QueryExecution.org$apache$spark$sql$execution$QueryExecution$$eagerlyExecute$1(QueryExecution.scala:154)
	at org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$3.applyOrElse(QueryExecution.scala:169)
	at org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$3.applyOrElse(QueryExecution.scala:164)
	at org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$transformDownWithPruning$1(TreeNode.scala:470)
	at org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(origin.scala:86)
	at org.apache.spark.sql.catalyst.trees.TreeNode.transformDownWithPruning(TreeNode.scala:470)
	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.org$apache$spark$sql$catalyst$plans$logical$AnalysisHelper$$super$transformDownWithPruning(LogicalPlan.scala:37)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning(AnalysisHelper.scala:360)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning$(AnalysisHelper.scala:356)
	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:37)
	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:37)
	at org.apache.spark.sql.catalyst.trees.TreeNode.transformDown(TreeNode.scala:446)
	at org.apache.spark.sql.execution.QueryExecution.eagerlyExecuteCommands(QueryExecution.scala:164)
	at org.apache.spark.sql.execution.QueryExecution.$anonfun$lazyCommandExecuted$1(QueryExecution.scala:126)
	at scala.util.Try$.apply(Try.scala:217)
	at org.apache.spark.util.Utils$.doTryWithCallerStacktrace(Utils.scala:1378)
	at org.apache.spark.util.LazyTry.tryT$lzycompute(LazyTry.scala:46)
	at org.apache.spark.util.LazyTry.tryT(LazyTry.scala:46)
	... 19 more
2025-07-23 11:19:44,733 INFO: Closing down clientserver connection
2025-07-23 11:23:36,491 INFO: Script /home/matteo/PycharmProjects/Tesi/BDA_Scripts/druid/druid_ingestion.sh eseguito con successo.
2025-07-23 11:23:38,237 ERROR: Traceback (most recent call last):
2025-07-23 11:23:38,237 ERROR: File "/home/matteo/PycharmProjects/Tesi/BDA_Scripts/retention_area/8_dw_feeder.py", line 234, in <module>
2025-07-23 11:23:38,237 ERROR: backup_bronze(spark, claims, access_key, secret_key, endpoint, s3_path)
2025-07-23 11:23:38,237 ERROR: File "/home/matteo/PycharmProjects/Tesi/BDA_Scripts/retention_area/8_dw_feeder.py", line 45, in backup_bronze
2025-07-23 11:23:38,237 ERROR: df_current.write.format("delta").mode("append").partitionBy("ingestion_date").save(s3_path)
2025-07-23 11:23:38,237 ERROR: File "/home/matteo/PycharmProjects/Tesi/.venv1/lib/python3.12/site-packages/pyspark/sql/readwriter.py", line 1745, in save
2025-07-23 11:23:38,238 ERROR: self._jwrite.save(path)
2025-07-23 11:23:38,238 ERROR: File "/home/matteo/PycharmProjects/Tesi/.venv1/lib/python3.12/site-packages/py4j/java_gateway.py", line 1362, in __call__
2025-07-23 11:23:38,238 ERROR: return_value = get_return_value(
2025-07-23 11:23:38,238 ERROR: ^
2025-07-23 11:23:38,238 ERROR: ^
2025-07-23 11:23:38,238 ERROR: ^
2025-07-23 11:23:38,238 ERROR: ^
2025-07-23 11:23:38,238 ERROR: ^
2025-07-23 11:23:38,238 ERROR: ^
2025-07-23 11:23:38,238 ERROR: ^
2025-07-23 11:23:38,238 ERROR: ^
2025-07-23 11:23:38,238 ERROR: ^
2025-07-23 11:23:38,238 ERROR: ^
2025-07-23 11:23:38,238 ERROR: ^
2025-07-23 11:23:38,238 ERROR: ^
2025-07-23 11:23:38,238 ERROR: ^
2025-07-23 11:23:38,238 ERROR: ^
2025-07-23 11:23:38,238 ERROR: ^
2025-07-23 11:23:38,238 ERROR: ^
2025-07-23 11:23:38,238 ERROR: ^
2025-07-23 11:23:38,238 ERROR: File "/home/matteo/PycharmProjects/Tesi/.venv1/lib/python3.12/site-packages/pyspark/errors/exceptions/captured.py", line 282, in deco
2025-07-23 11:23:38,238 ERROR: return f(*a, **kw)
2025-07-23 11:23:38,238 ERROR: ^
2025-07-23 11:23:38,238 ERROR: ^
2025-07-23 11:23:38,238 ERROR: ^
2025-07-23 11:23:38,238 ERROR: ^
2025-07-23 11:23:38,238 ERROR: ^
2025-07-23 11:23:38,238 ERROR: ^
2025-07-23 11:23:38,239 ERROR: ^
2025-07-23 11:23:38,239 ERROR: ^
2025-07-23 11:23:38,239 ERROR: ^
2025-07-23 11:23:38,239 ERROR: ^
2025-07-23 11:23:38,239 ERROR: ^
2025-07-23 11:23:38,239 ERROR: File "/home/matteo/PycharmProjects/Tesi/.venv1/lib/python3.12/site-packages/py4j/protocol.py", line 327, in get_return_value
2025-07-23 11:23:38,239 ERROR: raise Py4JJavaError(
2025-07-23 11:23:38,239 ERROR: py4j.protocol
2025-07-23 11:23:38,239 ERROR: .
2025-07-23 11:23:38,239 ERROR: Py4JJavaError
2025-07-23 11:23:38,280 ERROR: :
2025-07-23 11:23:38,280 ERROR: An error occurred while calling o60.save.
: java.lang.RuntimeException: java.lang.ClassNotFoundException: Class org.apache.hadoop.fs.s3a.S3AFileSystem not found
	at org.apache.hadoop.conf.Configuration.getClass(Configuration.java:2737)
	at org.apache.hadoop.fs.FileSystem.getFileSystemClass(FileSystem.java:3569)
	at org.apache.hadoop.fs.FileSystem.createFileSystem(FileSystem.java:3612)
	at org.apache.hadoop.fs.FileSystem.access$300(FileSystem.java:172)
	at org.apache.hadoop.fs.FileSystem$Cache.getInternal(FileSystem.java:3716)
	at org.apache.hadoop.fs.FileSystem$Cache.get(FileSystem.java:3667)
	at org.apache.hadoop.fs.FileSystem.get(FileSystem.java:557)
	at org.apache.hadoop.fs.Path.getFileSystem(Path.java:366)
	at org.apache.spark.sql.delta.DeltaLog$.apply(DeltaLog.scala:966)
	at org.apache.spark.sql.delta.DeltaLog$.forTable(DeltaLog.scala:801)
	at org.apache.spark.sql.delta.sources.DeltaDataSource.createRelation(DeltaDataSource.scala:197)
	at org.apache.spark.sql.execution.datasources.SaveIntoDataSourceCommand.run(SaveIntoDataSourceCommand.scala:55)
	at org.apache.spark.sql.execution.command.ExecutedCommandExec.sideEffectResult$lzycompute(commands.scala:79)
	at org.apache.spark.sql.execution.command.ExecutedCommandExec.sideEffectResult(commands.scala:77)
	at org.apache.spark.sql.execution.command.ExecutedCommandExec.executeCollect(commands.scala:88)
	at org.apache.spark.sql.execution.QueryExecution.$anonfun$eagerlyExecuteCommands$2(QueryExecution.scala:155)
	at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId0$8(SQLExecution.scala:162)
	at org.apache.spark.sql.execution.SQLExecution$.withSessionTagsApplied(SQLExecution.scala:268)
	at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId0$7(SQLExecution.scala:124)
	at org.apache.spark.JobArtifactSet$.withActiveJobArtifactState(JobArtifactSet.scala:94)
	at org.apache.spark.sql.artifact.ArtifactManager.$anonfun$withResources$1(ArtifactManager.scala:112)
	at org.apache.spark.sql.artifact.ArtifactManager.withClassLoaderIfNeeded(ArtifactManager.scala:106)
	at org.apache.spark.sql.artifact.ArtifactManager.withResources(ArtifactManager.scala:111)
	at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId0$6(SQLExecution.scala:124)
	at org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:291)
	at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId0$1(SQLExecution.scala:123)
	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:804)
	at org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId0(SQLExecution.scala:77)
	at org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:233)
	at org.apache.spark.sql.execution.QueryExecution.$anonfun$eagerlyExecuteCommands$1(QueryExecution.scala:155)
	at org.apache.spark.sql.execution.QueryExecution$.withInternalError(QueryExecution.scala:654)
	at org.apache.spark.sql.execution.QueryExecution.org$apache$spark$sql$execution$QueryExecution$$eagerlyExecute$1(QueryExecution.scala:154)
	at org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$3.applyOrElse(QueryExecution.scala:169)
	at org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$3.applyOrElse(QueryExecution.scala:164)
	at org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$transformDownWithPruning$1(TreeNode.scala:470)
	at org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(origin.scala:86)
	at org.apache.spark.sql.catalyst.trees.TreeNode.transformDownWithPruning(TreeNode.scala:470)
	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.org$apache$spark$sql$catalyst$plans$logical$AnalysisHelper$$super$transformDownWithPruning(LogicalPlan.scala:37)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning(AnalysisHelper.scala:360)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning$(AnalysisHelper.scala:356)
	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:37)
	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:37)
	at org.apache.spark.sql.catalyst.trees.TreeNode.transformDown(TreeNode.scala:446)
	at org.apache.spark.sql.execution.QueryExecution.eagerlyExecuteCommands(QueryExecution.scala:164)
	at org.apache.spark.sql.execution.QueryExecution.$anonfun$lazyCommandExecuted$1(QueryExecution.scala:126)
	at scala.util.Try$.apply(Try.scala:217)
	at org.apache.spark.util.Utils$.doTryWithCallerStacktrace(Utils.scala:1378)
	at org.apache.spark.util.Utils$.getTryWithCallerStacktrace(Utils.scala:1439)
	at org.apache.spark.util.LazyTry.get(LazyTry.scala:58)
	at org.apache.spark.sql.execution.QueryExecution.commandExecuted(QueryExecution.scala:131)
	at org.apache.spark.sql.execution.QueryExecution.assertCommandExecuted(QueryExecution.scala:192)
	at org.apache.spark.sql.classic.DataFrameWriter.runCommand(DataFrameWriter.scala:622)
	at org.apache.spark.sql.classic.DataFrameWriter.saveToV1Source(DataFrameWriter.scala:273)
	at org.apache.spark.sql.classic.DataFrameWriter.saveInternal(DataFrameWriter.scala:182)
	at org.apache.spark.sql.classic.DataFrameWriter.save(DataFrameWriter.scala:118)
	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:77)
	at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.base/java.lang.reflect.Method.invoke(Method.java:569)
	at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)
	at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)
	at py4j.Gateway.invoke(Gateway.java:282)
	at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)
	at py4j.commands.CallCommand.execute(CallCommand.java:79)
	at py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:184)
	at py4j.ClientServerConnection.run(ClientServerConnection.java:108)
	at java.base/java.lang.Thread.run(Thread.java:840)
	Suppressed: org.apache.spark.util.Utils$OriginalTryStackTraceException: Full stacktrace of original doTryWithCallerStacktrace caller
		at org.apache.hadoop.conf.Configuration.getClass(Configuration.java:2737)
		at org.apache.hadoop.fs.FileSystem.getFileSystemClass(FileSystem.java:3569)
		at org.apache.hadoop.fs.FileSystem.createFileSystem(FileSystem.java:3612)
		at org.apache.hadoop.fs.FileSystem.access$300(FileSystem.java:172)
		at org.apache.hadoop.fs.FileSystem$Cache.getInternal(FileSystem.java:3716)
		at org.apache.hadoop.fs.FileSystem$Cache.get(FileSystem.java:3667)
		at org.apache.hadoop.fs.FileSystem.get(FileSystem.java:557)
		at org.apache.hadoop.fs.Path.getFileSystem(Path.java:366)
		at org.apache.spark.sql.delta.DeltaLog$.apply(DeltaLog.scala:966)
		at org.apache.spark.sql.delta.DeltaLog$.forTable(DeltaLog.scala:801)
		at org.apache.spark.sql.delta.sources.DeltaDataSource.createRelation(DeltaDataSource.scala:197)
		at org.apache.spark.sql.execution.datasources.SaveIntoDataSourceCommand.run(SaveIntoDataSourceCommand.scala:55)
		at org.apache.spark.sql.execution.command.ExecutedCommandExec.sideEffectResult$lzycompute(commands.scala:79)
		at org.apache.spark.sql.execution.command.ExecutedCommandExec.sideEffectResult(commands.scala:77)
		at org.apache.spark.sql.execution.command.ExecutedCommandExec.executeCollect(commands.scala:88)
		at org.apache.spark.sql.execution.QueryExecution.$anonfun$eagerlyExecuteCommands$2(QueryExecution.scala:155)
		at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId0$8(SQLExecution.scala:162)
		at org.apache.spark.sql.execution.SQLExecution$.withSessionTagsApplied(SQLExecution.scala:268)
		at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId0$7(SQLExecution.scala:124)
		at org.apache.spark.JobArtifactSet$.withActiveJobArtifactState(JobArtifactSet.scala:94)
		at org.apache.spark.sql.artifact.ArtifactManager.$anonfun$withResources$1(ArtifactManager.scala:112)
		at org.apache.spark.sql.artifact.ArtifactManager.withClassLoaderIfNeeded(ArtifactManager.scala:106)
		at org.apache.spark.sql.artifact.ArtifactManager.withResources(ArtifactManager.scala:111)
		at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId0$6(SQLExecution.scala:124)
		at org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:291)
		at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId0$1(SQLExecution.scala:123)
		at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:804)
		at org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId0(SQLExecution.scala:77)
		at org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:233)
		at org.apache.spark.sql.execution.QueryExecution.$anonfun$eagerlyExecuteCommands$1(QueryExecution.scala:155)
		at org.apache.spark.sql.execution.QueryExecution$.withInternalError(QueryExecution.scala:654)
		at org.apache.spark.sql.execution.QueryExecution.org$apache$spark$sql$execution$QueryExecution$$eagerlyExecute$1(QueryExecution.scala:154)
		at org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$3.applyOrElse(QueryExecution.scala:169)
		at org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$3.applyOrElse(QueryExecution.scala:164)
		at org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$transformDownWithPruning$1(TreeNode.scala:470)
		at org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(origin.scala:86)
		at org.apache.spark.sql.catalyst.trees.TreeNode.transformDownWithPruning(TreeNode.scala:470)
		at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.org$apache$spark$sql$catalyst$plans$logical$AnalysisHelper$$super$transformDownWithPruning(LogicalPlan.scala:37)
		at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning(AnalysisHelper.scala:360)
		at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning$(AnalysisHelper.scala:356)
		at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:37)
		at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:37)
		at org.apache.spark.sql.catalyst.trees.TreeNode.transformDown(TreeNode.scala:446)
		at org.apache.spark.sql.execution.QueryExecution.eagerlyExecuteCommands(QueryExecution.scala:164)
		at org.apache.spark.sql.execution.QueryExecution.$anonfun$lazyCommandExecuted$1(QueryExecution.scala:126)
		at scala.util.Try$.apply(Try.scala:217)
		at org.apache.spark.util.Utils$.doTryWithCallerStacktrace(Utils.scala:1378)
		at org.apache.spark.util.LazyTry.tryT$lzycompute(LazyTry.scala:46)
		at org.apache.spark.util.LazyTry.tryT(LazyTry.scala:46)
		... 19 more
Caused by: java.lang.ClassNotFoundException: Class org.apache.hadoop.fs.s3a.S3AFileSystem not found
	at org.apache.hadoop.conf.Configuration.getClassByName(Configuration.java:2641)
	at org.apache.hadoop.conf.Configuration.getClass(Configuration.java:2735)
	at org.apache.hadoop.fs.FileSystem.getFileSystemClass(FileSystem.java:3569)
	at org.apache.hadoop.fs.FileSystem.createFileSystem(FileSystem.java:3612)
	at org.apache.hadoop.fs.FileSystem.access$300(FileSystem.java:172)
	at org.apache.hadoop.fs.FileSystem$Cache.getInternal(FileSystem.java:3716)
	at org.apache.hadoop.fs.FileSystem$Cache.get(FileSystem.java:3667)
	at org.apache.hadoop.fs.FileSystem.get(FileSystem.java:557)
	at org.apache.hadoop.fs.Path.getFileSystem(Path.java:366)
	at org.apache.spark.sql.delta.DeltaLog$.apply(DeltaLog.scala:966)
	at org.apache.spark.sql.delta.DeltaLog$.forTable(DeltaLog.scala:801)
	at org.apache.spark.sql.delta.sources.DeltaDataSource.createRelation(DeltaDataSource.scala:197)
	at org.apache.spark.sql.execution.datasources.SaveIntoDataSourceCommand.run(SaveIntoDataSourceCommand.scala:55)
	at org.apache.spark.sql.execution.command.ExecutedCommandExec.sideEffectResult$lzycompute(commands.scala:79)
	at org.apache.spark.sql.execution.command.ExecutedCommandExec.sideEffectResult(commands.scala:77)
	at org.apache.spark.sql.execution.command.ExecutedCommandExec.executeCollect(commands.scala:88)
	at org.apache.spark.sql.execution.QueryExecution.$anonfun$eagerlyExecuteCommands$2(QueryExecution.scala:155)
	at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId0$8(SQLExecution.scala:162)
	at org.apache.spark.sql.execution.SQLExecution$.withSessionTagsApplied(SQLExecution.scala:268)
	at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId0$7(SQLExecution.scala:124)
	at org.apache.spark.JobArtifactSet$.withActiveJobArtifactState(JobArtifactSet.scala:94)
	at org.apache.spark.sql.artifact.ArtifactManager.$anonfun$withResources$1(ArtifactManager.scala:112)
	at org.apache.spark.sql.artifact.ArtifactManager.withClassLoaderIfNeeded(ArtifactManager.scala:106)
	at org.apache.spark.sql.artifact.ArtifactManager.withResources(ArtifactManager.scala:111)
	at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId0$6(SQLExecution.scala:124)
	at org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:291)
	at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId0$1(SQLExecution.scala:123)
	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:804)
	at org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId0(SQLExecution.scala:77)
	at org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:233)
	at org.apache.spark.sql.execution.QueryExecution.$anonfun$eagerlyExecuteCommands$1(QueryExecution.scala:155)
	at org.apache.spark.sql.execution.QueryExecution$.withInternalError(QueryExecution.scala:654)
	at org.apache.spark.sql.execution.QueryExecution.org$apache$spark$sql$execution$QueryExecution$$eagerlyExecute$1(QueryExecution.scala:154)
	at org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$3.applyOrElse(QueryExecution.scala:169)
	at org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$3.applyOrElse(QueryExecution.scala:164)
	at org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$transformDownWithPruning$1(TreeNode.scala:470)
	at org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(origin.scala:86)
	at org.apache.spark.sql.catalyst.trees.TreeNode.transformDownWithPruning(TreeNode.scala:470)
	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.org$apache$spark$sql$catalyst$plans$logical$AnalysisHelper$$super$transformDownWithPruning(LogicalPlan.scala:37)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning(AnalysisHelper.scala:360)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning$(AnalysisHelper.scala:356)
	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:37)
	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:37)
	at org.apache.spark.sql.catalyst.trees.TreeNode.transformDown(TreeNode.scala:446)
	at org.apache.spark.sql.execution.QueryExecution.eagerlyExecuteCommands(QueryExecution.scala:164)
	at org.apache.spark.sql.execution.QueryExecution.$anonfun$lazyCommandExecuted$1(QueryExecution.scala:126)
	at scala.util.Try$.apply(Try.scala:217)
	at org.apache.spark.util.Utils$.doTryWithCallerStacktrace(Utils.scala:1378)
	at org.apache.spark.util.LazyTry.tryT$lzycompute(LazyTry.scala:46)
	at org.apache.spark.util.LazyTry.tryT(LazyTry.scala:46)
	... 19 more
2025-07-23 11:23:38,280 INFO: Closing down clientserver connection
2025-07-23 11:50:32,790 ERROR: Traceback (most recent call last):
2025-07-23 11:50:32,790 ERROR: File "/home/matteo/PycharmProjects/Tesi/BDA_Scripts/retention_area/8_dw_feeder.py", line 233, in <module>
2025-07-23 11:50:32,790 ERROR: backup_bronze(spark, claims, access_key, secret_key, endpoint, s3_path)
2025-07-23 11:50:32,790 ERROR: File "/home/matteo/PycharmProjects/Tesi/BDA_Scripts/retention_area/8_dw_feeder.py", line 45, in backup_bronze
2025-07-23 11:50:32,791 ERROR: df_current.write.format("delta").mode("append").partitionBy("ingestion_date").save(s3_path)
2025-07-23 11:50:32,791 ERROR: File "/home/matteo/PycharmProjects/Tesi/.venv1/lib/python3.12/site-packages/pyspark/sql/readwriter.py", line 1745, in save
2025-07-23 11:50:32,791 ERROR: self._jwrite.save(path)
2025-07-23 11:50:32,791 ERROR: File "/home/matteo/PycharmProjects/Tesi/.venv1/lib/python3.12/site-packages/py4j/java_gateway.py", line 1362, in __call__
2025-07-23 11:50:32,791 ERROR: return_value = get_return_value(
2025-07-23 11:50:32,791 ERROR: ^
2025-07-23 11:50:32,792 ERROR: ^
2025-07-23 11:50:32,792 ERROR: ^
2025-07-23 11:50:32,792 ERROR: ^
2025-07-23 11:50:32,792 ERROR: ^
2025-07-23 11:50:32,792 ERROR: ^
2025-07-23 11:50:32,792 ERROR: ^
2025-07-23 11:50:32,792 ERROR: ^
2025-07-23 11:50:32,792 ERROR: ^
2025-07-23 11:50:32,792 ERROR: ^
2025-07-23 11:50:32,792 ERROR: ^
2025-07-23 11:50:32,792 ERROR: ^
2025-07-23 11:50:32,792 ERROR: ^
2025-07-23 11:50:32,792 ERROR: ^
2025-07-23 11:50:32,792 ERROR: ^
2025-07-23 11:50:32,792 ERROR: ^
2025-07-23 11:50:32,792 ERROR: ^
2025-07-23 11:50:32,792 ERROR: File "/home/matteo/PycharmProjects/Tesi/.venv1/lib/python3.12/site-packages/pyspark/errors/exceptions/captured.py", line 288, in deco
2025-07-23 11:50:32,792 ERROR: raise converted from None
2025-07-23 11:50:32,792 ERROR: pyspark.errors.exceptions.captured
2025-07-23 11:50:32,792 ERROR: .
2025-07-23 11:50:32,792 ERROR: NumberFormatException
2025-07-23 11:50:32,796 ERROR: :
2025-07-23 11:50:32,797 ERROR: For input string: "60s"
2025-07-23 11:50:32,797 INFO: Closing down clientserver connection
2025-07-23 11:54:29,406 ERROR: Traceback (most recent call last):
2025-07-23 11:54:29,406 ERROR: File "/home/matteo/PycharmProjects/Tesi/BDA_Scripts/retention_area/8_dw_feeder.py", line 234, in <module>
2025-07-23 11:54:29,406 ERROR: backup_bronze(spark, claims, access_key, secret_key, endpoint, s3_path)
2025-07-23 11:54:29,406 ERROR: File "/home/matteo/PycharmProjects/Tesi/BDA_Scripts/retention_area/8_dw_feeder.py", line 45, in backup_bronze
2025-07-23 11:54:29,407 ERROR: df_current.write.format("delta").mode("append").partitionBy("ingestion_date").save(s3_path)
2025-07-23 11:54:29,407 ERROR: File "/home/matteo/PycharmProjects/Tesi/.venv1/lib/python3.12/site-packages/pyspark/sql/readwriter.py", line 1745, in save
2025-07-23 11:54:29,407 ERROR: self._jwrite.save(path)
2025-07-23 11:54:29,407 ERROR: File "/home/matteo/PycharmProjects/Tesi/.venv1/lib/python3.12/site-packages/py4j/java_gateway.py", line 1362, in __call__
2025-07-23 11:54:29,407 ERROR: return_value = get_return_value(
2025-07-23 11:54:29,407 ERROR: ^
2025-07-23 11:54:29,407 ERROR: ^
2025-07-23 11:54:29,408 ERROR: ^
2025-07-23 11:54:29,408 ERROR: ^
2025-07-23 11:54:29,408 ERROR: ^
2025-07-23 11:54:29,408 ERROR: ^
2025-07-23 11:54:29,408 ERROR: ^
2025-07-23 11:54:29,408 ERROR: ^
2025-07-23 11:54:29,408 ERROR: ^
2025-07-23 11:54:29,408 ERROR: ^
2025-07-23 11:54:29,408 ERROR: ^
2025-07-23 11:54:29,408 ERROR: ^
2025-07-23 11:54:29,408 ERROR: ^
2025-07-23 11:54:29,408 ERROR: ^
2025-07-23 11:54:29,408 ERROR: ^
2025-07-23 11:54:29,408 ERROR: ^
2025-07-23 11:54:29,408 ERROR: ^
2025-07-23 11:54:29,408 ERROR: File "/home/matteo/PycharmProjects/Tesi/.venv1/lib/python3.12/site-packages/pyspark/errors/exceptions/captured.py", line 288, in deco
2025-07-23 11:54:29,408 ERROR: raise converted from None
2025-07-23 11:54:29,408 ERROR: pyspark.errors.exceptions.captured
2025-07-23 11:54:29,408 ERROR: .
2025-07-23 11:54:29,408 ERROR: NumberFormatException
2025-07-23 11:54:29,412 ERROR: :
2025-07-23 11:54:29,412 ERROR: For input string: "60s"
2025-07-23 11:54:29,412 INFO: Closing down clientserver connection
2025-07-23 12:05:17,634 ERROR: Traceback (most recent call last):
2025-07-23 12:05:17,635 ERROR: File "/home/matteo/PycharmProjects/Tesi/BDA_Scripts/retention_area/8_dw_feeder.py", line 227, in <module>
2025-07-23 12:05:17,635 ERROR: backup_bronze(spark, claims, access_key, secret_key, endpoint, s3_path)
2025-07-23 12:05:17,635 ERROR: File "/home/matteo/PycharmProjects/Tesi/BDA_Scripts/retention_area/8_dw_feeder.py", line 45, in backup_bronze
2025-07-23 12:05:17,635 ERROR: df_current.write.format("delta").mode("append").partitionBy("ingestion_date").save(s3_path)
2025-07-23 12:05:17,636 ERROR: File "/home/matteo/PycharmProjects/Tesi/.venv1/lib/python3.12/site-packages/pyspark/sql/readwriter.py", line 1745, in save
2025-07-23 12:05:17,636 ERROR: self._jwrite.save(path)
2025-07-23 12:05:17,636 ERROR: File "/home/matteo/PycharmProjects/Tesi/.venv1/lib/python3.12/site-packages/py4j/java_gateway.py", line 1362, in __call__
2025-07-23 12:05:17,637 ERROR: return_value = get_return_value(
2025-07-23 12:05:17,637 ERROR: ^
2025-07-23 12:05:17,637 ERROR: ^
2025-07-23 12:05:17,637 ERROR: ^
2025-07-23 12:05:17,637 ERROR: ^
2025-07-23 12:05:17,637 ERROR: ^
2025-07-23 12:05:17,637 ERROR: ^
2025-07-23 12:05:17,637 ERROR: ^
2025-07-23 12:05:17,637 ERROR: ^
2025-07-23 12:05:17,637 ERROR: ^
2025-07-23 12:05:17,637 ERROR: ^
2025-07-23 12:05:17,637 ERROR: ^
2025-07-23 12:05:17,637 ERROR: ^
2025-07-23 12:05:17,637 ERROR: ^
2025-07-23 12:05:17,637 ERROR: ^
2025-07-23 12:05:17,637 ERROR: ^
2025-07-23 12:05:17,637 ERROR: ^
2025-07-23 12:05:17,637 ERROR: ^
2025-07-23 12:05:17,637 ERROR: File "/home/matteo/PycharmProjects/Tesi/.venv1/lib/python3.12/site-packages/pyspark/errors/exceptions/captured.py", line 288, in deco
2025-07-23 12:05:17,637 ERROR: raise converted from None
2025-07-23 12:05:17,637 ERROR: pyspark.errors.exceptions.captured
2025-07-23 12:05:17,637 ERROR: .
2025-07-23 12:05:17,637 ERROR: NumberFormatException
2025-07-23 12:05:17,642 ERROR: :
2025-07-23 12:05:17,642 ERROR: For input string: "60s"
2025-07-23 12:05:17,642 INFO: Closing down clientserver connection
2025-07-23 12:08:53,159 ERROR: Traceback (most recent call last):
2025-07-23 12:08:53,159 ERROR: File "/home/matteo/PycharmProjects/Tesi/BDA_Scripts/retention_area/8_dw_feeder.py", line 225, in <module>
2025-07-23 12:08:53,159 ERROR: backup_bronze(spark, claims, access_key, secret_key, endpoint, s3_path)
2025-07-23 12:08:53,159 ERROR: File "/home/matteo/PycharmProjects/Tesi/BDA_Scripts/retention_area/8_dw_feeder.py", line 45, in backup_bronze
2025-07-23 12:08:53,159 ERROR: df_current.write.format("delta").mode("append").partitionBy("ingestion_date").save(s3_path)
2025-07-23 12:08:53,160 ERROR: File "/home/matteo/PycharmProjects/Tesi/.venv1/lib/python3.12/site-packages/pyspark/sql/readwriter.py", line 1745, in save
2025-07-23 12:08:53,160 ERROR: self._jwrite.save(path)
2025-07-23 12:08:53,160 ERROR: File "/home/matteo/PycharmProjects/Tesi/.venv1/lib/python3.12/site-packages/py4j/java_gateway.py", line 1362, in __call__
2025-07-23 12:08:53,160 ERROR: return_value = get_return_value(
2025-07-23 12:08:53,160 ERROR: ^
2025-07-23 12:08:53,160 ERROR: ^
2025-07-23 12:08:53,160 ERROR: ^
2025-07-23 12:08:53,160 ERROR: ^
2025-07-23 12:08:53,160 ERROR: ^
2025-07-23 12:08:53,160 ERROR: ^
2025-07-23 12:08:53,160 ERROR: ^
2025-07-23 12:08:53,160 ERROR: ^
2025-07-23 12:08:53,160 ERROR: ^
2025-07-23 12:08:53,160 ERROR: ^
2025-07-23 12:08:53,160 ERROR: ^
2025-07-23 12:08:53,160 ERROR: ^
2025-07-23 12:08:53,161 ERROR: ^
2025-07-23 12:08:53,161 ERROR: ^
2025-07-23 12:08:53,161 ERROR: ^
2025-07-23 12:08:53,161 ERROR: ^
2025-07-23 12:08:53,161 ERROR: ^
2025-07-23 12:08:53,161 ERROR: File "/home/matteo/PycharmProjects/Tesi/.venv1/lib/python3.12/site-packages/pyspark/errors/exceptions/captured.py", line 288, in deco
2025-07-23 12:08:53,161 ERROR: raise converted from None
2025-07-23 12:08:53,161 ERROR: pyspark.errors.exceptions.captured
2025-07-23 12:08:53,161 ERROR: .
2025-07-23 12:08:53,161 ERROR: NumberFormatException
2025-07-23 12:08:53,165 ERROR: :
2025-07-23 12:08:53,166 ERROR: For input string: "60s"
2025-07-23 12:08:53,166 INFO: Closing down clientserver connection
2025-07-23 12:37:44,501 ERROR: Traceback (most recent call last):
2025-07-23 12:37:44,501 ERROR: File "/home/matteo/PycharmProjects/Tesi/BDA_Scripts/retention_area/8_dw_feeder.py", line 234, in <module>
2025-07-23 12:37:44,502 ERROR: backup_bronze(claims, access_key, secret_key, s3_name, s3_path)
2025-07-23 12:37:44,502 ERROR: TypeError
2025-07-23 12:37:44,502 ERROR: :
2025-07-23 12:37:44,502 ERROR: backup_bronze() missing 1 required positional argument: 's3_path'
2025-07-23 12:37:44,502 INFO: Closing down clientserver connection
2025-07-23 12:38:28,267 INFO: Errore caricando /home/matteo/BDA_project/ingestion_area/current/bronze/claims/_delta_log/.00000000000000000000.json.crc: Failed to upload /home/matteo/BDA_project/ingestion_area/current/bronze/claims/_delta_log/.00000000000000000000.json.crc to telematicbackup/s3a://telematicbackup/claims/_delta_log/.00000000000000000000.json.crc: An error occurred (PermanentRedirect) when calling the PutObject operation: The bucket you are attempting to access must be addressed using the specified endpoint. Please send all future requests to this endpoint.
2025-07-23 12:38:29,288 INFO: Errore caricando /home/matteo/BDA_project/ingestion_area/current/bronze/claims/_delta_log/.00000000000000000000.crc.crc: Failed to upload /home/matteo/BDA_project/ingestion_area/current/bronze/claims/_delta_log/.00000000000000000000.crc.crc to telematicbackup/s3a://telematicbackup/claims/_delta_log/.00000000000000000000.crc.crc: An error occurred (PermanentRedirect) when calling the PutObject operation: The bucket you are attempting to access must be addressed using the specified endpoint. Please send all future requests to this endpoint.
2025-07-23 12:38:30,334 INFO: Errore caricando /home/matteo/BDA_project/ingestion_area/current/bronze/claims/_delta_log/00000000000000000000.json: Failed to upload /home/matteo/BDA_project/ingestion_area/current/bronze/claims/_delta_log/00000000000000000000.json to telematicbackup/s3a://telematicbackup/claims/_delta_log/00000000000000000000.json: An error occurred (PermanentRedirect) when calling the PutObject operation: The bucket you are attempting to access must be addressed using the specified endpoint. Please send all future requests to this endpoint.
2025-07-23 12:38:31,234 INFO: Errore caricando /home/matteo/BDA_project/ingestion_area/current/bronze/claims/_delta_log/00000000000000000000.crc: Failed to upload /home/matteo/BDA_project/ingestion_area/current/bronze/claims/_delta_log/00000000000000000000.crc to telematicbackup/s3a://telematicbackup/claims/_delta_log/00000000000000000000.crc: An error occurred (PermanentRedirect) when calling the PutObject operation: The bucket you are attempting to access must be addressed using the specified endpoint. Please send all future requests to this endpoint.
2025-07-23 12:38:32,566 INFO: Errore caricando /home/matteo/BDA_project/ingestion_area/current/bronze/claims/ingestion_date=2025-07-23/.part-00000-d2d0833f-a279-41e8-a6df-a4715240fb2d.c000.snappy.parquet.crc: Failed to upload /home/matteo/BDA_project/ingestion_area/current/bronze/claims/ingestion_date=2025-07-23/.part-00000-d2d0833f-a279-41e8-a6df-a4715240fb2d.c000.snappy.parquet.crc to telematicbackup/s3a://telematicbackup/claims/ingestion_date=2025-07-23/.part-00000-d2d0833f-a279-41e8-a6df-a4715240fb2d.c000.snappy.parquet.crc: An error occurred (PermanentRedirect) when calling the PutObject operation: The bucket you are attempting to access must be addressed using the specified endpoint. Please send all future requests to this endpoint.
2025-07-23 12:38:33,794 INFO: Errore caricando /home/matteo/BDA_project/ingestion_area/current/bronze/claims/ingestion_date=2025-07-23/part-00000-d2d0833f-a279-41e8-a6df-a4715240fb2d.c000.snappy.parquet: Failed to upload /home/matteo/BDA_project/ingestion_area/current/bronze/claims/ingestion_date=2025-07-23/part-00000-d2d0833f-a279-41e8-a6df-a4715240fb2d.c000.snappy.parquet to telematicbackup/s3a://telematicbackup/claims/ingestion_date=2025-07-23/part-00000-d2d0833f-a279-41e8-a6df-a4715240fb2d.c000.snappy.parquet: An error occurred (PermanentRedirect) when calling the PutObject operation: The bucket you are attempting to access must be addressed using the specified endpoint. Please send all future requests to this endpoint.
2025-07-23 12:38:33,794 INFO: backup : /home/matteo/BDA_project/ingestion_area/current/bronze/claims eseguito in : s3a://telematicbackup/claims/
2025-07-23 12:38:33,854 INFO: Closing down clientserver connection
2025-07-23 12:55:14,968 INFO:  Errore caricando /home/matteo/BDA_project/ingestion_area/current/bronze/claims/_delta_log/.00000000000000000000.json.crc: Failed to upload /home/matteo/BDA_project/ingestion_area/current/bronze/claims/_delta_log/.00000000000000000000.json.crc to telematicbackup/claims/_delta_log/.00000000000000000000.json.crc: An error occurred (PermanentRedirect) when calling the PutObject operation: The bucket you are attempting to access must be addressed using the specified endpoint. Please send all future requests to this endpoint.
2025-07-23 12:55:16,402 INFO:  Errore caricando /home/matteo/BDA_project/ingestion_area/current/bronze/claims/_delta_log/.00000000000000000000.crc.crc: Failed to upload /home/matteo/BDA_project/ingestion_area/current/bronze/claims/_delta_log/.00000000000000000000.crc.crc to telematicbackup/claims/_delta_log/.00000000000000000000.crc.crc: An error occurred (PermanentRedirect) when calling the PutObject operation: The bucket you are attempting to access must be addressed using the specified endpoint. Please send all future requests to this endpoint.
2025-07-23 12:55:17,528 INFO:  Errore caricando /home/matteo/BDA_project/ingestion_area/current/bronze/claims/_delta_log/00000000000000000000.json: Failed to upload /home/matteo/BDA_project/ingestion_area/current/bronze/claims/_delta_log/00000000000000000000.json to telematicbackup/claims/_delta_log/00000000000000000000.json: An error occurred (PermanentRedirect) when calling the PutObject operation: The bucket you are attempting to access must be addressed using the specified endpoint. Please send all future requests to this endpoint.
2025-07-23 12:55:18,465 INFO:  Errore caricando /home/matteo/BDA_project/ingestion_area/current/bronze/claims/_delta_log/00000000000000000000.crc: Failed to upload /home/matteo/BDA_project/ingestion_area/current/bronze/claims/_delta_log/00000000000000000000.crc to telematicbackup/claims/_delta_log/00000000000000000000.crc: An error occurred (PermanentRedirect) when calling the PutObject operation: The bucket you are attempting to access must be addressed using the specified endpoint. Please send all future requests to this endpoint.
2025-07-23 12:55:19,372 INFO:  Errore caricando /home/matteo/BDA_project/ingestion_area/current/bronze/claims/ingestion_date=2025-07-23/.part-00000-d2d0833f-a279-41e8-a6df-a4715240fb2d.c000.snappy.parquet.crc: Failed to upload /home/matteo/BDA_project/ingestion_area/current/bronze/claims/ingestion_date=2025-07-23/.part-00000-d2d0833f-a279-41e8-a6df-a4715240fb2d.c000.snappy.parquet.crc to telematicbackup/claims/ingestion_date=2025-07-23/.part-00000-d2d0833f-a279-41e8-a6df-a4715240fb2d.c000.snappy.parquet.crc: An error occurred (PermanentRedirect) when calling the PutObject operation: The bucket you are attempting to access must be addressed using the specified endpoint. Please send all future requests to this endpoint.
2025-07-23 12:55:20,396 INFO:  Errore caricando /home/matteo/BDA_project/ingestion_area/current/bronze/claims/ingestion_date=2025-07-23/part-00000-d2d0833f-a279-41e8-a6df-a4715240fb2d.c000.snappy.parquet: Failed to upload /home/matteo/BDA_project/ingestion_area/current/bronze/claims/ingestion_date=2025-07-23/part-00000-d2d0833f-a279-41e8-a6df-a4715240fb2d.c000.snappy.parquet to telematicbackup/claims/ingestion_date=2025-07-23/part-00000-d2d0833f-a279-41e8-a6df-a4715240fb2d.c000.snappy.parquet: An error occurred (PermanentRedirect) when calling the PutObject operation: The bucket you are attempting to access must be addressed using the specified endpoint. Please send all future requests to this endpoint.
2025-07-23 12:55:20,397 ERROR: Traceback (most recent call last):
2025-07-23 12:55:20,397 ERROR: File "/home/matteo/PycharmProjects/Tesi/BDA_Scripts/retention_area/8_dw_feeder.py", line 234, in <module>
2025-07-23 12:55:20,397 ERROR: backup_bronze(claims, access_key, secret_key, bucket_name, "claims")
2025-07-23 12:55:20,397 ERROR: File "/home/matteo/PycharmProjects/Tesi/BDA_Scripts/retention_area/8_dw_feeder.py", line 60, in backup_bronze
2025-07-23 12:55:20,397 ERROR: print(f"backup : {current} eseguito in : {s3_path}")
2025-07-23 12:55:20,397 ERROR: ^
2025-07-23 12:55:20,397 ERROR: ^
2025-07-23 12:55:20,397 ERROR: ^
2025-07-23 12:55:20,397 ERROR: ^
2025-07-23 12:55:20,397 ERROR: ^
2025-07-23 12:55:20,397 ERROR: ^
2025-07-23 12:55:20,397 ERROR: ^
2025-07-23 12:55:20,397 ERROR: NameError
2025-07-23 12:55:20,397 ERROR: :
2025-07-23 12:55:20,397 ERROR: name 's3_path' is not defined
2025-07-23 12:55:20,397 ERROR: . Did you mean: 'sh_path'?
2025-07-23 12:55:20,397 INFO: Closing down clientserver connection
2025-07-23 12:55:57,058 INFO:  Errore caricando /home/matteo/BDA_project/ingestion_area/current/bronze/claims/_delta_log/.00000000000000000000.json.crc: Failed to upload /home/matteo/BDA_project/ingestion_area/current/bronze/claims/_delta_log/.00000000000000000000.json.crc to telematicbackup/claims/_delta_log/.00000000000000000000.json.crc: An error occurred (PermanentRedirect) when calling the PutObject operation: The bucket you are attempting to access must be addressed using the specified endpoint. Please send all future requests to this endpoint.
2025-07-23 12:56:03,099 INFO:  Errore caricando /home/matteo/BDA_project/ingestion_area/current/bronze/claims/_delta_log/.00000000000000000000.crc.crc: Failed to upload /home/matteo/BDA_project/ingestion_area/current/bronze/claims/_delta_log/.00000000000000000000.crc.crc to telematicbackup/claims/_delta_log/.00000000000000000000.crc.crc: An error occurred (PermanentRedirect) when calling the PutObject operation: The bucket you are attempting to access must be addressed using the specified endpoint. Please send all future requests to this endpoint.
2025-07-23 12:56:04,223 INFO:  Errore caricando /home/matteo/BDA_project/ingestion_area/current/bronze/claims/_delta_log/00000000000000000000.json: Failed to upload /home/matteo/BDA_project/ingestion_area/current/bronze/claims/_delta_log/00000000000000000000.json to telematicbackup/claims/_delta_log/00000000000000000000.json: An error occurred (PermanentRedirect) when calling the PutObject operation: The bucket you are attempting to access must be addressed using the specified endpoint. Please send all future requests to this endpoint.
2025-07-23 12:56:05,247 INFO:  Errore caricando /home/matteo/BDA_project/ingestion_area/current/bronze/claims/_delta_log/00000000000000000000.crc: Failed to upload /home/matteo/BDA_project/ingestion_area/current/bronze/claims/_delta_log/00000000000000000000.crc to telematicbackup/claims/_delta_log/00000000000000000000.crc: An error occurred (PermanentRedirect) when calling the PutObject operation: The bucket you are attempting to access must be addressed using the specified endpoint. Please send all future requests to this endpoint.
2025-07-23 12:56:06,477 INFO:  Errore caricando /home/matteo/BDA_project/ingestion_area/current/bronze/claims/ingestion_date=2025-07-23/.part-00000-d2d0833f-a279-41e8-a6df-a4715240fb2d.c000.snappy.parquet.crc: Failed to upload /home/matteo/BDA_project/ingestion_area/current/bronze/claims/ingestion_date=2025-07-23/.part-00000-d2d0833f-a279-41e8-a6df-a4715240fb2d.c000.snappy.parquet.crc to telematicbackup/claims/ingestion_date=2025-07-23/.part-00000-d2d0833f-a279-41e8-a6df-a4715240fb2d.c000.snappy.parquet.crc: An error occurred (PermanentRedirect) when calling the PutObject operation: The bucket you are attempting to access must be addressed using the specified endpoint. Please send all future requests to this endpoint.
2025-07-23 12:56:07,811 INFO:  Errore caricando /home/matteo/BDA_project/ingestion_area/current/bronze/claims/ingestion_date=2025-07-23/part-00000-d2d0833f-a279-41e8-a6df-a4715240fb2d.c000.snappy.parquet: Failed to upload /home/matteo/BDA_project/ingestion_area/current/bronze/claims/ingestion_date=2025-07-23/part-00000-d2d0833f-a279-41e8-a6df-a4715240fb2d.c000.snappy.parquet to telematicbackup/claims/ingestion_date=2025-07-23/part-00000-d2d0833f-a279-41e8-a6df-a4715240fb2d.c000.snappy.parquet: An error occurred (PermanentRedirect) when calling the PutObject operation: The bucket you are attempting to access must be addressed using the specified endpoint. Please send all future requests to this endpoint.
2025-07-23 12:56:07,811 INFO: backup : /home/matteo/BDA_project/ingestion_area/current/bronze/claims eseguito in : telematicbackup/claims
2025-07-23 12:56:08,095 INFO: Closing down clientserver connection
2025-07-23 13:01:40,888 INFO: Uploading /home/matteo/BDA_project/ingestion_area/current/bronze/claims/_delta_log/.00000000000000000000.json.crc to s3://telematicbackup/claims/_delta_log/.00000000000000000000.json.crc
2025-07-23 13:01:41,298 INFO:  Errore caricando /home/matteo/BDA_project/ingestion_area/current/bronze/claims/_delta_log/.00000000000000000000.json.crc: Failed to upload /home/matteo/BDA_project/ingestion_area/current/bronze/claims/_delta_log/.00000000000000000000.json.crc to telematicbackup/claims/_delta_log/.00000000000000000000.json.crc: An error occurred (AccessDenied) when calling the PutObject operation: User: arn:aws:iam::840935246162:user/matteo is not authorized to perform: s3:PutObject on resource: "arn:aws:s3:::telematicbackup/claims/_delta_log/.00000000000000000000.json.crc" because no identity-based policy allows the s3:PutObject action
2025-07-23 13:01:41,298 INFO: Uploading /home/matteo/BDA_project/ingestion_area/current/bronze/claims/_delta_log/.00000000000000000000.crc.crc to s3://telematicbackup/claims/_delta_log/.00000000000000000000.crc.crc
2025-07-23 13:01:41,489 INFO:  Errore caricando /home/matteo/BDA_project/ingestion_area/current/bronze/claims/_delta_log/.00000000000000000000.crc.crc: Failed to upload /home/matteo/BDA_project/ingestion_area/current/bronze/claims/_delta_log/.00000000000000000000.crc.crc to telematicbackup/claims/_delta_log/.00000000000000000000.crc.crc: An error occurred (AccessDenied) when calling the PutObject operation: User: arn:aws:iam::840935246162:user/matteo is not authorized to perform: s3:PutObject on resource: "arn:aws:s3:::telematicbackup/claims/_delta_log/.00000000000000000000.crc.crc" because no identity-based policy allows the s3:PutObject action
2025-07-23 13:01:41,489 INFO: Uploading /home/matteo/BDA_project/ingestion_area/current/bronze/claims/_delta_log/00000000000000000000.json to s3://telematicbackup/claims/_delta_log/00000000000000000000.json
2025-07-23 13:01:41,661 INFO:  Errore caricando /home/matteo/BDA_project/ingestion_area/current/bronze/claims/_delta_log/00000000000000000000.json: Failed to upload /home/matteo/BDA_project/ingestion_area/current/bronze/claims/_delta_log/00000000000000000000.json to telematicbackup/claims/_delta_log/00000000000000000000.json: An error occurred (AccessDenied) when calling the PutObject operation: User: arn:aws:iam::840935246162:user/matteo is not authorized to perform: s3:PutObject on resource: "arn:aws:s3:::telematicbackup/claims/_delta_log/00000000000000000000.json" because no identity-based policy allows the s3:PutObject action
2025-07-23 13:01:41,661 INFO: Uploading /home/matteo/BDA_project/ingestion_area/current/bronze/claims/_delta_log/00000000000000000000.crc to s3://telematicbackup/claims/_delta_log/00000000000000000000.crc
2025-07-23 13:01:41,846 INFO:  Errore caricando /home/matteo/BDA_project/ingestion_area/current/bronze/claims/_delta_log/00000000000000000000.crc: Failed to upload /home/matteo/BDA_project/ingestion_area/current/bronze/claims/_delta_log/00000000000000000000.crc to telematicbackup/claims/_delta_log/00000000000000000000.crc: An error occurred (AccessDenied) when calling the PutObject operation: User: arn:aws:iam::840935246162:user/matteo is not authorized to perform: s3:PutObject on resource: "arn:aws:s3:::telematicbackup/claims/_delta_log/00000000000000000000.crc" because no identity-based policy allows the s3:PutObject action
2025-07-23 13:01:41,846 INFO: Uploading /home/matteo/BDA_project/ingestion_area/current/bronze/claims/ingestion_date=2025-07-23/.part-00000-d2d0833f-a279-41e8-a6df-a4715240fb2d.c000.snappy.parquet.crc to s3://telematicbackup/claims/ingestion_date=2025-07-23/.part-00000-d2d0833f-a279-41e8-a6df-a4715240fb2d.c000.snappy.parquet.crc
2025-07-23 13:01:42,047 INFO:  Errore caricando /home/matteo/BDA_project/ingestion_area/current/bronze/claims/ingestion_date=2025-07-23/.part-00000-d2d0833f-a279-41e8-a6df-a4715240fb2d.c000.snappy.parquet.crc: Failed to upload /home/matteo/BDA_project/ingestion_area/current/bronze/claims/ingestion_date=2025-07-23/.part-00000-d2d0833f-a279-41e8-a6df-a4715240fb2d.c000.snappy.parquet.crc to telematicbackup/claims/ingestion_date=2025-07-23/.part-00000-d2d0833f-a279-41e8-a6df-a4715240fb2d.c000.snappy.parquet.crc: An error occurred (AccessDenied) when calling the PutObject operation: User: arn:aws:iam::840935246162:user/matteo is not authorized to perform: s3:PutObject on resource: "arn:aws:s3:::telematicbackup/claims/ingestion_date=2025-07-23/.part-00000-d2d0833f-a279-41e8-a6df-a4715240fb2d.c000.snappy.parquet.crc" because no identity-based policy allows the s3:PutObject action
2025-07-23 13:01:42,047 INFO: Uploading /home/matteo/BDA_project/ingestion_area/current/bronze/claims/ingestion_date=2025-07-23/part-00000-d2d0833f-a279-41e8-a6df-a4715240fb2d.c000.snappy.parquet to s3://telematicbackup/claims/ingestion_date=2025-07-23/part-00000-d2d0833f-a279-41e8-a6df-a4715240fb2d.c000.snappy.parquet
2025-07-23 13:01:42,261 INFO:  Errore caricando /home/matteo/BDA_project/ingestion_area/current/bronze/claims/ingestion_date=2025-07-23/part-00000-d2d0833f-a279-41e8-a6df-a4715240fb2d.c000.snappy.parquet: Failed to upload /home/matteo/BDA_project/ingestion_area/current/bronze/claims/ingestion_date=2025-07-23/part-00000-d2d0833f-a279-41e8-a6df-a4715240fb2d.c000.snappy.parquet to telematicbackup/claims/ingestion_date=2025-07-23/part-00000-d2d0833f-a279-41e8-a6df-a4715240fb2d.c000.snappy.parquet: An error occurred (AccessDenied) when calling the PutObject operation: User: arn:aws:iam::840935246162:user/matteo is not authorized to perform: s3:PutObject on resource: "arn:aws:s3:::telematicbackup/claims/ingestion_date=2025-07-23/part-00000-d2d0833f-a279-41e8-a6df-a4715240fb2d.c000.snappy.parquet" because no identity-based policy allows the s3:PutObject action
2025-07-23 13:01:42,261 INFO: backup : /home/matteo/BDA_project/ingestion_area/current/bronze/claims eseguito in : telematicbackup/claims
2025-07-23 13:01:42,494 INFO: Closing down clientserver connection
2025-07-23 13:07:47,361 INFO: Uploading /home/matteo/BDA_project/ingestion_area/current/bronze/claims/_delta_log/.00000000000000000000.json.crc to s3://telematicbackup/claims/_delta_log/.00000000000000000000.json.crc
2025-07-23 13:07:47,848 INFO: Uploading /home/matteo/BDA_project/ingestion_area/current/bronze/claims/_delta_log/.00000000000000000000.crc.crc to s3://telematicbackup/claims/_delta_log/.00000000000000000000.crc.crc
2025-07-23 13:07:47,974 INFO: Uploading /home/matteo/BDA_project/ingestion_area/current/bronze/claims/_delta_log/00000000000000000000.json to s3://telematicbackup/claims/_delta_log/00000000000000000000.json
2025-07-23 13:07:48,104 INFO: Uploading /home/matteo/BDA_project/ingestion_area/current/bronze/claims/_delta_log/00000000000000000000.crc to s3://telematicbackup/claims/_delta_log/00000000000000000000.crc
2025-07-23 13:07:48,241 INFO: Uploading /home/matteo/BDA_project/ingestion_area/current/bronze/claims/ingestion_date=2025-07-23/.part-00000-d2d0833f-a279-41e8-a6df-a4715240fb2d.c000.snappy.parquet.crc to s3://telematicbackup/claims/ingestion_date=2025-07-23/.part-00000-d2d0833f-a279-41e8-a6df-a4715240fb2d.c000.snappy.parquet.crc
2025-07-23 13:07:48,370 INFO: Uploading /home/matteo/BDA_project/ingestion_area/current/bronze/claims/ingestion_date=2025-07-23/part-00000-d2d0833f-a279-41e8-a6df-a4715240fb2d.c000.snappy.parquet to s3://telematicbackup/claims/ingestion_date=2025-07-23/part-00000-d2d0833f-a279-41e8-a6df-a4715240fb2d.c000.snappy.parquet
2025-07-23 13:07:48,833 INFO: backup : /home/matteo/BDA_project/ingestion_area/current/bronze/claims eseguito in : telematicbackup/claims
2025-07-23 13:07:48,979 INFO: Closing down clientserver connection
2025-07-23 13:12:02,720 INFO: Caricato: /home/matteo/BDA_project/ingestion_area/current/bronze/claims/_delta_log/.00000000000000000000.json.crc  s3://telematicbackup/claims/backup_2025-07-23/_delta_log/.00000000000000000000.json.crc
2025-07-23 13:12:02,842 INFO: Caricato: /home/matteo/BDA_project/ingestion_area/current/bronze/claims/_delta_log/.00000000000000000000.crc.crc  s3://telematicbackup/claims/backup_2025-07-23/_delta_log/.00000000000000000000.crc.crc
2025-07-23 13:12:02,986 INFO: Caricato: /home/matteo/BDA_project/ingestion_area/current/bronze/claims/_delta_log/00000000000000000000.json  s3://telematicbackup/claims/backup_2025-07-23/_delta_log/00000000000000000000.json
2025-07-23 13:12:03,119 INFO: Caricato: /home/matteo/BDA_project/ingestion_area/current/bronze/claims/_delta_log/00000000000000000000.crc  s3://telematicbackup/claims/backup_2025-07-23/_delta_log/00000000000000000000.crc
2025-07-23 13:12:03,271 INFO: Caricato: /home/matteo/BDA_project/ingestion_area/current/bronze/claims/ingestion_date=2025-07-23/.part-00000-d2d0833f-a279-41e8-a6df-a4715240fb2d.c000.snappy.parquet.crc  s3://telematicbackup/claims/backup_2025-07-23/ingestion_date=2025-07-23/.part-00000-d2d0833f-a279-41e8-a6df-a4715240fb2d.c000.snappy.parquet.crc
2025-07-23 13:12:03,798 INFO: Caricato: /home/matteo/BDA_project/ingestion_area/current/bronze/claims/ingestion_date=2025-07-23/part-00000-d2d0833f-a279-41e8-a6df-a4715240fb2d.c000.snappy.parquet  s3://telematicbackup/claims/backup_2025-07-23/ingestion_date=2025-07-23/part-00000-d2d0833f-a279-41e8-a6df-a4715240fb2d.c000.snappy.parquet
2025-07-23 13:12:03,798 INFO: backup : /home/matteo/BDA_project/ingestion_area/current/bronze/claims eseguito in : telematicbackup/claims
2025-07-23 13:12:03,852 INFO: Closing down clientserver connection
2025-07-23 13:14:06,001 INFO: Caricato: /home/matteo/BDA_project/ingestion_area/current/bronze/claims/_delta_log/.00000000000000000000.json.crc  s3://telematicbackup/claims/backup_2025-07-23T13-14-05/_delta_log/.00000000000000000000.json.crc
2025-07-23 13:14:06,131 INFO: Caricato: /home/matteo/BDA_project/ingestion_area/current/bronze/claims/_delta_log/.00000000000000000000.crc.crc  s3://telematicbackup/claims/backup_2025-07-23T13-14-05/_delta_log/.00000000000000000000.crc.crc
2025-07-23 13:14:06,271 INFO: Caricato: /home/matteo/BDA_project/ingestion_area/current/bronze/claims/_delta_log/00000000000000000000.json  s3://telematicbackup/claims/backup_2025-07-23T13-14-05/_delta_log/00000000000000000000.json
2025-07-23 13:14:06,407 INFO: Caricato: /home/matteo/BDA_project/ingestion_area/current/bronze/claims/_delta_log/00000000000000000000.crc  s3://telematicbackup/claims/backup_2025-07-23T13-14-05/_delta_log/00000000000000000000.crc
2025-07-23 13:14:06,552 INFO: Caricato: /home/matteo/BDA_project/ingestion_area/current/bronze/claims/ingestion_date=2025-07-23/.part-00000-d2d0833f-a279-41e8-a6df-a4715240fb2d.c000.snappy.parquet.crc  s3://telematicbackup/claims/backup_2025-07-23T13-14-05/ingestion_date=2025-07-23/.part-00000-d2d0833f-a279-41e8-a6df-a4715240fb2d.c000.snappy.parquet.crc
2025-07-23 13:14:07,028 INFO: Caricato: /home/matteo/BDA_project/ingestion_area/current/bronze/claims/ingestion_date=2025-07-23/part-00000-d2d0833f-a279-41e8-a6df-a4715240fb2d.c000.snappy.parquet  s3://telematicbackup/claims/backup_2025-07-23T13-14-05/ingestion_date=2025-07-23/part-00000-d2d0833f-a279-41e8-a6df-a4715240fb2d.c000.snappy.parquet
2025-07-23 13:14:07,029 INFO: backup : /home/matteo/BDA_project/ingestion_area/current/bronze/claims eseguito in : telematicbackup/claims
2025-07-23 13:14:07,570 INFO: Closing down clientserver connection
2025-07-23 13:16:18,293 INFO: Caricato: /home/matteo/BDA_project/ingestion_area/current/bronze/claims/_delta_log/.00000000000000000000.json.crc  s3://telematicbackup/claims/backup_2025-07-23T13-16-17/_delta_log/.00000000000000000000.json.crc
2025-07-23 13:16:18,430 INFO: Caricato: /home/matteo/BDA_project/ingestion_area/current/bronze/claims/_delta_log/.00000000000000000000.crc.crc  s3://telematicbackup/claims/backup_2025-07-23T13-16-17/_delta_log/.00000000000000000000.crc.crc
2025-07-23 13:16:18,571 INFO: Caricato: /home/matteo/BDA_project/ingestion_area/current/bronze/claims/_delta_log/00000000000000000000.json  s3://telematicbackup/claims/backup_2025-07-23T13-16-17/_delta_log/00000000000000000000.json
2025-07-23 13:16:18,713 INFO: Caricato: /home/matteo/BDA_project/ingestion_area/current/bronze/claims/_delta_log/00000000000000000000.crc  s3://telematicbackup/claims/backup_2025-07-23T13-16-17/_delta_log/00000000000000000000.crc
2025-07-23 13:16:18,856 INFO: Caricato: /home/matteo/BDA_project/ingestion_area/current/bronze/claims/ingestion_date=2025-07-23/.part-00000-d2d0833f-a279-41e8-a6df-a4715240fb2d.c000.snappy.parquet.crc  s3://telematicbackup/claims/backup_2025-07-23T13-16-17/ingestion_date=2025-07-23/.part-00000-d2d0833f-a279-41e8-a6df-a4715240fb2d.c000.snappy.parquet.crc
2025-07-23 13:16:19,393 INFO: Caricato: /home/matteo/BDA_project/ingestion_area/current/bronze/claims/ingestion_date=2025-07-23/part-00000-d2d0833f-a279-41e8-a6df-a4715240fb2d.c000.snappy.parquet  s3://telematicbackup/claims/backup_2025-07-23T13-16-17/ingestion_date=2025-07-23/part-00000-d2d0833f-a279-41e8-a6df-a4715240fb2d.c000.snappy.parquet
2025-07-23 13:16:19,394 INFO: backup : /home/matteo/BDA_project/ingestion_area/current/bronze/claims eseguito in : telematicbackup/claims
2025-07-23 13:16:19,870 INFO: Caricato: /home/matteo/BDA_project/ingestion_area/current/bronze/telematics/province/_delta_log/.00000000000000000000.json.crc  s3://telematicbackup/province/backup_2025-07-23T13-16-19/_delta_log/.00000000000000000000.json.crc
2025-07-23 13:16:20,013 INFO: Caricato: /home/matteo/BDA_project/ingestion_area/current/bronze/telematics/province/_delta_log/.00000000000000000000.crc.crc  s3://telematicbackup/province/backup_2025-07-23T13-16-19/_delta_log/.00000000000000000000.crc.crc
2025-07-23 13:16:20,148 INFO: Caricato: /home/matteo/BDA_project/ingestion_area/current/bronze/telematics/province/_delta_log/00000000000000000000.json  s3://telematicbackup/province/backup_2025-07-23T13-16-19/_delta_log/00000000000000000000.json
2025-07-23 13:16:20,283 INFO: Caricato: /home/matteo/BDA_project/ingestion_area/current/bronze/telematics/province/_delta_log/00000000000000000000.crc  s3://telematicbackup/province/backup_2025-07-23T13-16-19/_delta_log/00000000000000000000.crc
2025-07-23 13:16:20,681 INFO: Caricato: /home/matteo/BDA_project/ingestion_area/current/bronze/telematics/province/ingestion_date=2025-07-23/part-00001-660f05a9-38b2-4cc1-a878-04283a5ce7ab.c000.snappy.parquet  s3://telematicbackup/province/backup_2025-07-23T13-16-19/ingestion_date=2025-07-23/part-00001-660f05a9-38b2-4cc1-a878-04283a5ce7ab.c000.snappy.parquet
2025-07-23 13:16:20,825 INFO: Caricato: /home/matteo/BDA_project/ingestion_area/current/bronze/telematics/province/ingestion_date=2025-07-23/.part-00001-660f05a9-38b2-4cc1-a878-04283a5ce7ab.c000.snappy.parquet.crc  s3://telematicbackup/province/backup_2025-07-23T13-16-19/ingestion_date=2025-07-23/.part-00001-660f05a9-38b2-4cc1-a878-04283a5ce7ab.c000.snappy.parquet.crc
2025-07-23 13:16:21,123 INFO: Caricato: /home/matteo/BDA_project/ingestion_area/current/bronze/telematics/province/ingestion_date=2025-07-23/part-00000-87fb8cb0-4b87-4a6c-9b6b-fea2e4e93245.c000.snappy.parquet  s3://telematicbackup/province/backup_2025-07-23T13-16-19/ingestion_date=2025-07-23/part-00000-87fb8cb0-4b87-4a6c-9b6b-fea2e4e93245.c000.snappy.parquet
2025-07-23 13:16:21,258 INFO: Caricato: /home/matteo/BDA_project/ingestion_area/current/bronze/telematics/province/ingestion_date=2025-07-23/.part-00000-87fb8cb0-4b87-4a6c-9b6b-fea2e4e93245.c000.snappy.parquet.crc  s3://telematicbackup/province/backup_2025-07-23T13-16-19/ingestion_date=2025-07-23/.part-00000-87fb8cb0-4b87-4a6c-9b6b-fea2e4e93245.c000.snappy.parquet.crc
2025-07-23 13:16:21,448 INFO: Caricato: /home/matteo/BDA_project/ingestion_area/current/bronze/telematics/province/ingestion_date=2025-07-23/part-00002-a9464e15-db5b-4185-8e16-abacd13c5d16.c000.snappy.parquet  s3://telematicbackup/province/backup_2025-07-23T13-16-19/ingestion_date=2025-07-23/part-00002-a9464e15-db5b-4185-8e16-abacd13c5d16.c000.snappy.parquet
2025-07-23 13:16:21,585 INFO: Caricato: /home/matteo/BDA_project/ingestion_area/current/bronze/telematics/province/ingestion_date=2025-07-23/.part-00002-a9464e15-db5b-4185-8e16-abacd13c5d16.c000.snappy.parquet.crc  s3://telematicbackup/province/backup_2025-07-23T13-16-19/ingestion_date=2025-07-23/.part-00002-a9464e15-db5b-4185-8e16-abacd13c5d16.c000.snappy.parquet.crc
2025-07-23 13:16:21,585 INFO: backup : /home/matteo/BDA_project/ingestion_area/current/bronze/telematics/province eseguito in : telematicbackup/province
2025-07-23 13:16:22,077 INFO: Caricato: /home/matteo/BDA_project/ingestion_area/current/bronze/telematics/trip_summary/_delta_log/.00000000000000000000.json.crc  s3://telematicbackup/trip_summary/backup_2025-07-23T13-16-21/_delta_log/.00000000000000000000.json.crc
2025-07-23 13:16:22,207 INFO: Caricato: /home/matteo/BDA_project/ingestion_area/current/bronze/telematics/trip_summary/_delta_log/.00000000000000000000.crc.crc  s3://telematicbackup/trip_summary/backup_2025-07-23T13-16-21/_delta_log/.00000000000000000000.crc.crc
2025-07-23 13:16:22,430 INFO: Caricato: /home/matteo/BDA_project/ingestion_area/current/bronze/telematics/trip_summary/_delta_log/00000000000000000000.json  s3://telematicbackup/trip_summary/backup_2025-07-23T13-16-21/_delta_log/00000000000000000000.json
2025-07-23 13:16:22,593 INFO: Caricato: /home/matteo/BDA_project/ingestion_area/current/bronze/telematics/trip_summary/_delta_log/00000000000000000000.crc  s3://telematicbackup/trip_summary/backup_2025-07-23T13-16-21/_delta_log/00000000000000000000.crc
2025-07-23 13:16:22,742 INFO: Caricato: /home/matteo/BDA_project/ingestion_area/current/bronze/telematics/trip_summary/ingestion_date=2025-07-23/.part-00002-4ba04416-193c-486b-a047-bc89543d04b0.c000.snappy.parquet.crc  s3://telematicbackup/trip_summary/backup_2025-07-23T13-16-21/ingestion_date=2025-07-23/.part-00002-4ba04416-193c-486b-a047-bc89543d04b0.c000.snappy.parquet.crc
2025-07-23 13:16:22,885 INFO: Caricato: /home/matteo/BDA_project/ingestion_area/current/bronze/telematics/trip_summary/ingestion_date=2025-07-23/.part-00001-221cd4dc-a16e-48f7-a11d-724614f158ee.c000.snappy.parquet.crc  s3://telematicbackup/trip_summary/backup_2025-07-23T13-16-21/ingestion_date=2025-07-23/.part-00001-221cd4dc-a16e-48f7-a11d-724614f158ee.c000.snappy.parquet.crc
2025-07-23 13:16:23,565 INFO: Caricato: /home/matteo/BDA_project/ingestion_area/current/bronze/telematics/trip_summary/ingestion_date=2025-07-23/part-00001-221cd4dc-a16e-48f7-a11d-724614f158ee.c000.snappy.parquet  s3://telematicbackup/trip_summary/backup_2025-07-23T13-16-21/ingestion_date=2025-07-23/part-00001-221cd4dc-a16e-48f7-a11d-724614f158ee.c000.snappy.parquet
2025-07-23 13:16:24,157 INFO: Caricato: /home/matteo/BDA_project/ingestion_area/current/bronze/telematics/trip_summary/ingestion_date=2025-07-23/part-00000-4c2ae860-1a0d-4383-9a21-53c53572c28a.c000.snappy.parquet  s3://telematicbackup/trip_summary/backup_2025-07-23T13-16-21/ingestion_date=2025-07-23/part-00000-4c2ae860-1a0d-4383-9a21-53c53572c28a.c000.snappy.parquet
2025-07-23 13:16:24,504 INFO: Caricato: /home/matteo/BDA_project/ingestion_area/current/bronze/telematics/trip_summary/ingestion_date=2025-07-23/part-00002-4ba04416-193c-486b-a047-bc89543d04b0.c000.snappy.parquet  s3://telematicbackup/trip_summary/backup_2025-07-23T13-16-21/ingestion_date=2025-07-23/part-00002-4ba04416-193c-486b-a047-bc89543d04b0.c000.snappy.parquet
2025-07-23 13:16:24,642 INFO: Caricato: /home/matteo/BDA_project/ingestion_area/current/bronze/telematics/trip_summary/ingestion_date=2025-07-23/.part-00000-4c2ae860-1a0d-4383-9a21-53c53572c28a.c000.snappy.parquet.crc  s3://telematicbackup/trip_summary/backup_2025-07-23T13-16-21/ingestion_date=2025-07-23/.part-00000-4c2ae860-1a0d-4383-9a21-53c53572c28a.c000.snappy.parquet.crc
2025-07-23 13:16:24,642 INFO: backup : /home/matteo/BDA_project/ingestion_area/current/bronze/telematics/trip_summary eseguito in : telematicbackup/trip_summary
2025-07-23 13:16:25,118 INFO: Caricato: /home/matteo/BDA_project/ingestion_area/current/bronze/telematics/behaviour/_delta_log/.00000000000000000000.json.crc  s3://telematicbackup/behaviour/backup_2025-07-23T13-16-24/_delta_log/.00000000000000000000.json.crc
2025-07-23 13:16:25,245 INFO: Caricato: /home/matteo/BDA_project/ingestion_area/current/bronze/telematics/behaviour/_delta_log/.00000000000000000000.crc.crc  s3://telematicbackup/behaviour/backup_2025-07-23T13-16-24/_delta_log/.00000000000000000000.crc.crc
2025-07-23 13:16:25,389 INFO: Caricato: /home/matteo/BDA_project/ingestion_area/current/bronze/telematics/behaviour/_delta_log/00000000000000000000.json  s3://telematicbackup/behaviour/backup_2025-07-23T13-16-24/_delta_log/00000000000000000000.json
2025-07-23 13:16:25,520 INFO: Caricato: /home/matteo/BDA_project/ingestion_area/current/bronze/telematics/behaviour/_delta_log/00000000000000000000.crc  s3://telematicbackup/behaviour/backup_2025-07-23T13-16-24/_delta_log/00000000000000000000.crc
2025-07-23 13:16:25,674 INFO: Caricato: /home/matteo/BDA_project/ingestion_area/current/bronze/telematics/behaviour/ingestion_date=2025-07-23/.part-00001-e394dfc1-47f9-4a2c-9cb8-b290bb72cf25.c000.snappy.parquet.crc  s3://telematicbackup/behaviour/backup_2025-07-23T13-16-24/ingestion_date=2025-07-23/.part-00001-e394dfc1-47f9-4a2c-9cb8-b290bb72cf25.c000.snappy.parquet.crc
2025-07-23 13:16:25,813 INFO: Caricato: /home/matteo/BDA_project/ingestion_area/current/bronze/telematics/behaviour/ingestion_date=2025-07-23/.part-00002-16310c72-86c3-461c-80bb-5397b3e3482c.c000.snappy.parquet.crc  s3://telematicbackup/behaviour/backup_2025-07-23T13-16-24/ingestion_date=2025-07-23/.part-00002-16310c72-86c3-461c-80bb-5397b3e3482c.c000.snappy.parquet.crc
2025-07-23 13:16:26,218 INFO: Caricato: /home/matteo/BDA_project/ingestion_area/current/bronze/telematics/behaviour/ingestion_date=2025-07-23/part-00000-e317891f-9974-4726-874c-0c05a09bb2d9.c000.snappy.parquet  s3://telematicbackup/behaviour/backup_2025-07-23T13-16-24/ingestion_date=2025-07-23/part-00000-e317891f-9974-4726-874c-0c05a09bb2d9.c000.snappy.parquet
2025-07-23 13:16:26,405 INFO: Caricato: /home/matteo/BDA_project/ingestion_area/current/bronze/telematics/behaviour/ingestion_date=2025-07-23/part-00002-16310c72-86c3-461c-80bb-5397b3e3482c.c000.snappy.parquet  s3://telematicbackup/behaviour/backup_2025-07-23T13-16-24/ingestion_date=2025-07-23/part-00002-16310c72-86c3-461c-80bb-5397b3e3482c.c000.snappy.parquet
2025-07-23 13:16:26,539 INFO: Caricato: /home/matteo/BDA_project/ingestion_area/current/bronze/telematics/behaviour/ingestion_date=2025-07-23/.part-00000-e317891f-9974-4726-874c-0c05a09bb2d9.c000.snappy.parquet.crc  s3://telematicbackup/behaviour/backup_2025-07-23T13-16-24/ingestion_date=2025-07-23/.part-00000-e317891f-9974-4726-874c-0c05a09bb2d9.c000.snappy.parquet.crc
2025-07-23 13:16:26,888 INFO: Caricato: /home/matteo/BDA_project/ingestion_area/current/bronze/telematics/behaviour/ingestion_date=2025-07-23/part-00001-e394dfc1-47f9-4a2c-9cb8-b290bb72cf25.c000.snappy.parquet  s3://telematicbackup/behaviour/backup_2025-07-23T13-16-24/ingestion_date=2025-07-23/part-00001-e394dfc1-47f9-4a2c-9cb8-b290bb72cf25.c000.snappy.parquet
2025-07-23 13:16:26,888 INFO: backup : /home/matteo/BDA_project/ingestion_area/current/bronze/telematics/behaviour eseguito in : telematicbackup/behaviour
2025-07-23 13:16:27,418 INFO: Closing down clientserver connection
2025-07-24 22:24:19,748 ERROR: Traceback (most recent call last):
2025-07-24 22:24:19,753 ERROR: File "/home/matteo/PycharmProjects/Tesi/BDA_Scripts/retention_area/8_dw_feeder.py", line 221, in <module>
2025-07-24 22:24:19,753 ERROR: .mode("appena") \
2025-07-24 22:24:19,753 ERROR: ^
2025-07-24 22:24:19,753 ERROR: ^
2025-07-24 22:24:19,753 ERROR: ^
2025-07-24 22:24:19,753 ERROR: ^
2025-07-24 22:24:19,753 ERROR: ^
2025-07-24 22:24:19,753 ERROR: ^
2025-07-24 22:24:19,753 ERROR: ^
2025-07-24 22:24:19,753 ERROR: ^
2025-07-24 22:24:19,753 ERROR: ^
2025-07-24 22:24:19,753 ERROR: ^
2025-07-24 22:24:19,753 ERROR: ^
2025-07-24 22:24:19,753 ERROR: ^
2025-07-24 22:24:19,753 ERROR: ^
2025-07-24 22:24:19,753 ERROR: ^
2025-07-24 22:24:19,753 ERROR: File "/home/matteo/PycharmProjects/Tesi/.venv1/lib/python3.12/site-packages/pyspark/sql/readwriter.py", line 1010, in mode
2025-07-24 22:24:19,754 ERROR: self._jwrite = self._jwrite.mode(saveMode)
2025-07-24 22:24:19,754 ERROR: ^
2025-07-24 22:24:19,754 ERROR: ^
2025-07-24 22:24:19,754 ERROR: ^
2025-07-24 22:24:19,754 ERROR: ^
2025-07-24 22:24:19,754 ERROR: ^
2025-07-24 22:24:19,754 ERROR: ^
2025-07-24 22:24:19,754 ERROR: ^
2025-07-24 22:24:19,754 ERROR: ^
2025-07-24 22:24:19,754 ERROR: ^
2025-07-24 22:24:19,754 ERROR: ^
2025-07-24 22:24:19,754 ERROR: ^
2025-07-24 22:24:19,754 ERROR: ^
2025-07-24 22:24:19,754 ERROR: ^
2025-07-24 22:24:19,754 ERROR: ^
2025-07-24 22:24:19,754 ERROR: ^
2025-07-24 22:24:19,754 ERROR: ^
2025-07-24 22:24:19,754 ERROR: ^
2025-07-24 22:24:19,754 ERROR: ^
2025-07-24 22:24:19,754 ERROR: ^
2025-07-24 22:24:19,754 ERROR: ^
2025-07-24 22:24:19,754 ERROR: ^
2025-07-24 22:24:19,754 ERROR: ^
2025-07-24 22:24:19,754 ERROR: ^
2025-07-24 22:24:19,754 ERROR: ^
2025-07-24 22:24:19,754 ERROR: ^
2025-07-24 22:24:19,754 ERROR: ^
2025-07-24 22:24:19,754 ERROR: ^
2025-07-24 22:24:19,754 ERROR: File "/home/matteo/PycharmProjects/Tesi/.venv1/lib/python3.12/site-packages/py4j/java_gateway.py", line 1322, in __call__
2025-07-24 22:24:19,754 ERROR: return_value = get_return_value(
2025-07-24 22:24:19,754 ERROR: ^
2025-07-24 22:24:19,754 ERROR: ^
2025-07-24 22:24:19,754 ERROR: ^
2025-07-24 22:24:19,754 ERROR: ^
2025-07-24 22:24:19,754 ERROR: ^
2025-07-24 22:24:19,754 ERROR: ^
2025-07-24 22:24:19,755 ERROR: ^
2025-07-24 22:24:19,755 ERROR: ^
2025-07-24 22:24:19,755 ERROR: ^
2025-07-24 22:24:19,755 ERROR: ^
2025-07-24 22:24:19,755 ERROR: ^
2025-07-24 22:24:19,755 ERROR: ^
2025-07-24 22:24:19,755 ERROR: ^
2025-07-24 22:24:19,755 ERROR: ^
2025-07-24 22:24:19,755 ERROR: ^
2025-07-24 22:24:19,755 ERROR: ^
2025-07-24 22:24:19,755 ERROR: ^
2025-07-24 22:24:19,755 ERROR: File "/home/matteo/PycharmProjects/Tesi/.venv1/lib/python3.12/site-packages/pyspark/errors/exceptions/captured.py", line 175, in deco
2025-07-24 22:24:19,755 ERROR: raise converted from None
2025-07-24 22:24:19,755 ERROR: pyspark.errors.exceptions.captured
2025-07-24 22:24:19,755 ERROR: .
2025-07-24 22:24:19,755 ERROR: IllegalArgumentException
2025-07-24 22:24:19,757 ERROR: :
2025-07-24 22:24:19,757 ERROR: Unknown save mode: appena. Accepted save modes are 'overwrite', 'append', 'ignore', 'error', 'errorifexists', 'default'.
2025-07-24 22:24:19,757 INFO: Closing down clientserver connection
2025-07-24 22:26:49,906 ERROR: Traceback (most recent call last):
2025-07-24 22:26:49,906 ERROR: File "/home/matteo/PycharmProjects/Tesi/BDA_Scripts/retention_area/8_dw_feeder.py", line 223, in <module>
2025-07-24 22:26:49,906 ERROR: .parquet("s3a://telematicbackup/claims/")
2025-07-24 22:26:49,906 ERROR: ^
2025-07-24 22:26:49,906 ERROR: ^
2025-07-24 22:26:49,906 ERROR: ^
2025-07-24 22:26:49,906 ERROR: ^
2025-07-24 22:26:49,906 ERROR: ^
2025-07-24 22:26:49,906 ERROR: ^
2025-07-24 22:26:49,906 ERROR: ^
2025-07-24 22:26:49,906 ERROR: ^
2025-07-24 22:26:49,906 ERROR: ^
2025-07-24 22:26:49,906 ERROR: ^
2025-07-24 22:26:49,906 ERROR: ^
2025-07-24 22:26:49,906 ERROR: ^
2025-07-24 22:26:49,907 ERROR: ^
2025-07-24 22:26:49,907 ERROR: ^
2025-07-24 22:26:49,907 ERROR: ^
2025-07-24 22:26:49,907 ERROR: ^
2025-07-24 22:26:49,907 ERROR: ^
2025-07-24 22:26:49,907 ERROR: ^
2025-07-24 22:26:49,907 ERROR: ^
2025-07-24 22:26:49,907 ERROR: ^
2025-07-24 22:26:49,907 ERROR: ^
2025-07-24 22:26:49,907 ERROR: ^
2025-07-24 22:26:49,907 ERROR: ^
2025-07-24 22:26:49,907 ERROR: ^
2025-07-24 22:26:49,907 ERROR: ^
2025-07-24 22:26:49,907 ERROR: ^
2025-07-24 22:26:49,907 ERROR: ^
2025-07-24 22:26:49,907 ERROR: ^
2025-07-24 22:26:49,907 ERROR: ^
2025-07-24 22:26:49,907 ERROR: ^
2025-07-24 22:26:49,907 ERROR: ^
2025-07-24 22:26:49,907 ERROR: ^
2025-07-24 22:26:49,907 ERROR: ^
2025-07-24 22:26:49,907 ERROR: ^
2025-07-24 22:26:49,907 ERROR: ^
2025-07-24 22:26:49,907 ERROR: ^
2025-07-24 22:26:49,907 ERROR: ^
2025-07-24 22:26:49,907 ERROR: ^
2025-07-24 22:26:49,907 ERROR: ^
2025-07-24 22:26:49,907 ERROR: ^
2025-07-24 22:26:49,907 ERROR: File "/home/matteo/PycharmProjects/Tesi/.venv1/lib/python3.12/site-packages/pyspark/sql/readwriter.py", line 1656, in parquet
2025-07-24 22:26:49,907 ERROR: self._jwrite.parquet(path)
2025-07-24 22:26:49,907 ERROR: File "/home/matteo/PycharmProjects/Tesi/.venv1/lib/python3.12/site-packages/py4j/java_gateway.py", line 1322, in __call__
2025-07-24 22:26:49,907 ERROR: return_value = get_return_value(
2025-07-24 22:26:49,907 ERROR: ^
2025-07-24 22:26:49,907 ERROR: ^
2025-07-24 22:26:49,907 ERROR: ^
2025-07-24 22:26:49,907 ERROR: ^
2025-07-24 22:26:49,907 ERROR: ^
2025-07-24 22:26:49,907 ERROR: ^
2025-07-24 22:26:49,907 ERROR: ^
2025-07-24 22:26:49,907 ERROR: ^
2025-07-24 22:26:49,907 ERROR: ^
2025-07-24 22:26:49,907 ERROR: ^
2025-07-24 22:26:49,907 ERROR: ^
2025-07-24 22:26:49,907 ERROR: ^
2025-07-24 22:26:49,907 ERROR: ^
2025-07-24 22:26:49,907 ERROR: ^
2025-07-24 22:26:49,907 ERROR: ^
2025-07-24 22:26:49,907 ERROR: ^
2025-07-24 22:26:49,907 ERROR: ^
2025-07-24 22:26:49,907 ERROR: File "/home/matteo/PycharmProjects/Tesi/.venv1/lib/python3.12/site-packages/pyspark/errors/exceptions/captured.py", line 169, in deco
2025-07-24 22:26:49,907 ERROR: return f(*a, **kw)
2025-07-24 22:26:49,907 ERROR: ^
2025-07-24 22:26:49,907 ERROR: ^
2025-07-24 22:26:49,907 ERROR: ^
2025-07-24 22:26:49,907 ERROR: ^
2025-07-24 22:26:49,907 ERROR: ^
2025-07-24 22:26:49,907 ERROR: ^
2025-07-24 22:26:49,907 ERROR: ^
2025-07-24 22:26:49,907 ERROR: ^
2025-07-24 22:26:49,907 ERROR: ^
2025-07-24 22:26:49,907 ERROR: ^
2025-07-24 22:26:49,907 ERROR: ^
2025-07-24 22:26:49,907 ERROR: File "/home/matteo/PycharmProjects/Tesi/.venv1/lib/python3.12/site-packages/py4j/protocol.py", line 326, in get_return_value
2025-07-24 22:26:49,907 ERROR: raise Py4JJavaError(
2025-07-24 22:26:49,907 ERROR: py4j.protocol
2025-07-24 22:26:49,907 ERROR: .
2025-07-24 22:26:49,907 ERROR: Py4JJavaError
2025-07-24 22:26:49,907 ERROR: :
2025-07-24 22:26:49,907 ERROR: An error occurred while calling o54.parquet.
: java.lang.RuntimeException: java.lang.ClassNotFoundException: Class org.apache.hadoop.fs.s3a.S3AFileSystem not found
	at org.apache.hadoop.conf.Configuration.getClass(Configuration.java:2688)
	at org.apache.hadoop.fs.FileSystem.getFileSystemClass(FileSystem.java:3431)
	at org.apache.hadoop.fs.FileSystem.createFileSystem(FileSystem.java:3466)
	at org.apache.hadoop.fs.FileSystem.access$300(FileSystem.java:174)
	at org.apache.hadoop.fs.FileSystem$Cache.getInternal(FileSystem.java:3574)
	at org.apache.hadoop.fs.FileSystem$Cache.get(FileSystem.java:3521)
	at org.apache.hadoop.fs.FileSystem.get(FileSystem.java:540)
	at org.apache.hadoop.fs.Path.getFileSystem(Path.java:365)
	at org.apache.spark.sql.execution.datasources.DataSource.planForWritingFileFormat(DataSource.scala:454)
	at org.apache.spark.sql.execution.datasources.DataSource.planForWriting(DataSource.scala:530)
	at org.apache.spark.sql.DataFrameWriter.saveToV1Source(DataFrameWriter.scala:387)
	at org.apache.spark.sql.DataFrameWriter.saveInternal(DataFrameWriter.scala:360)
	at org.apache.spark.sql.DataFrameWriter.save(DataFrameWriter.scala:239)
	at org.apache.spark.sql.DataFrameWriter.parquet(DataFrameWriter.scala:789)
	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:77)
	at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.base/java.lang.reflect.Method.invoke(Method.java:569)
	at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)
	at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)
	at py4j.Gateway.invoke(Gateway.java:282)
	at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)
	at py4j.commands.CallCommand.execute(CallCommand.java:79)
	at py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)
	at py4j.ClientServerConnection.run(ClientServerConnection.java:106)
	at java.base/java.lang.Thread.run(Thread.java:840)
Caused by: java.lang.ClassNotFoundException: Class org.apache.hadoop.fs.s3a.S3AFileSystem not found
	at org.apache.hadoop.conf.Configuration.getClassByName(Configuration.java:2592)
	at org.apache.hadoop.conf.Configuration.getClass(Configuration.java:2686)
	... 25 more
2025-07-24 22:26:49,908 INFO: Closing down clientserver connection
2025-07-24 22:40:01,435 ERROR: Traceback (most recent call last):
2025-07-24 22:40:01,435 ERROR: File "/home/matteo/PycharmProjects/Tesi/BDA_Scripts/retention_area/8_dw_feeder.py", line 245, in <module>
2025-07-24 22:40:01,435 ERROR: backup_bronze(claims, access_key, secret_key, bucket_name, "claims")
2025-07-24 22:40:01,435 ERROR: File "/home/matteo/PycharmProjects/Tesi/BDA_Scripts/retention_area/8_dw_feeder.py", line 57, in backup_bronze
2025-07-24 22:40:01,435 ERROR: result = subprocess.run(cmd, capture_output=True, text=True, check=True)
2025-07-24 22:40:01,435 ERROR: ^
2025-07-24 22:40:01,435 ERROR: ^
2025-07-24 22:40:01,435 ERROR: ^
2025-07-24 22:40:01,435 ERROR: ^
2025-07-24 22:40:01,435 ERROR: ^
2025-07-24 22:40:01,435 ERROR: ^
2025-07-24 22:40:01,435 ERROR: ^
2025-07-24 22:40:01,435 ERROR: ^
2025-07-24 22:40:01,435 ERROR: ^
2025-07-24 22:40:01,435 ERROR: ^
2025-07-24 22:40:01,435 ERROR: ^
2025-07-24 22:40:01,435 ERROR: ^
2025-07-24 22:40:01,435 ERROR: ^
2025-07-24 22:40:01,435 ERROR: ^
2025-07-24 22:40:01,435 ERROR: ^
2025-07-24 22:40:01,435 ERROR: ^
2025-07-24 22:40:01,435 ERROR: ^
2025-07-24 22:40:01,435 ERROR: ^
2025-07-24 22:40:01,435 ERROR: ^
2025-07-24 22:40:01,435 ERROR: ^
2025-07-24 22:40:01,435 ERROR: ^
2025-07-24 22:40:01,435 ERROR: ^
2025-07-24 22:40:01,435 ERROR: ^
2025-07-24 22:40:01,435 ERROR: ^
2025-07-24 22:40:01,435 ERROR: ^
2025-07-24 22:40:01,435 ERROR: ^
2025-07-24 22:40:01,435 ERROR: ^
2025-07-24 22:40:01,435 ERROR: ^
2025-07-24 22:40:01,435 ERROR: ^
2025-07-24 22:40:01,435 ERROR: ^
2025-07-24 22:40:01,435 ERROR: ^
2025-07-24 22:40:01,435 ERROR: ^
2025-07-24 22:40:01,435 ERROR: ^
2025-07-24 22:40:01,435 ERROR: ^
2025-07-24 22:40:01,435 ERROR: ^
2025-07-24 22:40:01,435 ERROR: ^
2025-07-24 22:40:01,435 ERROR: ^
2025-07-24 22:40:01,435 ERROR: ^
2025-07-24 22:40:01,435 ERROR: ^
2025-07-24 22:40:01,435 ERROR: ^
2025-07-24 22:40:01,435 ERROR: ^
2025-07-24 22:40:01,435 ERROR: ^
2025-07-24 22:40:01,435 ERROR: ^
2025-07-24 22:40:01,435 ERROR: ^
2025-07-24 22:40:01,435 ERROR: ^
2025-07-24 22:40:01,435 ERROR: ^
2025-07-24 22:40:01,435 ERROR: ^
2025-07-24 22:40:01,435 ERROR: ^
2025-07-24 22:40:01,435 ERROR: ^
2025-07-24 22:40:01,435 ERROR: ^
2025-07-24 22:40:01,435 ERROR: ^
2025-07-24 22:40:01,435 ERROR: ^
2025-07-24 22:40:01,435 ERROR: ^
2025-07-24 22:40:01,435 ERROR: ^
2025-07-24 22:40:01,435 ERROR: ^
2025-07-24 22:40:01,435 ERROR: ^
2025-07-24 22:40:01,435 ERROR: ^
2025-07-24 22:40:01,435 ERROR: ^
2025-07-24 22:40:01,435 ERROR: ^
2025-07-24 22:40:01,435 ERROR: ^
2025-07-24 22:40:01,435 ERROR: ^
2025-07-24 22:40:01,435 ERROR: ^
2025-07-24 22:40:01,435 ERROR: ^
2025-07-24 22:40:01,435 ERROR: File "/usr/lib/python3.12/subprocess.py", line 548, in run
2025-07-24 22:40:01,435 ERROR: with Popen(*popenargs, **kwargs) as process:
2025-07-24 22:40:01,435 ERROR: ^
2025-07-24 22:40:01,435 ERROR: ^
2025-07-24 22:40:01,435 ERROR: ^
2025-07-24 22:40:01,435 ERROR: ^
2025-07-24 22:40:01,435 ERROR: ^
2025-07-24 22:40:01,435 ERROR: ^
2025-07-24 22:40:01,435 ERROR: ^
2025-07-24 22:40:01,435 ERROR: ^
2025-07-24 22:40:01,435 ERROR: ^
2025-07-24 22:40:01,435 ERROR: ^
2025-07-24 22:40:01,435 ERROR: ^
2025-07-24 22:40:01,435 ERROR: ^
2025-07-24 22:40:01,435 ERROR: ^
2025-07-24 22:40:01,435 ERROR: ^
2025-07-24 22:40:01,435 ERROR: ^
2025-07-24 22:40:01,435 ERROR: ^
2025-07-24 22:40:01,435 ERROR: ^
2025-07-24 22:40:01,435 ERROR: ^
2025-07-24 22:40:01,435 ERROR: ^
2025-07-24 22:40:01,435 ERROR: ^
2025-07-24 22:40:01,435 ERROR: ^
2025-07-24 22:40:01,435 ERROR: ^
2025-07-24 22:40:01,435 ERROR: ^
2025-07-24 22:40:01,435 ERROR: ^
2025-07-24 22:40:01,435 ERROR: ^
2025-07-24 22:40:01,435 ERROR: ^
2025-07-24 22:40:01,435 ERROR: ^
2025-07-24 22:40:01,436 ERROR: File "/usr/lib/python3.12/subprocess.py", line 1026, in __init__
2025-07-24 22:40:01,436 ERROR: self._execute_child(args, executable, preexec_fn, close_fds,
2025-07-24 22:40:01,436 ERROR: File "/usr/lib/python3.12/subprocess.py", line 1955, in _execute_child
2025-07-24 22:40:01,436 ERROR: raise child_exception_type(errno_num, err_msg, err_filename)
2025-07-24 22:40:01,436 ERROR: FileNotFoundError
2025-07-24 22:40:01,436 ERROR: :
2025-07-24 22:40:01,436 ERROR: [Errno 2] No such file or directory: 'hdfs'
2025-07-24 22:40:33,817 ERROR: Traceback (most recent call last):
2025-07-24 22:40:33,817 ERROR: File "/home/matteo/PycharmProjects/Tesi/BDA_Scripts/retention_area/8_dw_feeder.py", line 245, in <module>
2025-07-24 22:40:33,817 ERROR: backup_bronze(claims, access_key, secret_key, bucket_name, "claims")
2025-07-24 22:40:33,817 ERROR: File "/home/matteo/PycharmProjects/Tesi/BDA_Scripts/retention_area/8_dw_feeder.py", line 57, in backup_bronze
2025-07-24 22:40:33,817 ERROR: result = subprocess.run(cmd, capture_output=True, text=True, check=True)
2025-07-24 22:40:33,817 ERROR: ^
2025-07-24 22:40:33,817 ERROR: ^
2025-07-24 22:40:33,817 ERROR: ^
2025-07-24 22:40:33,817 ERROR: ^
2025-07-24 22:40:33,817 ERROR: ^
2025-07-24 22:40:33,817 ERROR: ^
2025-07-24 22:40:33,817 ERROR: ^
2025-07-24 22:40:33,817 ERROR: ^
2025-07-24 22:40:33,817 ERROR: ^
2025-07-24 22:40:33,817 ERROR: ^
2025-07-24 22:40:33,817 ERROR: ^
2025-07-24 22:40:33,817 ERROR: ^
2025-07-24 22:40:33,817 ERROR: ^
2025-07-24 22:40:33,817 ERROR: ^
2025-07-24 22:40:33,817 ERROR: ^
2025-07-24 22:40:33,817 ERROR: ^
2025-07-24 22:40:33,817 ERROR: ^
2025-07-24 22:40:33,817 ERROR: ^
2025-07-24 22:40:33,817 ERROR: ^
2025-07-24 22:40:33,817 ERROR: ^
2025-07-24 22:40:33,817 ERROR: ^
2025-07-24 22:40:33,817 ERROR: ^
2025-07-24 22:40:33,817 ERROR: ^
2025-07-24 22:40:33,817 ERROR: ^
2025-07-24 22:40:33,817 ERROR: ^
2025-07-24 22:40:33,817 ERROR: ^
2025-07-24 22:40:33,817 ERROR: ^
2025-07-24 22:40:33,817 ERROR: ^
2025-07-24 22:40:33,817 ERROR: ^
2025-07-24 22:40:33,817 ERROR: ^
2025-07-24 22:40:33,817 ERROR: ^
2025-07-24 22:40:33,817 ERROR: ^
2025-07-24 22:40:33,817 ERROR: ^
2025-07-24 22:40:33,817 ERROR: ^
2025-07-24 22:40:33,817 ERROR: ^
2025-07-24 22:40:33,817 ERROR: ^
2025-07-24 22:40:33,817 ERROR: ^
2025-07-24 22:40:33,817 ERROR: ^
2025-07-24 22:40:33,817 ERROR: ^
2025-07-24 22:40:33,817 ERROR: ^
2025-07-24 22:40:33,817 ERROR: ^
2025-07-24 22:40:33,817 ERROR: ^
2025-07-24 22:40:33,817 ERROR: ^
2025-07-24 22:40:33,817 ERROR: ^
2025-07-24 22:40:33,817 ERROR: ^
2025-07-24 22:40:33,817 ERROR: ^
2025-07-24 22:40:33,817 ERROR: ^
2025-07-24 22:40:33,817 ERROR: ^
2025-07-24 22:40:33,817 ERROR: ^
2025-07-24 22:40:33,817 ERROR: ^
2025-07-24 22:40:33,817 ERROR: ^
2025-07-24 22:40:33,817 ERROR: ^
2025-07-24 22:40:33,817 ERROR: ^
2025-07-24 22:40:33,817 ERROR: ^
2025-07-24 22:40:33,817 ERROR: ^
2025-07-24 22:40:33,817 ERROR: ^
2025-07-24 22:40:33,817 ERROR: ^
2025-07-24 22:40:33,817 ERROR: ^
2025-07-24 22:40:33,817 ERROR: ^
2025-07-24 22:40:33,817 ERROR: ^
2025-07-24 22:40:33,817 ERROR: ^
2025-07-24 22:40:33,817 ERROR: ^
2025-07-24 22:40:33,817 ERROR: ^
2025-07-24 22:40:33,817 ERROR: File "/usr/lib/python3.12/subprocess.py", line 548, in run
2025-07-24 22:40:33,817 ERROR: with Popen(*popenargs, **kwargs) as process:
2025-07-24 22:40:33,817 ERROR: ^
2025-07-24 22:40:33,817 ERROR: ^
2025-07-24 22:40:33,818 ERROR: ^
2025-07-24 22:40:33,818 ERROR: ^
2025-07-24 22:40:33,818 ERROR: ^
2025-07-24 22:40:33,818 ERROR: ^
2025-07-24 22:40:33,818 ERROR: ^
2025-07-24 22:40:33,818 ERROR: ^
2025-07-24 22:40:33,818 ERROR: ^
2025-07-24 22:40:33,818 ERROR: ^
2025-07-24 22:40:33,818 ERROR: ^
2025-07-24 22:40:33,818 ERROR: ^
2025-07-24 22:40:33,818 ERROR: ^
2025-07-24 22:40:33,818 ERROR: ^
2025-07-24 22:40:33,818 ERROR: ^
2025-07-24 22:40:33,818 ERROR: ^
2025-07-24 22:40:33,818 ERROR: ^
2025-07-24 22:40:33,818 ERROR: ^
2025-07-24 22:40:33,818 ERROR: ^
2025-07-24 22:40:33,818 ERROR: ^
2025-07-24 22:40:33,818 ERROR: ^
2025-07-24 22:40:33,818 ERROR: ^
2025-07-24 22:40:33,818 ERROR: ^
2025-07-24 22:40:33,818 ERROR: ^
2025-07-24 22:40:33,818 ERROR: ^
2025-07-24 22:40:33,818 ERROR: ^
2025-07-24 22:40:33,818 ERROR: ^
2025-07-24 22:40:33,818 ERROR: File "/usr/lib/python3.12/subprocess.py", line 1026, in __init__
2025-07-24 22:40:33,818 ERROR: self._execute_child(args, executable, preexec_fn, close_fds,
2025-07-24 22:40:33,818 ERROR: File "/usr/lib/python3.12/subprocess.py", line 1955, in _execute_child
2025-07-24 22:40:33,818 ERROR: raise child_exception_type(errno_num, err_msg, err_filename)
2025-07-24 22:40:33,818 ERROR: FileNotFoundError
2025-07-24 22:40:33,818 ERROR: :
2025-07-24 22:40:33,818 ERROR: [Errno 2] No such file or directory: 'hdfs'
2025-07-24 22:42:59,838 INFO: Caricato: /tmp/hdfs_tmp/_SUCCESS  s3://telematicbackup/claims/backup_2025-07-24T22-42-57/
2025-07-24 22:42:59,839 INFO: backup : hdfs://localhost:9000/user/dr.who/ingestion_area/current/bronze/claims eseguito in : telematicbackup/claims
2025-07-24 22:43:00,385 INFO: Closing down clientserver connection
2025-07-24 22:46:40,494 ERROR: Traceback (most recent call last):
2025-07-24 22:46:40,494 ERROR: File "/home/matteo/PycharmProjects/Tesi/BDA_Scripts/retention_area/8_dw_feeder.py", line 242, in <module>
2025-07-24 22:46:40,495 ERROR: backup_bronze(claims, access_key, secret_key, bucket_name, "claims")
2025-07-24 22:46:40,495 ERROR: File "/home/matteo/PycharmProjects/Tesi/BDA_Scripts/retention_area/8_dw_feeder.py", line 66, in backup_bronze
2025-07-24 22:46:40,495 ERROR: subprocess.run(["/opt/hadoop/bin/hdfs", "dfs", "-get", hdfs_file_path, str(local_file)], check=True)
2025-07-24 22:46:40,495 ERROR: File "/usr/lib/python3.12/subprocess.py", line 571, in run
2025-07-24 22:46:40,495 ERROR: raise CalledProcessError(retcode, process.args,
2025-07-24 22:46:40,495 ERROR: subprocess
2025-07-24 22:46:40,495 ERROR: .
2025-07-24 22:46:40,495 ERROR: CalledProcessError
2025-07-24 22:46:40,495 ERROR: :
2025-07-24 22:46:40,495 ERROR: Command '['/opt/hadoop/bin/hdfs', 'dfs', '-get', 'hdfs://localhost:9000/user/dr.who/ingestion_area/current/bronze/claims/_SUCCESS', '/tmp/hdfs_tmp/_SUCCESS']' returned non-zero exit status 1.
2025-07-24 22:46:40,495 INFO: Closing down clientserver connection
2025-07-24 22:50:43,438 INFO: Caricato: /home/matteo/BDA_project/ingestion_area/current/_SUCCESS  s3://telematicbackup/claims/backup_2025-07-24T22-50-41/
2025-07-24 22:50:44,542 INFO: Closing down clientserver connection
2025-07-24 22:53:05,286 INFO: Caricato: /home/matteo/BDA_project/ingestion_area/current/bronze/claims/_SUCCESS  s3://telematicbackup/claims/backup_2025-07-24T22-53-02/
2025-07-24 22:53:06,372 INFO: Closing down clientserver connection
2025-07-24 22:54:02,155 ERROR: Traceback (most recent call last):
2025-07-24 22:54:02,155 ERROR: File "/home/matteo/PycharmProjects/Tesi/BDA_Scripts/retention_area/8_dw_feeder.py", line 243, in <module>
2025-07-24 22:54:02,156 ERROR: backup_bronze(claims, access_key, secret_key, bucket_name, "claims")
2025-07-24 22:54:02,156 ERROR: File "/home/matteo/PycharmProjects/Tesi/BDA_Scripts/retention_area/8_dw_feeder.py", line 66, in backup_bronze
2025-07-24 22:54:02,156 ERROR: subprocess.run(["/opt/hadoop/bin/hdfs", "dfs", "-get", hdfs_file_path, str(local_file)], check=True)
2025-07-24 22:54:02,156 ERROR: File "/usr/lib/python3.12/subprocess.py", line 571, in run
2025-07-24 22:54:02,156 ERROR: raise CalledProcessError(retcode, process.args,
2025-07-24 22:54:02,156 ERROR: subprocess
2025-07-24 22:54:02,156 ERROR: .
2025-07-24 22:54:02,156 ERROR: CalledProcessError
2025-07-24 22:54:02,156 ERROR: :
2025-07-24 22:54:02,156 ERROR: Command '['/opt/hadoop/bin/hdfs', 'dfs', '-get', 'hdfs://localhost:9000/user/dr.who/ingestion_area/current/bronze/claims/_SUCCESS', '/home/matteo/BDA_project/ingestion_area/current/bronze/claims/_SUCCESS']' returned non-zero exit status 1.
2025-07-24 22:54:02,157 INFO: Closing down clientserver connection
2025-07-24 22:54:29,938 INFO: Caricato: /home/matteo/BDA_project/ingestion_area/current/bronze/claims/_SUCCESS  s3://telematicbackup/claims/backup_2025-07-24T22-54-27/
2025-07-24 22:54:30,815 ERROR: Errore caricando /home/matteo/BDA_project/ingestion_area/current/bronze/claims/_SUCCESS: Command '['/opt/hadoop/bin/hdfs', 'dfs', '-rm', '-r', '/home/matteo/BDA_project/ingestion_area/current/bronze/claims/_SUCCESS']' returned non-zero exit status 1.
2025-07-24 22:54:31,017 INFO: Closing down clientserver connection
2025-07-24 22:57:55,269 INFO: Caricato: /home/matteo/BDA_project/ingestion_area/current/bronze/claims/_SUCCESS  s3://telematicbackup/claims/backup_2025-07-24T22-57-52/
2025-07-24 22:57:56,156 ERROR: Errore caricando /home/matteo/BDA_project/ingestion_area/current/bronze/claims/_SUCCESS: Command '['/opt/hadoop/bin/hdfs', 'dfs', '-rm', '/home/matteo/BDA_project/ingestion_area/current/bronze/claims/_SUCCESS']' returned non-zero exit status 1.
2025-07-24 22:57:56,344 INFO: Closing down clientserver connection
2025-07-24 22:58:54,016 ERROR: Traceback (most recent call last):
2025-07-24 22:58:54,016 ERROR: File "/home/matteo/PycharmProjects/Tesi/BDA_Scripts/retention_area/8_dw_feeder.py", line 244, in <module>
2025-07-24 22:58:54,016 ERROR: backup_bronze(claims, access_key, secret_key, bucket_name, "claims")
2025-07-24 22:58:54,016 ERROR: File "/home/matteo/PycharmProjects/Tesi/BDA_Scripts/retention_area/8_dw_feeder.py", line 66, in backup_bronze
2025-07-24 22:58:54,016 ERROR: subprocess.run(["/opt/hadoop/bin/hdfs", "dfs", "-get", hdfs_file_path, str(local_file)], check=True)
2025-07-24 22:58:54,016 ERROR: File "/usr/lib/python3.12/subprocess.py", line 571, in run
2025-07-24 22:58:54,016 ERROR: raise CalledProcessError(retcode, process.args,
2025-07-24 22:58:54,016 ERROR: subprocess
2025-07-24 22:58:54,016 ERROR: .
2025-07-24 22:58:54,016 ERROR: CalledProcessError
2025-07-24 22:58:54,016 ERROR: :
2025-07-24 22:58:54,016 ERROR: Command '['/opt/hadoop/bin/hdfs', 'dfs', '-get', 'hdfs://localhost:9000/user/dr.who/ingestion_area/current/bronze/claims/_SUCCESS', '/home/matteo/BDA_project/ingestion_area/current/bronze/claims/_SUCCESS']' returned non-zero exit status 1.
2025-07-24 22:58:54,016 INFO: Closing down clientserver connection
2025-07-24 23:01:48,691 INFO: Caricato: /home/matteo/BDA_project/ingestion_area/current/bronze/claims/_SUCCESS  s3://telematicbackup/claims/backup_2025-07-24T23-01-46/
2025-07-24 23:01:48,770 INFO: Closing down clientserver connection
2025-07-24 23:10:27,868 INFO: hdfs://localhost:9000/user/dr.who/ingestion_area/current/silver/telematics spostato in : hdfs://localhost:9000/user/dr.who/ingestion_area/historical/silver/telematics
2025-07-24 23:10:30,537 INFO: Current Gold folder clean successfully.
2025-07-24 23:10:30,935 INFO: Closing down clientserver connection
2025-07-27 16:02:05,918 INFO: Scrittura completata per tabella: dim_speedings
2025-07-27 16:02:06,107 INFO: Scrittura completata per tabella: dim_accelerations
2025-07-27 16:02:06,340 INFO: Scrittura completata per tabella: dim_brakes
2025-07-27 16:02:06,585 INFO: Scrittura completata per tabella: dim_corners
2025-07-27 16:02:06,826 INFO: Scrittura completata per tabella: dim_lateral_movements
2025-07-27 16:02:07,585 INFO: Scrittura completata per tabella: dim_meters_travelled
2025-07-27 16:02:07,945 INFO: Scrittura completata per tabella: dim_seconds_travelled
2025-07-27 16:02:08,486 INFO: Scrittura completata per tabella: fact_trip
2025-07-27 16:02:09,566 INFO: hdfs://localhost:9000/user/dr.who/ingestion_area/current/bronze/telematics/behaviour spostato in : hdfs://localhost:9000/user/dr.who/ingestion_area/historical/bronze/telematics/behaviour
2025-07-27 16:02:12,271 INFO: Current Gold folder clean successfully.
2025-07-27 16:02:12,985 INFO: hdfs://localhost:9000/user/dr.who/ingestion_area/current/bronze/telematics/province spostato in : hdfs://localhost:9000/user/dr.who/ingestion_area/historical/bronze/telematics/province
2025-07-27 16:02:15,698 INFO: Current Gold folder clean successfully.
2025-07-27 16:02:16,562 INFO: hdfs://localhost:9000/user/dr.who/ingestion_area/current/bronze/telematics/trip_summary spostato in : hdfs://localhost:9000/user/dr.who/ingestion_area/historical/bronze/telematics/trip_summary
2025-07-27 16:02:19,235 INFO: Current Gold folder clean successfully.
2025-07-27 16:02:19,987 INFO: hdfs://localhost:9000/user/dr.who/ingestion_area/current/bronze/claims spostato in : hdfs://localhost:9000/user/dr.who/ingestion_area/historical/bronze/claims
2025-07-27 16:02:22,741 INFO: Current Gold folder clean successfully.
2025-07-27 16:02:23,701 INFO: hdfs://localhost:9000/user/dr.who/ingestion_area/current/silver/telematics spostato in : hdfs://localhost:9000/user/dr.who/ingestion_area/historical/silver/telematics
2025-07-27 16:02:26,446 INFO: Current Gold folder clean successfully.
2025-07-27 16:02:27,032 INFO: hdfs://localhost:9000/user/dr.who/ingestion_area/current/silver/claims spostato in : hdfs://localhost:9000/user/dr.who/ingestion_area/historical/silver/claims
2025-07-27 16:02:29,786 INFO: Current Gold folder clean successfully.
2025-07-27 16:02:30,113 INFO: Closing down clientserver connection
